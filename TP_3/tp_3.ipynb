{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP-3 Diffusion Models\n",
    "\n",
    "### Student information\n",
    "Student: Balthazar Neveu\n",
    "\n",
    "🔗 [Github](https://github.com/balthazarneveu/MVA24-delire) | [Online page for this class lab session](https://balthazarneveu.github.io/MVA24-delire)\n",
    "\n",
    "⭐ [Online HTML version of this notebook](https://balthazarneveu.github.io/MVA24-delire/TP_3/tp_3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    ">  Use \n",
    "> - the definition of the forward process $x_t \\sim \\mathcal{N}( \\sqrt{\\bar{\\alpha_t}}x_{0},\\,(1-\\bar{\\alpha_t}) I)$ \n",
    "> - Tweedie's identity\n",
    ">\n",
    ">  to show that \n",
    "> $ \\widehat{x_0}(x_t) = E[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t) \\right) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tweedie's formula__\n",
    "- $y = X+ N$\n",
    "  - $y$ noisy version of clean image\n",
    "  - $X$ with an additive white gaussian noise $N \\sim \\mathcal{N}(0, \\sigma^2 I)$ (variance $\\sigma^2$)\n",
    "- MMSE denoiser $D^{*}_{\\sigma^2}(y) = \\mathbb{E}[X|y=X+N]$.\n",
    "  - $D^{*}$ the star means this is the ideal maximum mean squared error denoiser.\n",
    "  - This will be modeled using a denoiser neural network trained on the AWGN denoising problem using the MSE loss. \n",
    "  - We'll only get an approximation of $D^{*}$\n",
    "- $\\frac{1}{\\sigma^2} (D^{*}_{\\sigma^2} - I)(x) = \\nabla \\log p_{\\sigma^2}(x)$  \n",
    "  - where $p_{\\sigma^2}(x)$ is the blurred version of the probability distribution of p by the Gaussian kernel of variance $\\sigma^2$ .\n",
    "  - $p_{\\sigma^2} := p \\star g_{\\sigma^2}$\n",
    "  - This means that the denoising residual is proportional to the score function (gradient of log propability) if the blurred distribution. The more noise level, the blurrier probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result proof__\n",
    "- From the forward process definition: \n",
    "  - (1) $x_t = \\sqrt{\\bar{\\alpha_t}}x_{0} + N$ \n",
    "  - with $N \\sim \\mathcal{N}(0, (1-\\bar{\\alpha_t})I) $\n",
    "  - Note: *$x_t$ is a noisy and \"darker\" (by a factor $\\bar{\\alpha_t}$) version of $x_{0}$. Making the image intensity tend to 0 is necessary to end up with a zero centered gaussian noise when $t \\rightarrow \\infty$*\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0|x_t]$ \n",
    "  - $\\widehat{x_0}(x_t)$ is the ideal MMSE denoiser result of $x_t$ .\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0| \\sqrt{\\bar{\\alpha_t}}x_{0} + N]$ by (1) the forward process definition\n",
    "\n",
    "We first need  to adapt to be able to apply Tweedie's formula ($\\bar{\\alpha_t}x_{0}$  and $x_{0}$ appear..., let's make it $X = \\sqrt{\\bar{\\alpha_t}}X_{0}$ so we can apply it):\n",
    "\n",
    "$$ \\widehat{x_0}(x_t) = \\frac{1}{\\sqrt{\\bar{\\alpha_t}}} \\mathbb{E}[\\sqrt{\\bar{\\alpha_t}} x_0| \\sqrt{\\bar{\\alpha_t}}x_{0} + N = x_{t}]$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply Tweedie's formula:\n",
    "$$ \\mathbb{E}(X | X + N =x_t) = x_t + {\\sigma^2} \\nabla \\log p_{\\sigma^2}(X=x_t)$$\n",
    "with a variance of $\\sigma^2 = (1-\\bar{\\alpha_t})$\n",
    "\n",
    "\n",
    "$$\\widehat{x_0}(x_t)  = \\frac{1}{\\sqrt{\\bar{\\alpha_t}}} \\left[ x_t + { (1-\\bar{\\alpha_t})} \\nabla \\log p_{(1-\\bar{\\alpha_t})}(X=x_t)\\right]$$ \n",
    "with $ p_{(1-\\bar{\\alpha_t})}(X=x_t) = p_{(1-\\bar{\\alpha_t})}(X_0=\\frac{x_t}{\\sqrt{\\bar{\\alpha_t}}}) $\n",
    "Here the smoothed probability of $X_0$ appears!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Forward process : $x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon_t$\n",
    "$$x_{0} = \\frac{x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t}{\\sqrt{\\bar\\alpha_t}}$$\n",
    "\n",
    "If we have an estimation of the noise $\\epsilon$ we should be able to recover the clean signal... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon_t = \\frac{x_t - \\sqrt{ \\bar\\alpha_t}x_0}{\\sqrt{1-\\bar\\alpha_t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From question 1: $$ \\widehat{x_0}(x_t) = \\mathbb{E}[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t) \\right) $$\n",
    "- From question 2: $$x_{0} = \\frac{x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t}{\\sqrt{\\bar\\alpha_t}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  $\\widehat{x_0}(x_t) = \\mathbb{E}[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\mathbb{E}[x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t | x_t] $\n",
    "   -  by definition of $\\widehat{x_0}(x_t)$\n",
    "   -  by substitution of the result from question 2 for $x_0$ under the expectation symbol. \n",
    "-  $\\sqrt{\\bar\\alpha_t}  \\widehat{x_0}(x_t)  = x_{t}  - \\sqrt{1-\\bar\\alpha_t} \\mathbb{E}[ \\epsilon_t | x_t]  $ \n",
    "   -  by linearity of the expectation\n",
    "   -  and since $\\mathbb{E}[X|X=x] = x$\n",
    "-  $\\sqrt{\\bar\\alpha_t}  \\widehat{x_0}(x_t) = x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t)$ *from question 1*\n",
    "-  We can remove $x_t$ on both sides.\n",
    "-  $(1-\\bar\\alpha_t) \\nabla \\log p(x_t) = -\\sqrt{1-\\bar\\alpha_t} \\mathbb{E}[ \\epsilon_t | x_t]$\n",
    "-  $\\Leftrightarrow  \\nabla \\log p(x_t) = -\\frac{1}{\\sqrt{1-\\bar\\alpha_t}} \\mathbb{E}[ \\epsilon_t | x_t] $\n",
    "\n",
    "$\\mathbb{E}[ \\epsilon_t | x_t] = \\hat{\\epsilon}(x_t)  \\approx \\hat{\\epsilon}_{\\theta}(x_t, t) $ is the expectation of the noise realization. This will be approximated by the neural network trained using MMSE. At that point, I think that the equal symbol does not stand and that we start making approximations.\n",
    "\n",
    "\n",
    "\n",
    "__Result__\n",
    "$$\\nabla \\log p(x_t) = - \\frac{1}{\\sqrt{1-\\bar\\alpha_t}} \\hat\\epsilon_\\theta(x_t)$$\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update equation can be rewritten as:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}  \\hat\\epsilon \\right) + \\sigma_t z_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} + \\underbrace{\\beta_t}_{1-\\alpha_t} . \\underbrace{\\left[- \\frac{1}{\\sqrt{1 - \\bar{\\alpha_t}}}  \\hat\\epsilon\\right]}_{\\nabla \\log p(x_t)} \\right) + \\sigma_t z_t$$\n",
    "\n",
    " \n",
    "$\\rightarrow x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} + (1 - \\alpha_t) \\nabla \\log p(x_t) \\right) + \\sigma_t z_t$\n",
    "\n",
    "Looks familiar... Looks like stochastic gradient ascent as we're trying to maximize the likelihood of$x_t$ to belong to the image distribution   (stochastic because of the $\\sigma_t z_t$ noise term).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
