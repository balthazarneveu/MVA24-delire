{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP-3 Diffusion Models\n",
    "\n",
    "### Student information\n",
    "Student: Balthazar Neveu\n",
    "\n",
    "ðŸ”— [Github](https://github.com/balthazarneveu/MVA24-delire) | [Online page for this class lab session](https://balthazarneveu.github.io/MVA24-delire)\n",
    "\n",
    "â­ [Online HTML version of this notebook](https://balthazarneveu.github.io/MVA24-delire/TP_3/tp_3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM: Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    ">  Use \n",
    "> - the definition of the forward process $x_t \\sim \\mathcal{N}( \\sqrt{\\bar{\\alpha_t}}x_{0},\\,(1-\\bar{\\alpha_t}) I)$ \n",
    "> - Tweedie's identity\n",
    ">\n",
    ">  to show that \n",
    "> $ \\widehat{x_0}(x_t) = E[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t) \\right) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tweedie's formula__\n",
    "- $y = X+ N$\n",
    "  - $y$ noisy version of clean image\n",
    "  - $X$ with an additive white gaussian noise $N \\sim \\mathcal{N}(0, \\sigma^2 I)$ (variance $\\sigma^2$)\n",
    "- MMSE denoiser $D^{*}_{\\sigma^2}(y) = \\mathbb{E}[X|y=X+N]$.\n",
    "  - $D^{*}$ the star means this is the ideal maximum mean squared error denoiser.\n",
    "  - This will be modeled using a denoiser neural network trained on the AWGN denoising problem using the MSE loss. \n",
    "  - We'll only get an approximation of $D^{*}$\n",
    "- $\\frac{1}{\\sigma^2} (D^{*}_{\\sigma^2} - I)(x) = \\nabla \\log p_{\\sigma^2}(x)$  \n",
    "  - where $p_{\\sigma^2}(x)$ is the blurred version of the probability distribution of p by the Gaussian kernel of variance $\\sigma^2$ .\n",
    "  - $p_{\\sigma^2} := p \\star g_{\\sigma^2}$\n",
    "  - This means that the denoising residual is proportional to the score function (gradient of log propability) if the blurred distribution. The more noise level, the blurrier probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result proof__\n",
    "- From the forward process definition: \n",
    "  - (1) $x_t = \\sqrt{\\bar{\\alpha_t}}x_{0} + N$ \n",
    "  - with $N \\sim \\mathcal{N}(0, (1-\\bar{\\alpha_t})I) $\n",
    "  - Note: *$x_t$ is a noisy and \"darker\" (by a factor $\\bar{\\alpha_t}$) version of $x_{0}$. Making the image intensity tend to 0 is necessary to end up with a zero centered gaussian noise when $t \\rightarrow \\infty$*\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0|x_t]$ \n",
    "  - $\\widehat{x_0}(x_t)$ is the ideal MMSE denoiser result of $x_t$ .\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0| \\sqrt{\\bar{\\alpha_t}}x_{0} + N]$ by (1) the forward process definition\n",
    "\n",
    "We first need  to adapt to be able to apply Tweedie's formula ($\\bar{\\alpha_t}x_{0}$  and $x_{0}$ appear..., let's make $X = \\sqrt{\\bar{\\alpha_t}}X_{0}$ appear on both sides so we can apply the formula):\n",
    "\n",
    "$$ \\widehat{x_0}(x_t) = \\frac{1}{\\sqrt{\\bar{\\alpha_t}}} \\mathbb{E}[\\underbrace{\\sqrt{\\bar{\\alpha_t}} x_0}_{X}| \\underbrace{\\sqrt{\\bar{\\alpha_t}}x_{0}}_{X} + N = x_{t}]$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply Tweedie's formula:\n",
    "- with a variance of $\\sigma^2 = (1-\\bar{\\alpha_t})$\n",
    "- with the reparameterization  $X = \\sqrt{\\bar{\\alpha_t}}X_{0}$ previously mentioned\n",
    "$$ \\mathbb{E}(X | X + N =x_t) = x_t + {\\sigma^2} \\nabla \\log p_{\\sigma^2}(X=x_t)$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\widehat{x_0}(x_t)  = \\frac{1}{\\sqrt{\\bar{\\alpha_t}}} \\left[ x_t + { (1-\\bar{\\alpha_t})} \\nabla \\log p_{(1-\\bar{\\alpha_t})}(X=x_t)\\right]$$ \n",
    "If we look cautiously, this is not exactly the result we'd expect as **a blurred probability appears**!\n",
    "\n",
    "with $ p_{(1-\\bar{\\alpha_t})}(X=x_t) = p_{(1-\\bar{\\alpha_t})}(X_0=\\frac{x_t}{\\sqrt{\\bar{\\alpha_t}}})$\n",
    "Here the smoothed probability of $X_0$ appears which is equal to the probability $p(X_t=x_t)$ of the **non smoothed but noisy version** $X_t$ defined by the forward process  (adding Gaussian noise is equivalent to convolve the distribution by a gaussian blur).\n",
    "\n",
    "Hence the final result\n",
    "\n",
    "$$\\widehat{x_0}(x_t)  = \\frac{1}{\\sqrt{\\bar{\\alpha_t}}} \\left[ x_t + { (1-\\bar{\\alpha_t})} \\nabla \\log p(x_t)\\right]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Forward process : $x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon_t$\n",
    "$$x_{0} = \\frac{x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t}{\\sqrt{\\bar\\alpha_t}}$$\n",
    "\n",
    "If we have an estimation of the noise $\\epsilon$ we should be able to recover the clean signal... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon_t = \\frac{x_t - \\sqrt{ \\bar\\alpha_t}x_0}{\\sqrt{1-\\bar\\alpha_t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From question 1: $$ \\widehat{x_0}(x_t) = \\mathbb{E}[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t) \\right) $$\n",
    "- From question 2: $$x_{0} = \\frac{x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t}{\\sqrt{\\bar\\alpha_t}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  $\\widehat{x_0}(x_t) = \\mathbb{E}[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\mathbb{E}[x_{t} - \\sqrt{1-\\bar\\alpha_t} \\epsilon_t | x_t] $\n",
    "   -  by definition of $\\widehat{x_0}(x_t)$\n",
    "   -  by substitution of the result from question 2 for $x_0$ under the expectation symbol. \n",
    "-  $\\sqrt{\\bar\\alpha_t}  \\widehat{x_0}(x_t)  = x_{t}  - \\sqrt{1-\\bar\\alpha_t} \\mathbb{E}[ \\epsilon_t | x_t]  $ \n",
    "   -  by linearity of the expectation\n",
    "   -  and since $\\mathbb{E}[X|X=x] = x$\n",
    "-  $\\sqrt{\\bar\\alpha_t}  \\widehat{x_0}(x_t) = x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t)$ *from question 1*\n",
    "-  We can remove $x_t$ on both sides.\n",
    "-  $(1-\\bar\\alpha_t) \\nabla \\log p(x_t) = -\\sqrt{1-\\bar\\alpha_t} \\mathbb{E}[ \\epsilon_t | x_t]$\n",
    "-  $\\Leftrightarrow  \\nabla \\log p(x_t) = -\\frac{1}{\\sqrt{1-\\bar\\alpha_t}} \\mathbb{E}[ \\epsilon_t | x_t] $\n",
    "\n",
    "$\\mathbb{E}[ \\epsilon_t | x_t] = \\hat{\\epsilon}(x_t)  \\approx \\hat{\\epsilon}_{\\theta}(x_t, t) $ is the expectation of the noise realization. This will be approximated by the neural network trained using MMSE. At that point, I think that the equal symbol does not stand and that we start making approximations.\n",
    "\n",
    "\n",
    "\n",
    "__Result__\n",
    "$$\\nabla \\log p(x_t) = - \\frac{1}{\\sqrt{1-\\bar\\alpha_t}} \\hat\\epsilon_\\theta(x_t)$$\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update equation can be rewritten as:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}  \\hat\\epsilon \\right) + \\sigma_t z_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} + \\underbrace{\\beta_t}_{1-\\alpha_t} . \\underbrace{\\left[- \\frac{1}{\\sqrt{1 - \\bar{\\alpha_t}}}  \\hat\\epsilon\\right]}_{\\nabla \\log p(x_t)} \\right) + \\sigma_t z_t$$\n",
    "\n",
    " \n",
    "$\\rightarrow x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_{t} + (1 - \\alpha_t) \\nabla \\log p(x_t) \\right) + \\sigma_t z_t$\n",
    "\n",
    "Looks familiar... Looks like stochastic gradient ascent as we're trying to maximize the likelihood of$x_t$ to belong to the image distribution   (stochastic because of the $\\sigma_t z_t$ noise term).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "![](figures/alpha_schedule_small.png)\n",
    "\n",
    "| Timesteps | 1000 | 100 | 10 | 3 | 2 |\n",
    "|:---:| :--:|:--:|:--: | :--:| :--:|\n",
    "|Generation time on Nvidia T500 | 5 min 30 | 32s | 2s | $< 1s$ | $< 1s$ |\n",
    "| Generated image |![](figures/dpm_sample.png)|  ![](figures/dpm_sample_t100.png)|  ![](figures/dpm_sample_t10.png)|![](figures/dpm_sample_t3.png)|![](figures/dpm_sample_t2.png)|\n",
    "\n",
    "\n",
    "It is important to take the steps to respect the diffusion framework conditions. If we reduce the amount of steps, we end up with a lack of details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Diffusion models for inverse problems\n",
    "\n",
    "![](figures/degradation.png)\n",
    "\n",
    "Degradation: Large motion blur with AWGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Let's check on a synthetic unitary test that we can compute the gradient of the \"forward model\" (blurring operator) with regard to the \n",
    "input.\n",
    "\n",
    "See the code below for more details.\n",
    "\n",
    "![](figures/blurt_kernel.png)\n",
    "![](figures/deblur_data_term_gradient.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the degradation model to an image and optimize a purely noisy prediction input to match the output by gradient descent\n",
    "# This allows checking that we can compute the gradient g of f.\n",
    "# Note that the gradient of a convolution is the transposed convolution.\n",
    "# Here the blur being large, the convolution is performed as an iFFT(FFT(x) x FFT(k)) \n",
    "import utils.utils_agem as agem\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "kernel  = torch.zeros(1, 1, 9, 9)\n",
    "diag_size  = 4\n",
    "for idx in range(-(diag_size//2), diag_size//2+1+1):\n",
    "    kernel[0, 0, kernel.shape[-2]//2+idx, kernel.shape[-1]//2+idx] = abs(idx)\n",
    "kernel/=kernel.sum()\n",
    "kernel.requires_grad = False\n",
    "forward_model = lambda x: agem.fft_blur(x, kernel.to(device))\n",
    "# inp = torch.rand(1, 1, 10, 10).to(device\n",
    "inp = torch.zeros(1, 3, 32, 32).to(device)\n",
    "inp[..., inp.shape[2]//4:3*inp.shape[2]//4, inp.shape[3]//4:3*inp.shape[3]//4] = 1.\n",
    "out = forward_model(inp)\n",
    "plt.figure(figsize=(10, 10/3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(inp[0, 0].cpu().numpy(), cmap='gray')\n",
    "plt.title('Groundtruth')\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.imshow(kernel[0, 0].cpu().numpy())\n",
    "plt.title('Blur kernel')\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(out[0, 0].cpu().numpy(), cmap='gray')\n",
    "plt.title('Degradation')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "pred = 0.2*torch.rand_like(inp)\n",
    "plt.figure(figsize=(30, 3))\n",
    "pred.requires_grad = True\n",
    "f = torch.norm(forward_model(pred) - out)\n",
    "for i in range(1001):\n",
    "    if i%100 == 0:\n",
    "        plt.subplot(1,11,i//100+1)\n",
    "        plt.imshow(pred[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'{i} step | loss {f.item():.3f}')\n",
    "        plt.axis('off')\n",
    "    f.backward()\n",
    "    pred.data -= 0.1*pred.grad\n",
    "    pred.grad.zero_()\n",
    "    f = torch.norm(forward_model(pred) - out)\n",
    "plt.suptitle('Optimization of the data term (blur) using the degradation model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 6\n",
    "$\\eta_t = \\frac{\\beta_t}{\\sqrt{\\alpha_t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
