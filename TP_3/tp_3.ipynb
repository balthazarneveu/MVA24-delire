{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP-3 Diffusion Models\n",
    "\n",
    "### Student information\n",
    "Student: Balthazar Neveu\n",
    "\n",
    "ðŸ”— [Github](https://github.com/balthazarneveu/MVA24-delire) | [Online page for this class lab session](https://balthazarneveu.github.io/MVA24-delire)\n",
    "\n",
    "â­ [Online HTML version of this notebook](https://balthazarneveu.github.io/MVA24-delire/TP_3/tp_3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    ">  Use \n",
    "> - the definition of the forward process $x_t \\sim \\mathcal{N}( \\sqrt{\\bar{\\alpha_t}}x_{0},\\,(1-\\bar{\\alpha_t}) I)$ \n",
    "> - Tweedie's identity\n",
    ">\n",
    ">  to show that \n",
    "> $ \\widehat{x_0}(x_t) = E[x_0|x_t] = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left( x_t + (1-\\bar\\alpha_t) \\nabla \\log p(x_t) \\right) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tweedie's formula__\n",
    "- $y = X+ N$\n",
    "  - $y$ noisy version of clean image\n",
    "  - $X$ with an additive white gaussian noise $N \\sim \\mathcal{N}(0, \\sigma^2 I)$ (variance $\\sigma^2$)\n",
    "- MMSE denoiser $D^{*}_{\\sigma^2}(y) = \\mathbb{E}[X|y=X+N]$.\n",
    "  - $D^{*}$ the star means this is the ideal maximum mean squared error denoiser.\n",
    "  - This will be modeled using a denoiser neural network trained on the AWGN denoising problem using the MSE loss. \n",
    "  - We'll only get an approximation of $D^{*}$\n",
    "- $\\frac{1}{\\sigma^2} (D^{*}_{\\sigma^2} - I)(x) = \\nabla \\log p_{\\sigma^2}(x)$  \n",
    "  - where $p_{\\sigma^2}(x)$ is the blurred version of the probability distribution of p by the Gaussian kernel of variance $\\sigma^2$ .\n",
    "  - $p_{\\sigma^2} := p \\star g_{\\sigma^2}$\n",
    "  - This means that the denoising residual is proportional to the score function (gradient of log propability) if the blurred distribution. The more noise level, the blurrier probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result proof__\n",
    "- From the forward process definition: \n",
    "  - (1) $x_t = \\sqrt{\\bar{\\alpha_t}}x_{0} + N$ \n",
    "  - with $N \\sim \\mathcal{N}(0, (1-\\bar{\\alpha_t})I) $\n",
    "  - Note: *$x_t$ is a noisy and \"darker\" (by a factor $\\bar{\\alpha_t}$) version of $x_{0}$. Making the image intensity tend to 0 is necessary to end up with a zero centered gaussian noise when $t \\rightarrow \\infty$*\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0|x_t]$ \n",
    "  - $\\widehat{x_0}(x_t)$ is the ideal MMSE denoiser result of $x_t$ .\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0| \\sqrt{\\bar{\\alpha_t}}x_{0} + N]$ by (1) the forward process definition\n",
    "\n",
    "We can apply Tweedie's identity:\n",
    "-  $\\frac{1}{\\sigma^2} (\\widehat{x_0}(x_t) - x_t) = \\nabla \\log p_{\\sigma^2}(x_t)$ where $\\sigma^2 = (1-\\bar{\\alpha_t})$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result proof__\n",
    "<!-- $x_t \\sim \\mathcal{N}( \\sqrt{\\bar{\\alpha_t}}x_{0},\\,(1-\\bar{\\alpha_t}) I)$  -->\n",
    "- From the forward process definition: \n",
    "  - (1) $x_t = \\sqrt{\\bar{\\alpha_t}}x_{0} + N$ \n",
    "  - with $N \\sim \\mathcal{N}(0, (1-\\bar{\\alpha_t})I) $\n",
    "  - Note: *$x_t$ is a noisy and \"darker\" (by a factor $\\bar{\\alpha_t}$) version of $x_{0}$. Making the image intensity tend to 0 is necessary to end up with a zero centered gaussian noise when $t \\rightarrow \\infty$*\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0|x_t]$ \n",
    "  - $\\widehat{x_0}(x_t)$ is the ideal MMSE denoiser result of $x_t$ .\n",
    "- $ \\widehat{x_0}(x_t) = E[x_0| \\sqrt{\\bar{\\alpha_t}}x_{0} + N]$ by (1) the forward process definition\n",
    "\n",
    "We can apply Tweedie's identity:\n",
    "-  $\\frac{1}{\\sigma^2} (\\widehat{x_0}(x_t) - x_t) = \\nabla \\log p_{\\sigma^2}(x_t)$ where $\\sigma^2 = (1-\\bar{\\alpha_t})$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\widehat{x_0}(x_t) = x_t + \\sigma^2 . \\nabla \\log p_{\\sigma^2}(x_t) = x_t  + (1-\\bar{\\alpha_t}) \\nabla \\log p_{\\sigma^2}(x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
