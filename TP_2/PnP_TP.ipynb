{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["rnJQFx3etvrh","gF8XbGcQAZiI","m6PZhhGkPiXP","Ugv9yzPGhOoQ","5A5Hybq0PODU","ILZYg3mVPsrG","jgN4rMm_me6Q","WhazS0GympcS","bc4d2GCUvymG","c1f8bgrwyxNa","4DkZ-TDcT-2p","IoeeJWDX85YX","pyjMlYDa-Tip","-FkW0Y24GaHS","RggtEIDhYqTd","adXHw-AUq2ez","eQFI-EGvAkQE","cWxO_i_1T7pL","XDluF4vAuYbB","DiMZBchW2Ul2"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-9Rq7fSS_03n"},"source":["# TP2: Plug & Play Algorithms - M2 MVA DeLIReS\n","\n","<u>*Andr√©s Almansa - M2 MVA - Deep Learning for Image Restoration and Synthesis*</u>\n","\n","\n","\n","In this practical session you will experiment with the Plug & Play ADMM and ISTA algorithms presented in the previous lecture, add a new inverse problem to that framework, and experimentally verify the validity of the convergence conditions and results that were presented in class.\n","\n","As a reference for these results you can refer to the course slides[^1], the references therein, in particular the articles by Ryu *et al.* (2019) [^2] and by Xu *et al.* (2020)[^3].\n","\n","### References\n","\n","[^1]: Slides of previous lecture http://up5.fr/delires2024-cours3 or http://up5.fr/delires2024 under `supports-des-cours/3-Cours-PnP-optim/`\n","\n","[^2]: Ryu, E. K., Liu, J., Wang, S., Chen, X., Wang, Z., & Yin, W. (2019). Plug-and-Play Methods Provably Converge with Properly Trained Denoisers. In *ICML*.  [arXiv:1905.05406](http://arxiv.org/abs/1905.05406)\n","\n","[^3]: Xu, X., Sun, Y., Liu, J., Wohlberg, B., & Kamilov, U. S. (2020). Provable Convergence of Plug-and-Play Priors with MMSE denoisers. [arXiv:2005.07685](http://arxiv.org/abs/2005.07685)"]},{"cell_type":"markdown","source":["## Sending your report\n","\n","When you are finished please upload your report here\n","\n","http://up5.fr/delires2024-cr-tp2\n","\n","You should upload a file with the following format `NAME-Surname-TP2.ext` where ext can be:\n","\n","* pdf - for your report\n","* ipynb - for your jupyter notebook\n","* zip - if you hand in a single zip-file containing both the jupyter notebook and your pdf report\n"],"metadata":{"id":"GKSo8hdDtZmf"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"rnJQFx3etvrh"}},{"cell_type":"markdown","metadata":{"id":"gF8XbGcQAZiI"},"source":["\n","To run this practical session (TP) you will need\n","\n","* python3\n","* a few standard python packages like numpy, pytorch, script, matplotlib\n","* the code provided for this TP\n","\n","For the python part you need access to a jupyter notebook server either on your local machine, on google colab or on a server provided by your university.\n","\n","I recommend to use a notebook server that allows the use of a GPU to accelerate calls to the pytorch framework, like Google colab (on which this TP was tested). It will ussually run the CNN-based denoisers much faster.\n","\n","The specific code for this TP can be obtained (in its original version) from the author's GitHub site:\n","\n","https://github.com/uclaopt/Provable_Plug_and_Play\n","\n","A modified code that makes the implementation more modular and makes it easier to switch between CPU and GPU powered servers is available here [^4], along with the code needed to run the TV denoisers.\n","\n","Download the notebook and open it with Google colab (or your preferred jupyter notebook server).\n","\n","Then download the zip file PnP-TP.zip and unzip it into your Google Drive.\n","\n","Then go to your notebook and run the following code in order to have access to the python code in the zip file.\n","\n","[^4]: http://up5.fr/delires2024-cours3. Or go to the main folder for course materials http://up5.fr/delires2024 and navigate to `supports-des-cours/3-Cours-PnP-optim/`. If this doesn't work try [this](https://partage.imt.fr/index.php/s/EJ7YRj9NTLodHSA) for the direct link to lecture 3, or [this](https://partage.imt.fr/index.php/s/5BLxg5yFHpoqA66) for the main folder."]},{"cell_type":"markdown","metadata":{"id":"m6PZhhGkPiXP"},"source":["### Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"F7gtWFuLyFJ4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HjsRHp8nPB5t"},"source":["Go to the folder in Google drive where you downloaded the provided code. And check its contents."]},{"cell_type":"code","metadata":{"id":"Xn1gt5leyYV-"},"source":["%cd /content/drive/My Drive/Colab Notebooks/MVA-DELIRES-2024/PnP-TP/\n","!ls ./images\n","!ls ./utils\n","!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Check GPU\n","\n","If a cuda GPU is not available, change the runtime type in the Runtime menu."],"metadata":{"id":"Ugv9yzPGhOoQ"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"iaRj8QluhbbL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following code checks if Apple Silicon or nvidia GPU is available.\n","In that case it sets `device` to \"mps\" or \"cuda\" respectively.\n","Otherwise fallback to `device=\"cpu\"`.\n","If you have access to a TPU in google colab or another non-cuda GPU you may need to change the code below."],"metadata":{"id":"srF6jMwpmgnA"}},{"cell_type":"code","source":["# Select best GPU, fallback to CPU if no GPU is available\n","import os, torch\n","\n","# If Apple Silicon processor is available set device to \"mps\"\n","if torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","# If nvidia GPU is available set device to \"cuda\"\n","elif torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","# otherwise fallback to \"cpu\"\n","else:\n","    device = torch.device(\"cpu\")\n","    print (\"GPU not found using CPU.\")\n","\n","# device = torch.device(\"cpu\")\n","\n","device"],"metadata":{"id":"sbGZcoQwlvw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5A5Hybq0PODU"},"source":["### Install required packages"]},{"cell_type":"code","metadata":{"id":"DU0HpEyRPThd"},"source":["! pip install tifffile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6c7x_9_2eqzh"},"source":["# Load BM3D denoiser\n","! pip install bm3d\n","from bm3d import bm3d, BM3DProfile\n","\n","class BM3DDenoiser:\n","    def __init__(self,\n","        sigma = 40,                  # Noise stdev for an 8-bit image in [0,255]\n","        rescale = False):             # If true rescale [min,max] to [0,1] before applying denoiser\n","        self.sigma = sigma/255.0\n","        self.rescale = rescale\n","    def denoise(self,xtilde):\n","        \"\"\"\n","        Inputs:\n","            :xtilde     noisy image\n","        Outputs:\n","            :x          denoised image\n","        \"\"\"\n","        # scale xtilde to be in range of [0,1] (for the clean image)\n","        if self.rescale:\n","            mintmp = np.min(xtilde)\n","            #mintmp = 0.0\n","            maxtmp = np.max(xtilde)\n","            #maxtmp = 255.0\n","            xtilde = (xtilde - mintmp) / (maxtmp - mintmp)\n","\n","        # upgrade to 3D tensor\n","        xtildeBM3D = np.atleast_3d(xtilde)\n","        x = bm3d(xtildeBM3D,self.sigma)\n","\n","        # rescale the denoised v back to original scale\n","        if self.rescale:\n","            x = x * (maxtmp - mintmp) + mintmp\n","\n","        return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILZYg3mVPsrG"},"source":["## Preliminaries: test the denoisers\n"]},{"cell_type":"markdown","metadata":{"id":"jgN4rMm_me6Q"},"source":["### Load input image"]},{"cell_type":"code","metadata":{"id":"853SOGOhPxYY"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tifffile as tiff\n","\n","# Load clean image\n","# u_total = plt.imread('./images/simpson_nb512.png')\n","# u_total = tiff.imread('./images/cameraman.tif')\n","u_total = tiff.imread('./images/barbara.tif')\n","\n","# Extract a small part\n","#u = np.zeros((256,256), dtype = np.float64)\n","#u = u_total[0:256, 0:256]\n","u = u_total #[100:200,100:200]\n","\n","# Normalize to [0,1]\n","maxval = 255.0\n","minval = 0\n","u = (u - minval)/(maxval-minval)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhazS0GympcS"},"source":["### Simulate noisy image"]},{"cell_type":"code","metadata":{"id":"FVipGz6Jmvqu"},"source":["# Use always the same seed for random numbers for reproducibility\n","np.random.seed(42)\n","\n","# Simulate noisy image\n","sigma255 = 40\n","sigma1 = sigma255/255.0\n","noise = np.random.normal(loc=0, scale=sigma1, size=(u.shape))\n","utilde = u + noise\n","\n","# Show original and noisy images\n","fig = plt.figure(figsize = (16,16))\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.imshow(u, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('Original image', fontsize = 15)\n","ax2 = fig.add_subplot(1,2,2)\n","ax2.imshow(utilde, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('Noisy image', fontsize = 15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc4d2GCUvymG"},"source":["### Test TV and DnCNN denoisers\n","\n","Load an input image and add random gaussian noise with $\\sigma_{255}=40$, 15 or 5.\n","\n","Byte-encoded *gray*-scale images take values in the range [0,255].\n","\n","But denoising algorithms assume that clean images take values in the range [0,1].\n","\n","So you need to first normalize your clean and noisy images and sigma values to the range [0,1]:\n","\n","$u_1 = u_{255}/255$\n","\n","$\\sigma_1 = \\sigma_{255}/255$\n","\n","The DnCNN denoiser can be deployed as is, but for the TV denoising you need to choose the regularization parameter $\\lambda$ in\n","\n","$\\hat{u}_\\lambda = \\arg\\min_u \\frac{1}{2}\\|u-\\tilde{u}\\|^2 + \\lambda TV(u)$"]},{"cell_type":"code","metadata":{"id":"O1-MUxbhvzBJ"},"source":["from TV_denoiser import TVDenoiser\n","from pytorch_denoiser import PyTorchDenoiser\n","\n","# TV denoiser\n","TVlamb = sigma1**2\n","niter = 100\n","denoiser = TVDenoiser(TVlamb,niter)\n","uTV = denoiser.denoise(utilde)\n","\n","# DnCNN denoiser\n","denoiser = PyTorchDenoiser('DnCNN',sigma255,cuda=True,rescale=False)\n","uDnCNN = denoiser.denoise(utilde)\n","\n","# BM3D denoiser With the default settings.\n","#denoiser = BM3DDenoiser(sigma255)\n","#uBM3D = denoiser.denoise(utilde)\n","\n","# Print performance and plot results\n","from utils.psnr import rmse, psnr\n","# printperf(residual, L2err) :\n","# -- prints out residual/sigma, rmse and psnr values\n","# peak=np.linalg.norm(u);\n","peak=1.0\n","printperf = lambda residual,L2err : print('residual/sigma={:.2f}\\t RMSE={:.4f} ({:.1f} dB)'.format(residual,L2err,20*np.log(peak/L2err)/np.log(10)))\n","\n","print('TV Denoiser performance')\n","L2err = rmse(uTV,u)\n","residual = rmse(uTV,utilde)\n","printperf(residual/sigma1, L2err)\n","\n","print('DnCNN Denoiser performance')\n","L2err = rmse(uDnCNN,u)\n","residual = rmse(uDnCNN,utilde)\n","printperf(residual/sigma1, L2err)\n","\n","#print('BM3D Denoiser performance')\n","#L2err = rmse(uBM3D,u)\n","#residual = rmse(uBM3D,utilde)\n","#printperf(residual/sigma1, L2err)\n","\n","\n","# Show denoised images\n","fig = plt.figure(figsize = (16,16))\n","ax1 = fig.add_subplot(1,3,1)\n","ax1.imshow(uTV, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('TV denoising', fontsize = 15)\n","ax2 = fig.add_subplot(1,3,2)\n","ax2.imshow(uDnCNN, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('DnCNN denoising', fontsize = 15)\n","#ax3 = fig.add_subplot(1,3,3)\n","#ax3.imshow(uBM3D, cmap = 'gray', vmin=0, vmax = 1)\n","#ax3.set_title('BM3D denoising', fontsize = 15)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1f8bgrwyxNa"},"source":["## Find optimal $\\lambda$ for TV denoiser\n","\n","The DnCNN denoiser can be deployed as is (for fixed pretrained noise values), but for the TV denoising you need to choose the regularization parameter $\\lambda$ in\n","\n","$\\hat{u}_\\lambda = \\arg\\min_u \\frac{1}{2}\\|u-\\tilde{u}\\|^2 + \\lambda TV(u)$\n","\n","Good values for $\\lambda$ are usually between $\\sigma$ and $\\sigma^2$, but this really depends on the image contents: Total variation penalizes high frequency oscilatory patterns, so in those areas a smaller value of $\\lambda$ may be required to preserve them.\n","\n","A way to circumvent this problem is to systematically search for a value of $\\lambda$ whose residual is close to the noise level:\n","\n","$\\|\\hat{u}_\\lambda - \\tilde{u}\\|^2 \\approx \\beta^2 \\sigma^2 N$\n","\n","To be conservative (we prefer to leave a bit of noise rather than removing details) we choose a value of $\\beta < 1$ like $\\beta=0.9$\n","\n","At each iteration the value of $\\lambda$ can be updated in the following manner:\n","\n","$\\lambda_{n+1} = \\lambda_n e^{\\rho(\\sigma\\beta - \\|\\hat{u}_{\\lambda_n} - \\tilde{u}\\|/\\sqrt{N})}$\n","\n","This will increase or decrease $\\lambda$ as needed to meet the residual constraint.\n","\n","You can stop after 100 iterations or when the constraint is met up to 1% tolerance\n","\n","> _**QUESTION 1**: Complete the code below as indicated_\n","\n","> _**QUESTION 2**: Provide the optimal value of $\\lambda$ and the RMSEs / PSNRs that you obtain for TV and the different PyTorch denoisers. Which denoiser provides the best performance?_\n","\n","> _**QUESTION 3**: Is there a big difference between DnCNN and RealSN_DnCNN? Explain why or why not._\n","\n"]},{"cell_type":"code","metadata":{"id":"7m5GHu-fTASU"},"source":["# Optimal lambda\n","print('Searching for optimal lambda for TV denoising')\n","TVlamb = sigma1**2\n","rho = 10\n","beta = 0.95\n","for i in range(1,100):\n","  uTV = TVDenoiser(lamb=TVlamb,niter=100).denoise(utilde)\n","  res = rmse(uTV,utilde)\n","  error = rmse(uTV,u)\n","  TVlamb = ... ## COMPLETE this code\n","  print('residual/sigma = {:.3f}\\t rmse = {:.4e}\\t lambda = {:.4e}'.format((res/sigma1),error,TVlamb))\n","  stop_cond= ... ## COMPLETE this code\n","  if stop_cond:\n","    break\n","\n","# Denoise\n","print('TV Denoiser performance')\n","denoiser = TVDenoiser(TVlamb,100)\n","uTV = denoiser.denoise(utilde)\n","L2err = rmse(uTV,u)\n","residual = rmse(uTV,utilde)\n","printperf(residual/sigma1, L2err)\n","\n","print('DnCNN Denoiser performance')\n","denoiser = PyTorchDenoiser('DnCNN',sigma255,cuda=True,rescale=False)\n","uDnCNN = denoiser.denoise(utilde)\n","L2err = rmse(uDnCNN,u)\n","residual = rmse(uDnCNN,utilde)\n","printperf(residual/sigma1, L2err)\n","\n","\n","# Show denoised images\n","fig = plt.figure(figsize = (16,16))\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.imshow(uTV, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('TV denoising', fontsize = 15)\n","ax2 = fig.add_subplot(1,2,2)\n","ax2.imshow(uDnCNN, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('DnCNN denoising', fontsize = 15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXC3eXNrLz87"},"source":["## Plug & Play ADMM / DRS for Gaussian denosing"]},{"cell_type":"markdown","source":["### Introduction"],"metadata":{"id":"4DkZ-TDcT-2p"}},{"cell_type":"markdown","source":["In the previous section TV denoising can be adapted to different values of $\\sigma$ by changing the value of $\\lambda$. The DnCNN denoiser, however, is pretrained for 3 fixed values of $\\sigma \\in \\{5,15,40\\}$. Now we are going to use(write) a DRS(ADMM) splitting algorithm that can denoise images with other noise levels, using the pretrained DnCNN as a regularizer.\n","\n","The code allows you to solve inverse problems of the form\n","\n","$\n","\\newcommand{\\prox}{{\\operatorname{prox}}}\n","\\newcommand{\\x}{\\mathbf{x}}\n","\\newcommand{\\xtilde}{\\mathbf{\\tilde{x}}}\\newcommand{\\xhat}{\\mathbf{\\hat{x}}}\n","\\newcommand{\\y}{\\mathbf{y}}\n","\\newcommand{\\z}{\\mathbf{z}}\n","\\newcommand{\\v}{\\mathbf{v}}\n","\\newcommand{\\w}{\\mathbf{w}}\n","\\xhat = \\arg\\min_\\x F(\\x) + \\gamma G(\\x)$\n","\n","or equivalently for $\\gamma = \\sigma^2/\\alpha$\n","\n","$\\xhat = \\arg\\min_\\x \\alpha F(\\x) + \\sigma^2 G(\\x)$\n","\n","\n","where\n","* $F$ is a ($\\mu$-strongly) convex data-fitting term, and\n","* $G$ is a possibly non-convex regularisation term, which is not necessarily known explicitly. We only need to know its associated proximal operator\n","$$\\prox_{\\sigma^2 G}(\\x) \\approx D_\\sigma(\\x),$$ which is well approximated by the DnCNN denoiser.\n","\n","The code will compute the Plug & Play ADMM algorithm to find $\\xhat$.\n","\n","* $ x^{k+1} = \\prox_{\\sigma^2 G}(y^k - u^k) $\n","* $ y^{k+1} = \\prox_{\\alpha F}(x^{k+1} + u^k) $\n","* $ u^{k+1} = u^k + x^{k+1} - y^{k+1} $\n","\n","According to Theorem 2 and Corolary 3 this algorithm will converge as long as the residual operator $D_\\sigma - I$ is $L$-Lipschitz (or $L$-contractive) with $L<1$, and the $\\alpha$ parameter satisfies\n","\n","$$ \\alpha \\mu > \\frac{L}{1+L - 2L^2} $$"],"metadata":{"id":"BucDBuhzJERh"}},{"cell_type":"markdown","metadata":{"id":"IoeeJWDX85YX"},"source":["### Proximal operator for Gaussian denoising\n","\n","For Gaussian denoising the data fitting term writes\n","\n","$ F(\\x) = \\frac{1}{2 s^2}\\| \\x - \\y \\|^2 $\n","\n","its proximal operator admits a closed form:\n","\n","$ \\prox_{\\alpha F}(\\x) = \\arg\\min_\\v \\frac12 \\|\\v - \\x \\|^2 + \\alpha F(\\v)\n","= \\frac{\\x + \\y * \\alpha / s^2}{1+\\alpha/s^2}$\n","\n",">_**QUESTION 4**: Complete the following code to compute $\\prox_{\\alpha F}$ operator for Gaussian denoising._"]},{"cell_type":"code","metadata":{"id":"hZxl5iCOQGT5"},"source":["#%% ---- define problem-specific proxF ----\n","\n","def prox_datafit_gaussian_denoising(x, y, alpha, s):\n","    \"\"\"\n","    Proximal Operator for Gaussian denoising:\n","\n","    f(x) = || x - y ||^2 / (2 s^2)\n","\n","    prox_{alpha f} (x) = (x + y*alpha/s^2)/(1+alpha/s^2)\n","\n","    Parameters:\n","        :x - the argument to the proximal operator.\n","        :y - the noisy observation (flattened).\n","        :opts - the kwargs for hyperparameters.\n","            :alpha - the value of alpha.\n","            :s - the standard deviation of the gaussian noise in y.\n","    \"\"\"\n","    a = alpha/(s**2)\n","    # v = ... # COMPLETE this code\n","    return v\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyjMlYDa-Tip"},"source":["### PnP DRS denoising experiment with $s = \\sigma$\n","\n","Now use the provided PnP DRS algorithm to denoise an image with noise variance $s^2 = \\sigma^2$.\n","\n","In this case you should use the regularization parameter $\\gamma = 1$."]},{"cell_type":"code","metadata":{"id":"4wnl9RyPAf2g","cellView":"form"},"source":["# @title set parameters & run denoisig experiment\n","\n","sigma255=40 # @param [5, 15, 40] {type:\"raw\"}\n","gamma=1.0 # @param {type:\"number\"}\n","maxitr=20 # @param {type:\"slider\", min:10, max:100, step:10}\n","\n","# ---- Regularization model prox_{sigma^2 G} = denoise ----\n","# noise level\n","#sigma255 = 40             # for uint8 images\n","sigma1 = sigma255/255.0   # for images normalized in [0,1]\n","# load pretrained denoiser NN model into Denoiser object\n","model_type = \"RealSN_DnCNN\"     # Alternatives: DnCNN/ SimpleCNN / RealSN_DnCNN / RealSN_SimpleCNN\n","sigma = sigma255                # Alternatives: 5, 15, 40\n","cuda = True                     # If true use pytorch cuda optimisations\n","rescale = False                 # If true rescale [min,max] to [0,1] before applying denoiser\n","Denoiser = PyTorchDenoiser(model_type,sigma,cuda,rescale)\n","# define denoise(x) function (as a call to the Denoiser object)\n","denoise = lambda x : Denoiser.denoise(x)\n","\n","# ADMM / DRS parameters\n","#gamma = 1.0                       # Regularization parameter\n","alpha = sigma1**2/gamma\n","#maxitr = 20                     # ADMM max number of iterations\n","# Monitors\n","verbose = 1                     # display debugging messages\n","from optim import drs, GaussianMonitor\n","monitor = GaussianMonitor(u) # ground truth is only used\n","                             # to track PSNR during iterations\n","\n","# Set noise variance s^2 equal to denoiser variance sigma^2\n","s = sigma1\n","\n","# Datta fitting : prox_{alpha F}\n","proxF = lambda x : prox_datafit_gaussian_denoising(x, utilde, alpha, s)\n","\n","\n","\n","# optional PnP DRS/ADMM parameters\n","opts = dict(maxitr=maxitr,\n","            verbose=verbose,\n","            monitor=monitor)\n","\n","\n","# ---- simulate noisy image ----\n","np.random.seed(42) # Use always the same seed for reproducibility\n","s = sigma1\n","noise = np.random.normal(loc=0, scale=s, size=(u.shape))\n","utilde = u + noise\n","\n","\n","# Start the algorithm\n","#start = time.time()\n","init = utilde\n","#init = np.random.normal(loc=0.5, scale=0.5, size=(noisy.shape))\n","out = drs(proxF,denoise,init,**opts)\n","#out = admm(proxF,denoise,init,**opts)\n","#end = time.time()\n","#print(end-start)\n","\n","#%% ---- plot result ----\n","fig = plt.figure(figsize = (32,32))\n","ax1 = fig.add_subplot(2,2,1)\n","ax1.imshow(u, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('Original image', fontsize = 30)\n","\n","ax2 = fig.add_subplot(2,2,2)\n","ax2.imshow(utilde, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('Noisy image (PSNR ={0:.2f})'.format(psnr(u, utilde)), fontsize = 30)\n","\n","ax3 = fig.add_subplot(2,2,3)\n","ax3.imshow(out, cmap ='gray', vmin= 0, vmax=1)\n","ax3.set_title(r'Restored image, (PSNR ={0:.2f}, niter={1})'.format(psnr(u, out),maxitr), fontsize = 30)\n","\n","ax4 = fig.add_subplot(2,2,4)\n","p = ax4.imshow(out-u, cmap ='gray')\n","plt.colorbar(p,orientation='vertical')\n","_ = ax4.set_title(r'Error, (PSNR ={0:.2f})'.format(psnr(u, out)), fontsize = 30)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FkW0Y24GaHS"},"source":["### PnP DRS denoising experiment with $s \\neq \\sigma$\n","\n","Now use the provided PnP DRS algorithm to denoise an image with noise variance $s^2 \\neq \\sigma^2$.\n","\n","In this case the regularization parameter $\\gamma$ needs to be adapted to find the optimal value.\n","\n",">_**QUESTION 5:** Take $s=30$ and use the same strategy as for TV denoising to search for the optimal $\\gamma$ for $\\sigma=5, 15, 40$. Make a table with the optimal values of $\\gamma$ for each $\\sigma$ and the corresponding PSNRs._\n","1. _What is the maximum value of $\\gamma$ that ensures convergence?_\n","2. _Is convergence guaranteed in all cases? Which ones?_\n","3. _Which value of $\\sigma$ provides the best results? Can you explain why?_\n","\n","A partial answer to the convergence analysis is provided in the cell below\n","\n",">_**QUESTION 6:** Compare the results obtained with RealSN_DnCNN and with DnCNN, TV and BM3D. Which one provides the best performance? Can you explain why?_\n","\n",">_**QUESTION 7:** Test different initialisations, including 0 and random images. How important is the initialization to obtain a good result? Is this consistent with the convergence results seen in the Lecture ?_\n","\n","\n","| $\\sigma$ | $\\gamma$ | PSNR |\n","| -------- | -------- | ---- |\n","| 40 | 1.36 | 22.91 dB |\n"]},{"cell_type":"code","metadata":{"id":"5X6PN8hYfdZn"},"source":["# u = normalized input image (defined above)\n","\n","# ---- Degradation model F (white gaussian noise) ----\n","s = 30.0\n","s1 = s / (maxval-minval)\n","np.random.seed(42)\n","noise = np.random.normal(loc=0, scale=s1, size=(u.shape))\n","utilde = u + noise\n","\n","\n","# ---- Regularization model prox_{sigma^2 G} = denoise ----\n","# Denoiser noise level - Alternatives: 5, 15, 40\n","sigma255 = 5              # for uint8 images\n","sigma1 = sigma255/255.0   # for images normalized in [0,1]\n","# load pretrained denoiser NN model into Denoiser object\n","model_type = \"RealSN_DnCNN\"     # Alternatives: DnCNN/ SimpleCNN / RealSN_DnCNN / RealSN_SimpleCNN\n","sigma = sigma255                # Alternatives: 5, 15, 40\n","cuda = True                     # If true use pytorch cuda optimisations\n","rescale = False                 # If true rescale [min,max] to [0,1] before applying denoiser\n","# pytorch deoiser\n","Denoiser = PyTorchDenoiser(model_type,sigma,cuda,rescale)\n","# define denoise(x) function (as a call to the Denoiser object)\n","denoise = lambda x : Denoiser.denoise(x)\n","\n","\n","\n","# ADMM / DRS parameters\n","gamma = sigma1/s1                   # Regularization parameter\n","alpha = sigma1**2/gamma\n","maxitr = 100                    # ADMM max number of iterations\n","# Monitors\n","verbose = 0                     # display debugging messages\n","\n","proxF = lambda x : prox_datafit_gaussian_denoising(x, utilde, alpha, s1)\n","\n","\n","# COMPLETE THE CODE\n","# find optimal gamma\n","# compute out = result of PnP ADMM denoising\n","# repeat for sigma255 = 40, 15, 5\n","\n","# begin ANSWER\n","# ...\n","# end ANSWER\n","\n","# ---- optional ADMM parameters ----\n","monitor = GaussianMonitor(u)\n","opts = dict(maxitr=maxitr,\n","            verbose=1,\n","            monitor=monitor)\n","\n","#%% ---- plug and play !!!! ----\n","out = drs(proxF,denoise,utilde,**opts)\n","#out = admm(proxF,denoise,utilde,**opts)\n","\n","\n","#%% ---- plot result ----\n","fig = plt.figure(figsize = (32,32))\n","ax1 = fig.add_subplot(2,2,1)\n","ax1.imshow(u, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('Original image', fontsize = 30)\n","\n","ax2 = fig.add_subplot(2,2,2)\n","ax2.imshow(utilde, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('Noisy image (PSNR ={0:.2f})'.format(psnr(u, utilde)), fontsize = 30)\n","\n","ax3 = fig.add_subplot(2,2,3)\n","ax3.imshow(out, cmap ='gray', vmin= 0, vmax=1)\n","ax3.set_title(r'Restored image, (PSNR ={0:.2f}, niter={1})'.format(psnr(u, out),maxitr), fontsize = 30)\n","\n","ax4 = fig.add_subplot(2,2,4)\n","p = ax4.imshow(out-u, cmap ='gray')\n","plt.colorbar(p,orientation='vertical')\n","_ = ax4.set_title(r'Error, (PSNR ={0:.2f})'.format(psnr(u, out)), fontsize = 30)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Check convergence conditions"],"metadata":{"id":"RggtEIDhYqTd"}},{"cell_type":"markdown","metadata":{"id":"bHj3lalgyH3n"},"source":["\n","The PnP-ADMM algorithm for minimizing $\\alpha F(x) + \\sigma^2 G(x)$ is guaranteed to converge to a fixed point as long as:\n","* $F$ is $\\mu$-strongly convex\n","* $Lip(I-D) = L < 1$ where $D=\\prox_{\\sigma^2 G}$ is the denoiser\n","* The regularization parameter $\\gamma = \\sigma^2/\\alpha$ satisfies\n","$$ \\gamma < \\mu \\sigma^2 \\frac{1+L - 2 L^2}{L}$$\n","\n","For the denoising problem we have\n","$$ F(x) = \\frac{\\|x - y\\|^2}{2s^2} $$\n","\n","**Find the maximal value of $\\mu$ such that $F$ is $\\mu$ strongly convex**\n","\n","$F$ is $\\mu$-strongly convex iff $\\nabla^2 F - \\mu Id$ is positive semidefinite, i.e. iff $(\\frac{1}{s^2}-\\mu)Id$ posite semidefinite. $\\mu = \\frac{1}{s^2}$\n","\n","The _estimated_ value of $Lip(I-D) = L$ for the different denoisers is, according to Ryu(2019, Figure 1)\n","\n","| $D$   | $L$ |\n","| ----- | ---------- |\n","| RealSN_DnCNN | 0,464 |\n","| DnCNN | 0,484 |\n","| BM3D  | 1,198 |\n","\n","You can write a variant of the DRS algorithm which tracks the expansion factor at each iteration of the of the PnP-DRS algorithm.\n","Let $T$ be the operator that computes one iteration of PnP-DRS.\n","\n","$$ u_{k+1} = T(u_k) $$\n","\n","The algorithm converges to a fixed point $u^*$ if $T(u^*)=u^*$.\n","This convergence is ensures as long as $T$ is nonexpansive meaning that\n","\n","$$ \\|T(x) - T(y)\\| \\leq \\varphi \\|x-y\\| \\quad \\forall x, y \\quad \\text{ and } \\varphi <1 $$\n","\n","The expansion factor $\\varphi$ can be estimated as the maximal value of\n","\n","$$ \\varphi_k = \\|T(x_k) - T(x_{k-1}) \\| / \\|x_k - x_{k-1}\\| $$\n","\n","$x_k = T(x_{k-1})$\n","\n","complete the code below to output an array factors with these estimates\n","\n"]},{"cell_type":"code","metadata":{"id":"VgR3W2HIyGQ8"},"source":["\n","def drs2(proxF,proxG,init,**opts):\n","    \"\"\"\n","    Douglas Rachford Splitting (notation differs from Ryu's paper as follows)\n","    zk     = x_old + u_old\n","    xk+1/2 = v\n","    xk+1   = x\n","    This variant tracks the contraction factors at each iteration\n","    \"\"\"\n","\n","    \"\"\" Process parameters. \"\"\"\n","    maxitr = opts.get('maxitr', 50)\n","    verbose = opts.get('verbose', 1)\n","    monitor  = opts.get('monitor', None)  # debugging messages\n","\n","\n","    \"\"\" Initialization. \"\"\"\n","\n","    n = init.size  # == np.prod(init.shape), i.e. number of elements in ndarray\n","\n","    x = np.copy(init)\n","    v = np.zeros_like(init, dtype=np.float64)\n","    u = np.zeros_like(init, dtype=np.float64)\n","    x_old = np.zeros_like(init, dtype=np.float64)\n","\n","    factors = np.zeros([maxitr-1], dtype=np.float64)\n","\n","    \"\"\" Main loop. \"\"\"\n","\n","    for i in range(maxitr):\n","\n","        # record the variables in the current iteration\n","        x_older = np.copy(x_old)\n","        x_old = np.copy(x)\n","        v_old = np.copy(v)\n","        u_old = np.copy(u)\n","\n","        \"\"\" proximal step. \"\"\"\n","\n","        v = proxF(x+u)\n","        # prox_poisson_datafit(x+u,noisy,lam)\n","\n","        \"\"\" denoising step. \"\"\"\n","\n","        xtilde = np.copy(2*v - x_old - u_old)\n","        #x = denoiser(np.reshape(xtilde, (m,n)))\n","        #x = np.reshape(x, -1)\n","        x = proxG(xtilde)\n","\n","        \"\"\" dual update \"\"\"\n","\n","        u = np.copy(u_old + x_old - v)\n","\n","        \"\"\" Monitors \"\"\"\n","\n","        if verbose and not (monitor is None) :\n","            monitor.drs_iter(i,x,v,u,x_old,v_old,u_old)\n","\n","        if (i>0):\n","          factors[i-1]= np.linalg.norm(x-x_old)/np.linalg.norm(x_old-x_older)\n","\n","    \"\"\" Get restored image. \"\"\"\n","    #x = np.reshape((x) , (m, n))\n","    return x , factors\n","\n","\n","\n","\n","# u = normalized input image (defined above)\n","\n","# ---- Degradation model F (white gaussian noise) ----\n","s = 30.0\n","s1 = s / (maxval-minval)\n","np.random.seed(42)\n","noise = np.random.normal(loc=0, scale=s1, size=(u.shape))\n","utilde = u + noise\n","\n","\n","# ---- Regularization model prox_{sigma^2 G} = denoise ----\n","# noise level\n","sigma255 = 15             # for uint8 images\n","sigma1 = sigma255/255.0   # for images normalized in [0,1]\n","# load pretrained denoiser NN model into Denoiser object\n","model_type = \"RealSN_DnCNN\"     # Alternatives: DnCNN/ SimpleCNN / RealSN_DnCNN / RealSN_SimpleCNN\n","#model_type = \"DnCNN\"     # Alternatives: DnCNN/ SimpleCNN / RealSN_DnCNN / RealSN_SimpleCNN\n","sigma = sigma255                # Alternatives: 5, 15, 40\n","cuda = True                     # If true use pytorch cuda optimisations\n","rescale = False                 # If true rescale [min,max] to [0,1] before applying denoiser\n","# Pytorch denoiser\n","Denoiser = PyTorchDenoiser(model_type,sigma,cuda,rescale)\n","# BM3D denoiser\n","# Denoiser = BM3DDenoiser(sigma255)\n","# define denoise(x) function (as a call to the Denoiser object)\n","denoise = lambda x : Denoiser.denoise(x)\n","\n","\n","# ADMM / DRS parameters\n","gamma0 = sigma1/s1                   # Regularization parameter\n","alpha = sigma1**2/gamma\n","maxitr = 20                    # ADMM max number of iterations\n","# Monitors\n","verbose = 0                     # display debugging messages\n","\n","proxF = lambda x : prox_datafit_gaussian_denoising(x, noisy, alpha, s1)\n","\n","\n","# Maximal (theoretical) gamma\n","mu = 1.0/(s1**2)\n","eps = 0.464 # RealSN_DnCNN\n","#eps = 1.198 # BM3D\n","gamma_max = mu*(sigma1**2)*(1.0+eps-2*(eps**2))/eps\n","\n","# geometric mean\n","gmean = lambda a: np.exp(np.mean(np.log(a)))\n","\n","init = utilde\n","print('Theoretical gamma max = {:.3e}'.format(gamma_max))\n","# test a few gammas around the approximately optimal gamma\n","for factor in np.geomspace(1/4,16,num=10):\n","  gamma = gamma0 * factor\n","  alpha = sigma1**2/gamma\n","  opts = dict(maxitr=maxitr, verbose=0, monitor=None)\n","  # Datta fitting : prox_{alpha F}\n","  proxF = lambda x : prox_datafit_gaussian_denoising(x, utilde, alpha, s1)\n","  out,factors = drs2(proxF,denoise,init,**opts)\n","  res = rmse(out,utilde)\n","  error = rmse(out,u)\n","  gamma = gamma*np.exp(rho*(s1*beta-res))\n","  print('residual/sigma = {:.3f}\\t rmse = {:.3e}\\t gamma = {:.3e}\\t max eps = {:.2f}\\t mean eps = {:.2f}'.format((res/s1),error,gamma,np.max(factors),gmean(factors)))\n","  stop_cond = (np.abs(s1*beta-res)<s1*0.005)\n","  if stop_cond:\n","    break\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adXHw-AUq2ez"},"source":["### PnP ADMM algorithm\n","\n",">_**QUESTION 8:**_\n",">\n",">_a) write the ADMM algorithm to solve $\\xhat = \\arg\\min_\\x F(\\x) + G(\\x)$ given the proximal operators of $F$ and $G$. (You can use the provided Douglas Rachford algorithm, which is very similar and equivalent to ADMM, or the code squeleton below)._\n",">\n",">_b) A renaming of variables with a reordering of the three updates allows to show the equivalence of ADMM and DRS. Prove this equivalence._\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OMo9DlBJrdqN"},"source":["def admm(proxF,proxG,init,**opts):\n","    \"\"\"\n","    ADMM - min_x F(x) + G(x) by ADMM splitting\n","    Inputs:\n","        :proxF      Proximal operator of F\n","        :proxG      Proximal operator of G\n","        :init       Initial value of x\n","    Optional inputs\n","        :maxitr     default 50\n","        :verbose    default 1 (show messages)\n","        :monitor    default None\n","    Outputs\n","        :x          minimizer\n","    \"\"\"\n","\n","    \"\"\" Process parameters. \"\"\"\n","    maxitr = opts.get('maxitr', 50)\n","    verbose = opts.get('verbose', 1)\n","    monitor  = opts.get('monitor', None)  # debugging messages\n","\n","\n","    \"\"\" Initialization. \"\"\"\n","\n","    n = init.size  # == np.prod(init.shape), i.e. number of elements in ndarray\n","\n","    y = np.copy(init)\n","    x = np.zeros_like(init, dtype=np.float64)\n","    u = np.zeros_like(init, dtype=np.float64)\n","\n","    \"\"\" Main loop. \"\"\"\n","\n","    for i in range(maxitr):\n","\n","        # record the variables in the current iteration\n","        x_old = np.copy(x)\n","        y_old = np.copy(y)\n","        u_old = np.copy(u)\n","\n","        \"\"\" denoising step. \"\"\"\n","\n","        # COMPLETE THIS CODE\n","\n","        \"\"\" proximal step. \"\"\"\n","\n","        # COMPLETE THIS CODE\n","\n","        \"\"\" dual update \"\"\"\n","\n","        # COMPLETE THIS CODE\n","\n","        \"\"\" Monitors \"\"\"\n","\n","        if verbose and not (monitor is None) :\n","            monitor.admm_iter(i,x,y,u,x_old,y_old,u_old)\n","\n","    \"\"\" Get restored image. \"\"\"\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQFI-EGvAkQE"},"source":["## Missing Pixels\n","\n","Another common inverse problem is missing pixels where the observation model is\n","\n","$$ y_i = m_i x_i + n_i$$\n","\n","where $m_i \\sim \\operatorname{Ber}(p)$ is known\n","and $n_i \\sim \\mathcal{N}(0,s^2)$ is unknown.\n","\n","For a random selection of $p=10\\%$ of the pixels $m_i=1$ and the pixel value $x_i$ is corrupted by white gaussian noise with a small variance $s^2$.\n","For the remaining $90\\%$ of the pixels $x_i$ is completely unknown.\n","\n",">_**QUESTION 9:** Write the potential and the proximal operator for this degradation (it has a closed form), and solve the inverse problem using PnP-ADMM as before._\n","\n"]},{"cell_type":"code","metadata":{"id":"M-YoYKN5ApbL"},"source":["#%% ---- define problem-specific proxF ----\n","\n","def prox_datafit_inpainting(x, y, mask, alpha, s):\n","    \"\"\"\n","    Proximal Operator for Gaussian denoising:\n","\n","    f(x) = || M*x - y ||^2 / (2 s^2)\n","\n","    prox_{alpha f} (x[i]) = (x[i]*s^2 + y[i]*alpha)/(s^2+alpha) if M[i]==1\n","                          = x[i]                                if M[i]==0\n","\n","    Parameters:\n","        :x - the argument to the proximal operator.\n","        :y - the noisy observation.\n","        :mask - binary image of the same size as x\n","        :opts - the kwargs for hyperparameters.\n","            :alpha - the value of alpha.\n","            :s - the standard deviation of the gaussian noise in y.\n","    \"\"\"\n","    s2 = (s**2)\n","    # v = ... # COMPLETE this code\n","    return v\n","\n","\n","#%% ---- define problem-specific monitor ----\n","import time\n","\n","#from skimage.measure import compare_psnr\n","from utils.psnr import rmse, psnr\n","\n","class InpaintingMonitor:\n","    def __init__(self,clean,mask,sigma):\n","        self.clean = clean\n","        self.mask = mask\n","        self.sigma = sigma\n","    def drs_iter(self,i,x,v,u,x_old,v_old,u_old):\n","        fpr = np.sqrt(np.sum(np.square((x + u - x_old - u_old)))/x.size)\n","        error_in_mask = np.sqrt(np.sum(np.square(mask*(x-self.clean)))/(np.sum(mask)))\n","        psnrs = psnr(x, self.clean, 1.0)\n","        if i%50==-1%50:\n","            print(\"i = {},\\t psnr = {:.2f}, fpr = {:.2f}, error in mask = {:.2f}\".format(i+1, psnrs, fpr, error_in_mask))\n","    admm_iter = drs_iter\n","monitor = InpaintingMonitor(im,mask,sigma1)\n","\n","\n","# COMPLETE THIS CODE\n","# load image u\n","# simulate degraded image utilde (noise with standard deviation $s = 1/255$)\n","# restore u with PnP ADMM seeking for the optimal \\gamma\n","\n","\n","#%% ---- plot result ----\n","fig = plt.figure(figsize = (32,32))\n","ax1 = fig.add_subplot(2,2,1)\n","ax1.imshow(u, cmap = 'gray', vmin=0, vmax = 1)\n","ax1.set_title('Original image', fontsize = 30)\n","\n","ax2 = fig.add_subplot(2,2,2)\n","ax2.imshow(utilde, cmap = 'gray', vmin=0, vmax = 1)\n","ax2.set_title('Degraded image (PSNR ={0:.2f})'.format(psnr(u, utilde)), fontsize = 30)\n","\n","ax3 = fig.add_subplot(2,2,3)\n","ax3.imshow(out, cmap ='gray', vmin= 0, vmax=1)\n","ax3.set_title('Restored image, (PSNR ={0:.2f}, niter={1})'.format(psnr(u, out),maxitr), fontsize = 30)\n","\n","ax4 = fig.add_subplot(2,2,4)\n","p = ax4.imshow(out-u, cmap ='gray')\n","plt.colorbar(p,orientation='vertical')\n","_ = ax4.set_title('Error, (PSNR ={0:.2f})'.format(psnr(u, out)), fontsize = 30)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWxO_i_1T7pL"},"source":["# BONUS (optional)"]},{"cell_type":"markdown","source":["## BONUS: Deblurring (optional)\n"],"metadata":{"id":"XDluF4vAuYbB"}},{"cell_type":"markdown","metadata":{"id":"SPBmEO6FwC0l"},"source":["Now you shall use your code to solve a new inverse problem: Image deblurring with Gaussian noise\n","\n","In this case $A(\\x) = \\x*\\mathbf{h} = (\\sum_j x_{i-j} h_{j})_i$ where $\\mathbf{h}$ is a known convolution kernel for instance a gaussian kernel or a box filter, and\n","\n","$$F(\\x)=\\|A(\\x)-\\y\\|^2\\frac{1}{2s^2}$$\n","\n","> _**QUESTION 10 (optional)** Add the code needed to simulate this new degradation._\n","\n","In order to solve this inverse problem within the PnP ADMM framework you need to write a function that implements the proximal operator of the data-fitting term $F$  multiplied by $\\alpha$  namely\n","\n","$$ \\prox_{\\alpha F}(\\z) = \\arg\\min_\\x \\alpha F(\\x) + \\frac{1}{2} \\| \\x - \\z \\|_2^2 $$\n","\n","> _**QUESTION 11 (optional)**: Show that in this case this minimization can be solved in closed form (in the Fourier domain), and use this closed form to write the proximal operator of the data fitting term._\n","\n","Provide your reconstruction results for deblurring with a Gaussian kernel with standard deviation $5$ and white Gaussian noise with $s=5$.\n","\n","$\\newcommand{\\k}{\\mathbf{k}}$\n","$\\newcommand{\\khat}{\\hat{\\k}}$\n","$\\newcommand{\\hhat}{\\hat{\\mathbf{h}}}$\n","\n","> _**QUESTION 12 (optional)**: A simple way to provide a good initialization is to compute the Wiener filter for some reasonable prior._\n","\n","Let $\\xhat$ be the discrete Fourier transform of $\\x$.\n","\n","As a simple prior you can assume that the Fourier coefficients are independent and that they follow  a zero-mean Gaussian distribution $|\\xhat_m| \\sim N(0,F(|m|)^2)$ where $F(r)$ is a decreasing function of the frequency modulus.\n","\n","This model is commonly used $F(r) = C (a+r)^{-p}$ where $a\\approx 1$, $p\\in[1,2]$. Find the values of $C, a$ and $p$ that best fit your image and use this as a prior for $\\x$.\n","\n","Under this prior the Wiener filter (the linear estimator which minimizes the quadratic risk) can be shown to be obtained as the convolution $\\k*\\xtilde$ where the deconvolution kernel is computed in the Fourier domain as:\n","\n","$\\khat_m = \\frac{\\overline{\\hhat}_m|F(|m|)^2}{|\\hhat_m|^2||F(|m|)|^2+\\sigma^2}$\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qyK02rm7iC2Y"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DiMZBchW2Ul2"},"source":["## BONUS: PnP ISTA (optional)\n","\n","\n","The Plug & Play ISTA algorithm presented in the lecture (Xu et al 2020) seeks to solve\n","\n","$$ \\hat{x} = \\arg\\min_x F(x) + \\gamma G(x) $$\n","\n","of equivalently (for $\\gamma = \\sigma^2/\\alpha$):\n","\n","$$ \\hat{x} = \\arg\\min_x \\alpha F(x) + \\sigma^2 G(x) $$\n","\n","by running the following splitting scheme\n","\n","* $v_k = x_k - \\nabla(\\alpha F)(x_k)$         (gradient descent step)\n","* $x_{k+1} = \\prox_{\\sigma^2 G}(v_k) = D_{\\delta\\sigma^2}(v_k)$   (proximal descent step)\n","\n","Convergence is ensured as long as\n","\n","* $D_{\\sigma^2}$ is an MMSE denoiser and\n","* $\\alpha < 1 / Lip(\\nabla F)$\n","\n","\n","\n","> _**QUESTION 13 (optional):** Complete the code below to compute the pnp ista algorithm find the condition on $\\gamma$ to garantee the convergence of PnP ISTA find the optimal $\\gamma$ for $\\sigma = 40, 15, 5$ does it satisfy the convergence condition ? For which $\\sigma$ do you obtain the best reconstruction ?_\n","\n"]},{"cell_type":"code","metadata":{"id":"mWthsJwg2Tup"},"source":["def ista(gradF,proxG,init,**opts):\n","    \"\"\"\n","    ISTA - min_x F(x) + G(x) by Proximal-gradient splitting\n","\n","    v_k     = x_k - gradF(x_k)\n","    x_{k+1} = proxG(v_k)\n","\n","    Convergence Condition: delta = 1 < 1/ Lip(gradF)\n","\n","    Inputs:\n","        :gradF      gradient of F\n","        :proxG      Proximal operator of G\n","        :init       Initial value of x\n","        :delta      gradient descent step (default 1)\n","    Optional inputs\n","        :maxitr     default 50\n","        :verbose    default 1 (show messages)\n","    Outputs\n","        :x          minimizer\n","    \"\"\"\n","\n","    \"\"\" Process parameters. \"\"\"\n","    maxitr = opts.get('maxitr', 50)\n","    verbose = opts.get('verbose', 1)\n","    monitor  = opts.get('monitor', None)  # debugging messages\n","\n","\n","    \"\"\" Initialization. \"\"\"\n","\n","    n = init.size  # == np.prod(init.shape), i.e. number of elements in ndarray\n","\n","    x = np.copy(init)\n","    v = np.zeros_like(init, dtype=np.float64)\n","\n","    \"\"\" Main loop. \"\"\"\n","\n","    for i in range(maxitr):\n","\n","        # record the variables in the current iteration\n","        x_old = np.copy(x)\n","        v_old = np.copy(v)\n","\n","        \"\"\" gradient descent step. \"\"\"\n","\n","        v = ... # COMPLETE\n","\n","        \"\"\" proximal descent step. \"\"\"\n","\n","        x = ... # COMPLETE\n","\n","        \"\"\" Monitors \"\"\"\n","\n","        if verbose :\n","            print(\"i = {},\\t fpr = {}\".format(i+1, np.linalg.norm(x-x_old)/np.linalg.norm(x)))\n","\n","\n","    \"\"\" Get restored image. \"\"\"\n","    return x\n","\n","\n","def grad_datafit_gaussian_denoising(x, y, alpha=1.0, s=1.0):\n","    \"\"\"\n","    Gradient Operator for Gaussian denoising:\n","\n","    f(x) = || x - y ||^2 / (2 s^2)\n","\n","    grad {alpha f} (x) = (x - y)*alpha/s^2\n","\n","    Parameters:\n","        :x - the argument to the proximal operator.\n","        :y - the noisy observation (flattened).\n","        :opts - the kwargs for hyperparameters.\n","            :alpha - the value of alpha.\n","            :s - the standard deviation of the gaussian noise in y.\n","    \"\"\"\n","    v = np.copy((x-y)*alpha/(s**2))\n","    return v\n","\n","#gradF = lambda x : grad_datafit_gaussian_denoising(x, noisy, alpha, delta, sigma_noise)\n"],"execution_count":null,"outputs":[]}]}