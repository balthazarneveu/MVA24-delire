{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2\n",
    "- Master MVA ENS-Paris Saclay\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default denoisers\n",
    "\n",
    "Evaluated on observation with additive white gaussian noise. $sigma_{255}=40$\n",
    "| $u$ = Original / $\\tilde{u}$ = Noisy | \n",
    "|:--:|\n",
    "| ![](figures/in_out.png) |\n",
    "|Denoised with standard, non tuned denoisers|\n",
    "| ![](figures/basic_default_denoisers.png) |\n",
    "\n",
    "\n",
    "- TV-Denoising has not been tuned (and it can do better as we'll see later).\n",
    "- DN-CNN trained with a noise level $sigma_{255}=40$ is used here - so the denoising network is expected to work : there's no gap between its training conditions and evaluation conditions.\n",
    "- BM3D is used with the right noise input level (`BM3DDenoiser(40)`), it is a non machine learning based algorithm. \n",
    "\n",
    "| BM3D with different tunings.|\n",
    "|:--:|\n",
    "| ![bm3d_sensitivity](figures/BM3D_tuning.png) |\n",
    "|On the left, BM3D would expect a not too noisy image and so leave a lot of residual noise. On the right side, BM3D runs on an image where that was more noisy that reality so it ends up oversmoothing|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal TV Denoiser\n",
    "\n",
    "**Auto tuning TV denoiser**\n",
    "\n",
    "### Question 1\n",
    "\n",
    "\n",
    "```python\n",
    "for i in range(1,100):\n",
    "    uTV = TVDenoiser(lamb=TVlamb,niter=100).denoise(utilde)\n",
    "    res = rmse(uTV,utilde)\n",
    "    # Estimated residual between the noisy image and the denoised image\n",
    "    error = rmse(uTV,u)\n",
    "    # True error between the prediction and the clean image\n",
    "    non_gaussianity_error = sigma1*beta-res\n",
    "    # if the error was gaussian with the right amount of noise, this should be 0\n",
    "    correction_factor = rho*non_gaussianity_error\n",
    "    TVlamb *= np.exp(correction_factor)\n",
    "    stop_cond= np.fabs(non_gaussianity_error) < 0.01 * (sigma1*beta)\n",
    "    if stop_cond:\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Initial tuning : $\\lambda^{TV}_{\\text{n=0}} = 0.024 =  \\sigma^2$\n",
    "\n",
    "$\\rightarrow  \\lambda^{TV}_{n=6} \\approx 0.125 \\approx 5.1 \\sigma^ 2 $ to achieve a 1% tolerance (`residual/sigma = 0.955`)\n",
    "\n",
    "Although simple, the proposed automatic tuning technique seems to be quite robust to initial values. Even if you start with a high value for $\\lambda$ like $10 \\sigma^{2}$ where the initial images are oversmoothed, the tuning parameter correction converges to the right value.\n",
    "\n",
    "![](figures/tuning_parameter_convergence.png)\n",
    "\n",
    "\n",
    "\n",
    "![](figures/single_standalone_awgn_denoisers.png)\n",
    "\n",
    "|Denoiser | RMSE(normalized gray levels) | PSNR(dB) |\n",
    "|:---:|:---:|:---:|\n",
    "|Tuned TV Denoiser |  0.0630 | 24.0 dB |\n",
    "|DnCNN Denoiser | 0.0450 | 26.9 dB |\n",
    "|Real SN_DnCNN | 0.0448 | 27.0 dB |\n",
    "\n",
    "`Real_SN_DnCNN` and `DnCNN` denoising have similar denoising quality, and way better than the correctly tuned TV denoiser  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "`RealSN_DnCNN` has the same exact architecture as `DnCNN` but has been trained in a very specific fashion:\n",
    "The residual satisfies the Lipschitz constraint. This will not change anything to the denoising capabilities of the network under AWGN (this is what we observed in the previous table in question 2., additionnally the 2 denoised results look very much alike visually).\n",
    "\n",
    "The only difference is part of the training: compared to a \"classical\" MSE minimization with gradient descent the weights are modified during training so that the residual satisfies the Lipschitz constraint (Relu is 1-Lipschitz, Convolutions coefficients at each layer need to be normalized by their largest eigen value). RealSN stands for real Spectral Normalization and uses a \"tricky\" implementation (not naÃ¯vely performing SVD at each step, instead relying on an iterative power method = avoids computing SVD at every training step for all netwrok layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 \n",
    "\n",
    "$\\alpha = \\frac{\\sigma^2}{\\gamma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_datafit_gaussian_denoising(x, y, alpha, s):\n",
    "    \"\"\"\n",
    "    Proximal Operator for Gaussian denoising:\n",
    "\n",
    "    f(x) = || x - y ||^2 / (2 s^2)\n",
    "\n",
    "    prox_{alpha f} (x) = (x + y*alpha/s^2)/(1+alpha/s^2)\n",
    "\n",
    "    Parameters:\n",
    "        :x - the argument to the proximal operator.\n",
    "        :y - the noisy observation (flattened).\n",
    "        :opts - the kwargs for hyperparameters.\n",
    "            :alpha - the value of alpha.\n",
    "            :s - the standard deviation of the gaussian noise in y.\n",
    "    \"\"\"\n",
    "    a = alpha/(s**2)\n",
    "    v = (x+y*a)/(1+a)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where $s=\\sigma$, the proximal term becomes\n",
    "$\\text{prox}_{\\alpha.F(x)}=\\frac{\\gamma x+y}{\\gamma +1}$ wich is a weighted sum of x and y (we're blending the denoised result from the Denoiser). When $\\gamma=1$ , this is just the average $\\frac{x+y}{2}$.\n",
    "\n",
    "<!-- UNROLLING ADMM\n",
    "Let's unroll the first iteration:\n",
    "Init: \n",
    "- $u^{0}=0$ zero residual\n",
    "- $y^{0} = x^{0} = \\tilde{u}$ noisy image\n",
    "\n",
    "- Step 1: (*Regularization*) - We first denoise the image.\n",
    "$$x^{1} = \\text{prox}_{\\sigma^2 G}(y^{0}-u^{0}) \\approx D_{\\sigma}(y^{0}-u^{0}) = \\text{Denoiser}_{\\sigma}(\\tilde{u})$$\n",
    "- Step 2: (*Data term*) - We'll add back a bit of noise by averageing the denoised image with the original noisy image.\n",
    "$$ y^{1} = \\text{prox}_{\\alpha F}(x^{1} + u^0, \\tilde{u}) =  \\frac{\\gamma . (x^{1} + u^0)+\\tilde{u}}{\\gamma +1} = \n",
    "\\frac{\\gamma .\\text{Denoiser}_{\\sigma}(\\tilde{u})  + \\tilde{u}}{\\gamma +1}$$ -->\n",
    "\n",
    "\n",
    "<!-- - Step 3: Estimated residual $$u_1 = \\frac{x^{1}-y^{0}}{2} =  \\frac{\\text{Denoiser}_{\\sigma}(\\tilde{u}) - \\tilde{u}}{\\gamma+1}$$ -->\n",
    "\n",
    "\n",
    "<!-- $ y^{k+1} = \\text{prox}_{\\alpha F}(x^{k+1} + u^k) $ -->\n",
    "<!-- $$\\text{prox}_{\\sigma^2 G}(x) \\approx D_{\\sigma}(x)$$ -->\n",
    "\n",
    "The $\\gamma$ parameter definitely acts as a tuning parameter for the denoiser as can be seen in the next figure. Good news is that compared to the \"classic trick\" (telling the network to process the image with a tweaked noise value), this tuning parameter actually seems to make sense. \n",
    "\n",
    "| $\\gamma=0.8$ | $\\gamma=1.0$ | $\\gamma=1.2$ |\n",
    "|:----:|:----:|:----:|\n",
    "|![](figures/drs_same_noise_gamma_0p8.png) |![](figures/drs_same_noise_gamma_1p0.png)|![](figures/drs_same_noise_gamma_1p2.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto tuning of PnP Gaussian denoising\n",
    "\n",
    "### Question 5\n",
    "| $\\sigma = 5$ | $\\sigma = 15$ | $\\sigma = 40$ |\n",
    "|:-----: |:-----: |:-----: |\n",
    "| $\\gamma=0.351$|  $\\gamma=0.5723$  | $\\gamma=0.1.3715$ | \n",
    "|PSNR=21.85dB | PSNR=23.28dB | PSNR=22.90dB |\n",
    "\n",
    "\n",
    "| $\\sigma = 5$ | $\\sigma = 15$ | $\\sigma = 40$ |\n",
    "|:-----: |:-----: |:-----: |\n",
    "|![](figures/pnp_drs_sigma=5.png) | ![](figures/pnp_drs_sigma=15.png) | ![](figures/pnp_drs_sigma=40.png) |\n",
    "\n",
    "\n",
    "The results when using PnP DRS for Gaussian Denoising with DnCNN at $\\sigma=40$ are much better than if we'd simply performed inference with the DnCNN $\\sigma=40$\n",
    "\n",
    "![](figures/DnCNN_s=30_sigma=40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet\n",
    "$\\hat{x} = argmin F(x) + \\lambda G(x) = \\alpha * F(x) + \\sigma^2 G(x)$\n",
    "\n",
    "$\\lambda = \\frac{\\sigma^2}{\\alpha}$ , $\\alpha<1$ to leave a bit of noise.\n",
    "\n",
    "- $s$ actual standard deviation in $\\tilde{u}$\n",
    "- $\\sigma$ noise standard deviation used to train the denoiser whose residual is considered as the poximal of $G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
