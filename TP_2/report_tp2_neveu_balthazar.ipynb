{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2\n",
    "- Master MVA ENS-Paris Saclay\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default denoisers\n",
    "\n",
    "Evaluated on observation with additive white gaussian noise. $sigma_{255}=40$\n",
    "| $u$ = Original / $\\tilde{u}$ = Noisy | Denoised with standard, non tuned denoisers|\n",
    "|:--:|:---:|\n",
    "| ![](figures/in_out.png) | ![](figures/basic_default_denoisers.png) |\n",
    "\n",
    "\n",
    "- TV-Denoising has not been tuned.\n",
    "- DN-CNN used here has been trained on the expected noise level (`BM3DDenoiser(40)` so the denoising network is expected to work - no gap between its training conditions and evaluation conditions)\n",
    "- BM3D is a non machine learning based algorithm. The results are \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal TV Denoiser\n",
    "\n",
    "**Auto tuning TV denoiser**\n",
    "\n",
    "### Question 1\n",
    "\n",
    "\n",
    "```python\n",
    "for i in range(1,100):\n",
    "    uTV = TVDenoiser(lamb=TVlamb,niter=100).denoise(utilde)\n",
    "    res = rmse(uTV,utilde)\n",
    "    # Estimated residual between the noisy image and the denoised image\n",
    "    error = rmse(uTV,u)\n",
    "    # True error between the prediction and the clean image\n",
    "    non_gaussianity_error = sigma1*beta-res\n",
    "    # if the error was gaussian with the right amount of noise, this should be 0\n",
    "    correction_factor = rho*non_gaussianity_error\n",
    "    TVlamb *= np.exp(correction_factor)\n",
    "    stop_cond= np.fabs(non_gaussianity_error) < 0.01 * (sigma1*beta)\n",
    "    if stop_cond:\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Initial tuning : $\\lambda^{TV}_{\\text{n=0}} = 0.024 =  \\sigma^2$\n",
    "\n",
    "$\\rightarrow  \\lambda^{TV}_{n=6} \\approx 0.125 \\approx 5.1 \\sigma^ 2 $ to achieve a 1% tolerance (`residual/sigma = 0.955`)\n",
    "\n",
    "Although simple, the proposed automatic tuning technique seems to be quite robust to initial values. Even if you start with a high value for $\\lambda$ like $10 \\sigma^{2}$ where the initial images are oversmoothed, the tuning parameter correction converges to the right value.\n",
    "\n",
    "![](figures/tuning_parameter_convergence.png)\n",
    "\n",
    "\n",
    "\n",
    "![](figures/single_standalone_awgn_denoisers.png)\n",
    "\n",
    "|Denoiser | RMSE(normalized gray levels) | PSNR(dB) |\n",
    "|:---:|:---:|:---:|\n",
    "|Tuned TV Denoiser |  0.0630 | 24.0 dB |\n",
    "|DnCNN Denoiser | 0.0450 | 26.9 dB |\n",
    "|Real SN_DnCNN | 0.0448 | 27.0 dB |\n",
    "\n",
    "`Real_SN_DnCNN` and `DnCNN` denoising have similar denoising quality, and way better than the correctly tuned TV denoiser  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "`RealSN_DnCNN` has the same exact architecture as `DnCNN` but has been trained in a very specific fashion:\n",
    "The residual satisfies the Lipschitz constraint. This will not change anything to the denoising capabilities of the network under AWGN (this is what we observed in the previous table in question 2., additionnally the 2 denoised results look very much alike visually).\n",
    "\n",
    "The only difference is part of the training: compared to a \"classical\" MSE minimization with gradient descent the weights are modified during training so that the residual satisfies the Lipschitz constraint (Relu is 1-Lipschitz, Convolutions coefficients at each layer need to be normalized by their largest eigen value). RealSN stands for real Spectral Normalization and uses a \"tricky\" implementation (not naÃ¯vely performing SVD at each step, instead relying on an iterative power method = avoids computing SVD at every training step for all netwrok layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_datafit_gaussian_denoising(x, y, alpha, s):\n",
    "    \"\"\"\n",
    "    Proximal Operator for Gaussian denoising:\n",
    "\n",
    "    f(x) = || x - y ||^2 / (2 s^2)\n",
    "\n",
    "    prox_{alpha f} (x) = (x + y*alpha/s^2)/(1+alpha/s^2)\n",
    "\n",
    "    Parameters:\n",
    "        :x - the argument to the proximal operator.\n",
    "        :y - the noisy observation (flattened).\n",
    "        :opts - the kwargs for hyperparameters.\n",
    "            :alpha - the value of alpha.\n",
    "            :s - the standard deviation of the gaussian noise in y.\n",
    "    \"\"\"\n",
    "    a = alpha/(s**2)\n",
    "    v = (x+y*a)/(1+a)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto tuning of PnP Gaussian denoising\n",
    "\n",
    "### Question 5\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet\n",
    "$\\hat{x} = argmin F(x) + \\lambda G(x) = \\alpha * F(x) + \\sigma^2 G(x)$\n",
    "\n",
    "$\\lambda = \\frac{\\sigma^2}{\\alpha}$ , $\\alpha<1$ to leave a bit of noise.\n",
    "\n",
    "- $s$ actual standard deviation in $\\tilde{u}$\n",
    "- $\\sigma$ noise standard deviation used to train the denoiser whose residual is considered as the poximal of $G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
