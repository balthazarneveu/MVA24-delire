{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1\n",
    "- Master MVA ENS-Paris Saclay\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCT denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. \n",
    "#### Maximizing the likelihood\n",
    "Given $Y = X+B$ where $B \\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\mathcal{p}(X) = \\frac{e^{-\\|X\\|}}{ \\int_{-\\infty}^{\\infty} e^{-\\|X\\|}dx}$\n",
    "\n",
    "We're looking for the most likely value $x$ of $X$  given the observation $y$ of the random variable $Y$.\n",
    "\n",
    "$x^{*} = \\text{argmax}_{x}P(X=x|Y=y)$\n",
    "\n",
    "Let's first apply Bayes rule $P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(Y=y)}{P(X=x)} = \\frac{P(B=y-x|X=x)P(Y=y)}{P(X=x)}  \\propto  \\frac{e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2}}}{e^{-\\|x\\|}}P(Y=y)$\n",
    "\n",
    "Since we're searching for the argmax of this expression regarding $x$ ($y$ is constant so $P(Y=y)$ is constant too).\n",
    "\n",
    "We have $x^{*} = \\text{argmax}_{x} e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|}$\n",
    "\n",
    "*since $e^{-u}$ is a monotonic decreasing function.*\n",
    "\n",
    "$x^{*}= \\text{argmin}_{x} (-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|) $\n",
    "\n",
    "-------\n",
    "#### Finding the solution of the cost function\n",
    "Let's minimize $C(x) = -\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|$\n",
    "\n",
    "- if $x>0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} + 1$. Critical point shall statisfy $x=y - \\sigma^{2}$ \n",
    "- if $x<0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} - 1$. Critical point shall statisfy $x=y + \\sigma^{2}$ \n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "- If $ y > \\sigma^2$, then $ y - \\sigma^2 > 0$, and the critical point $ x = y - \\sigma^2$ is valid as it falls in the $ x > 0$ case.\n",
    "- If $ y < -\\sigma^2$, then $ y + \\sigma^2 < 0$, and the critical point $ x = y + \\sigma^2$ is valid as it falls in the $ x < 0$ case.\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2$, the solution might be $ x = 0$ as neither of the critical points fall within their respective ranges.\n",
    "\n",
    "\n",
    "Let's assume $y>0$\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.\n",
    "`DCT_denoise` performs a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "- The hard thesholding function can be differentiated with regard to the input $y$ but not with regard to the threshold $T=\\sigma^{2}$ unfortunately.\n",
    "- A workaround is to use **a very rough approximation** of the hard thresholding function to stay in the standard framework of torch operators: using a bias and a Relu, it is possible to perform an operation of soft thresholding.\n",
    "- Best idea is to use an differentiable approximation of the hard thresholding function. \n",
    "\n",
    "##### Approximate differentiable hard thresholding function\n",
    "\n",
    "Another idea is to approximate the hard thresholding by a function satisfying the following properties:\n",
    "-  *differentiable* with regard to the threshold (and the input obviously)\n",
    "-  *parametric*: use a temperature $\\lambda$ parameter so that when the temperature varies from $+\\infty$ and 0, the thresholding function varies between a soft threshold and a hard threshold of value $T$.\n",
    "-  Using this idea, you can use the approximate differentiable function in a deep learning standard framework and proggressively vary the temperature\n",
    "\n",
    "$$f(x) = \\text{ReLU}(x - T + T. tanh(\\frac{(x-T)}{\\lambda}))$$\n",
    "\n",
    "![](figures/thresholding_functions.png)\n",
    "\n",
    "\n",
    "```python\n",
    "# Definition of various thresholding functions\n",
    "def hard_thresholding(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Non-differentiable with regard to threshold\"\"\"\n",
    "    return torch.where(torch.abs(x) > threshold, x, torch.zeros_like(x))\n",
    "\n",
    "\n",
    "def soft_thresholding(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Differentiable with regard to threshold, does not preserve energy of input signal (biased)\"\"\"\n",
    "    return torch.nn.functional.relu(x - threshold) - torch.nn.functional.relu(-(x + threshold))\n",
    "\n",
    "\n",
    "def assym_differentiable_hard_thresholding(x: torch.Tensor, threshold: torch.Tensor, temperature: float=1) -> torch.Tensor:\n",
    "    x_offset = x - threshold\n",
    "    return torch.nn.functional.relu(x_offset + threshold*torch.tanh(x_offset/temperature))\n",
    "\n",
    "\n",
    "def differentiable_hard_thresholding(x: torch.Tensor, threshold: torch.Tensor, temperature: float=1) -> torch.Tensor:\n",
    "    \"\"\"Approximated of hard thresholding, differentiable with regard to threshold\n",
    "    When temperature is high, it is close to soft thresholding\n",
    "    When temperature is close to 0, it is close to hard thresholding\n",
    "    \"\"\"\n",
    "    return assym_differentiable_hard_thresholding(x, threshold, temperature) - assym_differentiable_hard_thresholding(-x, threshold, temperature)\n",
    "```\n",
    "\n",
    "##### Proof of concept\n",
    "We build a simple toy example where an input gaussian distribution of standard deviation 8 is hard-thresholded with a trheshold of $2.4$. \n",
    "\n",
    "![toy_example](figures/toy_example.png)\n",
    "\n",
    "The goal is to fit/learn this threshold. We'll preform Stochastic gradient descent using the Mean Square Error (LÂ²) and we'll decrease the temperature progressively.\n",
    "\n",
    "![learnable_hard_threshold](figures/learnable_threshold.png)\n",
    "\n",
    "It is doable to learn the right hard threshold using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.\n",
    "The number of significative operations (multiplications) per pixels is in $2*N^{4} $\n",
    "- $N^2$ frequencies (number of channels) multiplied by \n",
    "  - convolution kernel of size $N^2$ multipliciations.\n",
    "  - 2 because of DCT and inverse DCT.\n",
    "  - No bias addition, $N^2$ thresholdings.\n",
    "- Final normalization is negligible.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.\n",
    "Best threshold (ratio) value for the Zebre picture with AWGN $\\sigma=25$ is $2.7$.\n",
    "\n",
    "$r = \\frac{T}{\\sigma} = 2.7$\n",
    "\n",
    "![ratio_search](figures/figure_ratio_search_2_73.png)\n",
    "\n",
    "| Noisy | DCT denoised $r=2.7$| DCT denoised $r=5$|\n",
    "|:----:| :----:| :----:|\n",
    "|![](figures/zebre_noisy.png) | ![](figures/zebre_DCT_denoised_opt.png) | ![](figures/zebre_DCT_denoised.png)  |\n",
    "|  $\\sigma=25$ | Optimum DCT denoiser in the MSE sense | Oversmoothed, threshold is too high |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
