{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1\n",
    "- Master MVA ENS-Paris Saclay\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCT denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. \n",
    "#### Maximizing the likelihood\n",
    "Given $Y = X+B$ where $B \\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\mathcal{p}(X) = \\frac{e^{-\\|X\\|}}{ \\int_{-\\infty}^{\\infty} e^{-\\|X\\|}dx}$\n",
    "\n",
    "We're looking for the most likely value $x$ of $X$  given the observation $y$ of the random variable $Y$.\n",
    "\n",
    "$x^{*} = \\text{argmax}_{x}P(X=x|Y=y)$\n",
    "\n",
    "Let's first apply Bayes rule $P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(Y=y)}{P(X=x)} = \\frac{P(B=y-x|X=x)P(Y=y)}{P(X=x)}  \\propto  \\frac{e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2}}}{e^{-\\|x\\|}}P(Y=y)$\n",
    "\n",
    "Since we're searching for the argmax of this expression regarding $x$ ($y$ is constant so $P(Y=y)$ is constant too).\n",
    "\n",
    "We have $x^{*} = \\text{argmax}_{x} e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|}$\n",
    "\n",
    "*since $e^{-u}$ is a monotonic decreasing function.*\n",
    "\n",
    "$x^{*}= \\text{argmin}_{x} (-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|) = \\text{argmin}_{x} C(x)$\n",
    "\n",
    "Intuitively, the cost function\n",
    "$$C(x) = -\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|$$\n",
    "is the sum of a:\n",
    "- data fidelity term ($L^2$ loss - prediction $x$ shall look like the observation $y$, relatively to the noise level $\\sigma$)\n",
    "- a prior term on the signal ($x$ shall have a small $L1$ norm so it matches best with its prior distribution).\n",
    "\n",
    "-------\n",
    "#### Finding the solution of the cost function\n",
    "Let's minimize $C(x)$\n",
    "\n",
    "- if $x>0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} + 1$. Critical point shall statisfy $x=y - \\sigma^{2}$ \n",
    "- if $x<0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} - 1$. Critical point shall statisfy $x=y + \\sigma^{2}$ \n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "- If $ y > \\sigma^2$, then $ y - \\sigma^2 > 0$. Validity of the critical point $ x = y - \\sigma^2$ ($ x > 0$ satisfied).\n",
    "- If $ y < -\\sigma^2$, then $ y + \\sigma^2 < 0$, Validity of the critical point $ x = y + \\sigma^2$ ($ x < 0$ satisfied).\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2$, the solution might be $ x = 0$ as neither of the critical points fall within their respective ranges.\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "x^* = \\begin{cases} \n",
    "    y - \\sigma^2 \\text{ if } y > \\sigma^2 \\\\\n",
    "    y + \\sigma^2 \\text{ if } y < -\\sigma^2 \\\\\n",
    "    0 \\text{ if } -\\sigma^2 \\leq y \\leq \\sigma^2\n",
    "    \\end{cases}\n",
    "$\n",
    " \n",
    "This is the **soft thresholding** function.\n",
    "![soft thresholding](figures/soft_threshold.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.\n",
    "`DCT_denoise` performs a **hard thresholding** in the frequency domain \n",
    "- applies the DCT transform which is a convolution:\n",
    "  -  by $N*N$ *(number of frequencies)* kernels of size $(N,N)$ .\n",
    "  -  This is performed in the code by `nn.Conv2d` with frozen weights (non learnable).\n",
    "  -  To be an exact DCT transform, one should use a stride of $N$ (shift by a block every time) \n",
    "- performs a **hard thresholding** ($\\neq$ soft thresholding) not in the spatial domain but rather in the frequency domain.\n",
    "- applies the inverse DCT transform to go back to the spatial domain. \n",
    "  - This is performed again by a similar non learnable convolution with the iDCT frozen kernels.\n",
    "  - Since the thresholded result has the same size as the original image (not downsampled by a factor $N$) - there's redundancy in the spectrum representation and the reconstruction will end up summing $NxN$ iDCT proposal for each pixel (instead of 1)... having $NxN$ overlapping candidate allows reducing blocking artifacts. There's an explicit way to remove these redundant operations have been to use the torch\n",
    "\n",
    "Minimizing the L2 error in the spatial domain is equivalent to minimizing the L2 error for each frequencies (Parseval theorem). \n",
    "\n",
    "\n",
    "The assumption for the distribution of X in question 1. is hard to justify in the spatial domain (no reason to have a signal centered at zero with an exponential decay... something similar to the \"gray\" world assumption).\n",
    "But in the frequency domain, it is more likely to be true (this trick is used in JPEG compression, a lot of image spectrum energy coefficients are close to zero).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "- The hard thesholding function can be differentiated with regard to the input $y$ but not with regard to the threshold $T=\\sigma^{2}$ unfortunately.\n",
    "- A workaround is to use **a very rough approximation** of the hard thresholding function to stay in the standard framework of torch operators: using a bias and a Relu, it is possible to perform an operation of soft thresholding.\n",
    "- Best idea is to use an differentiable approximation of the hard thresholding function. \n",
    "\n",
    "##### Approximate differentiable hard thresholding function\n",
    "\n",
    "Another idea is to approximate the hard thresholding by a function satisfying the following properties:\n",
    "-  *differentiable* with regard to the threshold (and the input obviously)\n",
    "-  *parametric*: use a temperature $\\lambda$ parameter so that when the temperature varies from $+\\infty$ and 0, the thresholding function varies between a soft threshold and a hard threshold of value $T$.\n",
    "-  Using this idea, you can use the approximate differentiable function in a deep learning standard framework and proggressively vary the temperature\n",
    "\n",
    "$$f(x) = \\text{ReLU}(x - T + T. tanh(\\frac{(x-T)}{\\lambda}))$$\n",
    "\n",
    "![](figures/thresholding_functions.png)\n",
    "\n",
    "\n",
    "```python\n",
    "# Definition of various thresholding functions\n",
    "def hard_thresholding(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Non-differentiable with regard to threshold\"\"\"\n",
    "    return torch.where(torch.abs(x) > threshold, x, torch.zeros_like(x))\n",
    "\n",
    "\n",
    "def soft_thresholding(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Differentiable with regard to threshold, does not preserve energy of input signal (biased)\"\"\"\n",
    "    return torch.nn.functional.relu(x - threshold) - torch.nn.functional.relu(-(x + threshold))\n",
    "\n",
    "\n",
    "def assym_differentiable_hard_thresholding(x: torch.Tensor, threshold: torch.Tensor, temperature: float=1) -> torch.Tensor:\n",
    "    x_offset = x - threshold\n",
    "    return torch.nn.functional.relu(x_offset + threshold*torch.tanh(x_offset/temperature))\n",
    "\n",
    "\n",
    "def differentiable_hard_thresholding(x: torch.Tensor, threshold: torch.Tensor, temperature: float=1) -> torch.Tensor:\n",
    "    \"\"\"Approximated of hard thresholding, differentiable with regard to threshold\n",
    "    When temperature is high, it is close to soft thresholding\n",
    "    When temperature is close to 0, it is close to hard thresholding\n",
    "    \"\"\"\n",
    "    return assym_differentiable_hard_thresholding(x, threshold, temperature) - assym_differentiable_hard_thresholding(-x, threshold, temperature)\n",
    "```\n",
    "\n",
    "##### Proof of concept\n",
    "We build a simple toy example where an input gaussian distribution of standard deviation 8 is hard-thresholded with a trheshold of $2.4$. \n",
    "\n",
    "![toy_example](figures/toy_example.png)\n",
    "\n",
    "The goal is to fit/learn this threshold. We'll preform Stochastic gradient descent using the Mean Square Error (LÂ²) and we'll decrease the temperature progressively.\n",
    "\n",
    "![learnable_hard_threshold](figures/learnable_threshold.png)\n",
    "\n",
    "It is doable to learn the right hard threshold using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.\n",
    "The number of significative operations (multiplications) per pixels is in $2*N^{4} $\n",
    "- $N^2$ frequencies (number of channels) multiplied by \n",
    "  - convolution kernel of size $N^2$ multipliciations.\n",
    "  - 2 because of DCT and inverse DCT.\n",
    "  - No bias addition, $N^2$ thresholdings.\n",
    "- Final normalization is negligible.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.\n",
    "Best threshold (ratio) value for the Zebre picture with AWGN $\\sigma=25$ is $2.7$.\n",
    "\n",
    "$r = \\frac{T}{\\sigma} = 2.7$\n",
    "\n",
    "![ratio_search](figures/figure_ratio_search_2_73.png)\n",
    "\n",
    "| Noisy | DCT denoised $r=2.7$| DCT denoised $r=5$|\n",
    "|:----:| :----:| :----:|\n",
    "|![](figures/zebre_noisy.png) | ![](figures/zebre_DCT_denoised_opt.png) | ![](figures/zebre_DCT_denoised.png)  |\n",
    "|  $\\sigma=25$ | Optimum DCT denoiser in the MSE sense | Oversmoothed, threshold is too high |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
