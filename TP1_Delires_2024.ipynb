{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efsYlgNueSDB"
      },
      "outputs": [],
      "source": [
        "# Downloading data\n",
        "!wget  -O donnees_TP1_Delires_2024.tgz https://perso.telecom-paris.fr/ladjal/donnees_TP1_Delires_2024.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAu-tEnLekNF"
      },
      "outputs": [],
      "source": [
        "#unpack the data.\n",
        "#everything will be in content/Donnees_TPA_Delires_2024\n",
        "\n",
        "!tar xvzf donnees_TP1_Delires_2024.tgz\n",
        "zebres=\"Donnees_TP1_Delires_2024/Data/FFDNET/zebres.png\"\n",
        "testimage1=\"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/img_043.png\"\n",
        "testimage2=\"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/119082.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGG-jV1tFDju"
      },
      "source": [
        "Through all the practical work images are to be considered as float images of values in the range [0..1]. So you will some times see some /255.\n",
        "\n",
        " This scaling is important when a network has been trained with images in a specfic intervalle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwTk7TF64eoY"
      },
      "outputs": [],
      "source": [
        "#Common functions Usage for every part of the practical work\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image as pil_image\n",
        "\n",
        "def viewimage(im, normalize=True,titre='',displayfilename=False):\n",
        "   imin=im.copy().astype(np.float32)\n",
        "   if normalize:\n",
        "       imin-=imin.min()\n",
        "       if imin.max()>0:\n",
        "           imin/=imin.max()\n",
        "   else:\n",
        "       imin=imin.clip(0,255)/255\n",
        "\n",
        "\n",
        "   imin=(imin*255).astype(np.uint8)\n",
        "   filename=tempfile.mktemp(titre+'.png')\n",
        "   if displayfilename:\n",
        "       print (filename)\n",
        "   plt.imsave(filename, imin, cmap='gray')\n",
        "   IPython.display.display(IPython.display.Image(filename))\n",
        "\n",
        "#La fonction viewimage_color est la mÃªme que viewimage. Ca a l'air de marcher\n",
        "#USE ONLY viewimage\n",
        "def viewimage_color(im, normalize=True,titre='',displayfilename=False):\n",
        "   imin=im.copy().astype(np.float32)\n",
        "   if normalize:\n",
        "       imin-=imin.min()\n",
        "       if imin.max()>0:\n",
        "           imin/=imin.max()\n",
        "   else:\n",
        "       imin=imin.clip(0,255)/255\n",
        "\n",
        "\n",
        "   imin=(imin*255).astype(np.uint8)\n",
        "   filename=tempfile.mktemp(titre+'.png')\n",
        "   if displayfilename:\n",
        "       print (filename)\n",
        "   plt.imsave(filename, imin, cmap='gray')\n",
        "   IPython.display.display(IPython.display.Image(filename))\n",
        "\n",
        "\n",
        "def read_image_from_disk(filename):\n",
        "  \"\"\"reads an image from the disk. \"\"\"\n",
        "  image = pil_image.open(filename).convert('RGB')\n",
        "  imgnp=np.array(image).astype(np.float32)\n",
        "  #(h,w,c)=imgnp.shape\n",
        "  #imgnp=imgnp[:(h//scale)*scale,:(w//scale)*scale,:]\n",
        "  return imgnp/255.0\n",
        "# NOTE FOR Denoising, we only use gray images for simplicity.\n",
        "def read_gray_image(filename):\n",
        "  img=read_image_from_disk(filename)\n",
        "  return img.sum(axis=2)/3\n",
        "\n",
        "def norm2(X):\n",
        "  return ((X**2).sum())**0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhYt6ysFRsn3"
      },
      "source": [
        "# DCT denoiser\n",
        "The DCT denoiser is described in the model in the next cell and a cell that uses it is given"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjYG10QHRqqA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy import fftpack as scifft\n",
        "\n",
        "def To_Tensor(X):\n",
        "  return torch.from_numpy(X.copy()).to(device)\n",
        "\n",
        "def From_Tensor(X):\n",
        "  return X.detach().cpu().numpy()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #POSSIBLE BUG choose other that cuda:0\n",
        "print (device)\n",
        "\n",
        "\n",
        "def idct2_blocks(N):\n",
        "  X=np.zeros((N,N,N*N),dtype=np.float32)\n",
        "  for k in range(N):\n",
        "    for l in range(N):\n",
        "      X[k,l,k*N+l]=1\n",
        "  O=scifft.idct(X,type=2,axis=0,norm='ortho')\n",
        "  O=scifft.idct(O,type=2,axis=1,norm='ortho')\n",
        "  return O\n",
        "\n",
        "class DCT_denoiser(nn.Module): #N must be odd\n",
        "  def __init__(self, N=7):\n",
        "    vects=idct2_blocks(N)\n",
        "    W1=To_Tensor(np.expand_dims(vects,0)).to(device).permute(3,0,1,2)\n",
        "    W2=To_Tensor(np.expand_dims(np.fliplr(np.flipud(vects)),0)).to(device).permute(0,3,1,2)\n",
        "    super(DCT_denoiser, self).__init__()\n",
        "    # First convolutional layer with 7x7 kernel and 49 features\n",
        "    self.conv1 = nn.Conv2d(1, N*N, kernel_size=N, stride=1, padding='valid')\n",
        "    self.conv1.weight.data = W1\n",
        "    self.conv1.bias.data=torch.zeros(N*N).to(device)\n",
        "    #self.relu = nn.ReLU()\n",
        "    # Second convolutional layer with 1x1 kernel and 1 feature\n",
        "    self.conv2 = nn.Conv2d(N*N, 1, kernel_size=7, stride=1, padding=(N-1))\n",
        "    self.conv2.weight.data = W2\n",
        "    self.conv2.bias.data=torch.zeros(1).to(device)\n",
        "    #self.seuil=To_Tensor(s*np.ones((1),dtype=np.float32))\n",
        "    self.N=N\n",
        "  def get_mask(self,sh): # renvoie le mask par lequel diviser la sortie pour obtenir la vraie restauration\n",
        "    N=self.N\n",
        "    sigy=np.concatenate((np.arange(1,N),N*np.ones((sh[0]-2*N+2,)),np.arange(N-1,0,-1))).reshape((sh[0],1))\n",
        "    sigx= np.concatenate((np.arange(1,N),N*np.ones((sh[1]-2*N+2,)),np.arange(N-1,0,-1))).reshape((1,sh[1]))\n",
        "    return sigx*sigy\n",
        "\n",
        "  def forward(self, x,s):\n",
        "    # Input x should have shape (batch_size, channels, height, width)\n",
        "    x = self.conv1(x)\n",
        "    seuil=To_Tensor(s*np.ones((1),dtype=np.float32))\n",
        "    x = torch.where(x.abs() < seuil, torch.tensor(0.0), x)\n",
        "    x = self.conv2(x)\n",
        "    return x\n",
        "\n",
        "def apply_denoiser(model, img,noiselevel,ratio):\n",
        "  \"\"\" takes an image, adds noise to it (noislevel), denoise it with the model\n",
        "  and returne (clean, noisy, denoised)\"\"\"\n",
        "  noiselevel/=255\n",
        "  img=img.copy().astype(np.float32)\n",
        "  noisy=img+np.random.randn(*img.shape)*noiselevel\n",
        "  noisy=noisy.astype(np.float32)\n",
        "  noisynet=np.expand_dims(noisy,0)\n",
        "  noisynet=np.expand_dims(noisynet,0)\n",
        "  noisynet=To_Tensor(noisynet)\n",
        "  out=model(noisynet,ratio*noiselevel) #with mutiply noislevel by ratio to obtain te threshold\n",
        "  out=From_Tensor(out)\n",
        "  out=out[0,0]\n",
        "  mask=model.get_mask(img.shape)\n",
        "  return (img,noisy,out/mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CW6ZixxWwqX"
      },
      "outputs": [],
      "source": [
        "#TEST DCT_denoiser.\n",
        "N=7\n",
        "test=zebres\n",
        "img=read_gray_image(test)\n",
        "my_denoiser=DCT_denoiser(N=N)\n",
        "viewimage(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6kx98Djfgc4"
      },
      "outputs": [],
      "source": [
        "nl=25\n",
        "ratios=np.arange(0,5,0.1)\n",
        "errs=0*ratios\n",
        "k=0\n",
        "for ratio  in ratios:\n",
        "  (CL,NOI,DENOI)=apply_denoiser(my_denoiser,img,nl,ratio)\n",
        "  errs[k]=norm2(CL-DENOI)\n",
        "  print (\"for ratio=\",ratio,\" we have residual error = \",norm2(CL-DENOI))\n",
        "  k+=1\n",
        "plt.xlabel('ratio')\n",
        "plt.ylabel('error')\n",
        "plt.plot(ratios,errs)\n",
        "arg_err_min = np.argmin(errs)\n",
        "min_ratio = ratios[arg_err_min]\n",
        "min_error = errs[arg_err_min]\n",
        "plt.plot(min_ratio, min_error, 'ro', label=f\"min error {min_error:.2f} at ratio {min_ratio:.2f}\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title(f\"Error as a function of ratio for noise level {nl}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "viewimage(NOI)\n",
        "viewimage(DENOI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4IHbvjgwu9V"
      },
      "source": [
        "The Next sections are dedicated the the study of a more complex denoising network: FFDNET\n",
        "The implementation is taken from\n",
        "https://www.ipol.im/pub/art/2019/231/\n",
        "\n",
        "The denoising is all done in grayscale, but you can easily use the provided data to have a color denoiser.\n",
        "This is done to simplify comparisons and the code of the practical work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42WKqvU9xFXT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"Donnees_TP1_Delires_2024/Code/FFDNET\")\n",
        "from models import FFDNet\n",
        "from utils import batch_psnr, normalize, init_logger_ipol, \\\n",
        "                                variable_to_cv2_image, remove_dataparallel_wrapper, is_rgb\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #POSSIBLE BUG choose other that cuda:0\n",
        "print (device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "in_ch = 1\n",
        "FFDNet(num_input_channels=in_ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYZpA4ucza4P"
      },
      "outputs": [],
      "source": [
        "# CREATION OF THE DENOINSING NETWORK\n",
        "\n",
        "\n",
        "\n",
        "def create_ffdnet():\n",
        "  model_fn=\"Donnees_TP1_Delires_2024/Data/FFDNET/Pretrained/net_gray.pth\"\n",
        "\n",
        "  in_ch=1 # this is to be changed for rgb images\n",
        "  net = FFDNet(num_input_channels=in_ch)\n",
        "\n",
        "  if device==torch.device('cuda:0'):\n",
        "    state_dict = torch.load(model_fn)\n",
        "    device_ids = [0]\n",
        "    model = nn.DataParallel(net, device_ids=device_ids).cuda()\n",
        "    #state_dict=state_dict.to(device)\n",
        "  else:\n",
        "    state_dict = torch.load(model_fn, map_location='cpu')\n",
        "    # CPU mode: remove the DataParallel wrapper\n",
        "    state_dict = remove_dataparallel_wrapper(state_dict)\n",
        "    model = net\n",
        "\n",
        "\n",
        "  model.load_state_dict(state_dict)\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "def To_Tensor(X):\n",
        "  return torch.from_numpy(X).to(device)\n",
        "\n",
        "def From_Tensor(X):\n",
        "  return X.detach().cpu().numpy()\n",
        "\n",
        "def Apply_FFDNET_model(model,im,noiselevel=25):\n",
        "  \"\"\"Given a perfect image, we add noise and denoise with ffdnet.\n",
        "  noiselevel is given in the range 0-255\n",
        "  Returns the denoised image.\n",
        "  Remember the program adds noise to the image you gave.\n",
        "  If you do not want any added noise run the program with noiselevel=0\n",
        "  \"\"\"\n",
        "  # the input must be of even size\n",
        "  (dy,dx)=im.shape\n",
        "  ndx=dx\n",
        "  ndy=dy\n",
        "  if dy%2==1:\n",
        "    ndy=dy+1\n",
        "  if dx%2==1:\n",
        "    ndx=dx+1\n",
        "  imnew=np.zeros((ndy,ndx),dtype=np.float32)\n",
        "  imnew[:dy,:dx]=im\n",
        "  imnew[:,-1]=imnew[:,dx-1]\n",
        "  imnew[-1,:]=imnew[dy-1,:]\n",
        "  noiselevel/=255.0\n",
        "  noiselevel=np.ones((1),dtype=np.float32)*noiselevel\n",
        "  noisyimage=imnew+np.random.randn(*imnew.shape)*noiselevel\n",
        "  noisyimage=np.expand_dims(noisyimage,0)\n",
        "  noisyimage=np.expand_dims(noisyimage,0)\n",
        "  with torch.no_grad():\n",
        "    outim=model(To_Tensor(noisyimage).to(device),To_Tensor(noiselevel).to(device))\n",
        "\n",
        "  outim=From_Tensor(outim)[0,0]\n",
        "  outim=noisyimage[0,0]-outim # the model estimates the noise, not the clean image\n",
        "  outim=outim[:dy,:dx] #in case even size constraint had to be enforced\n",
        "  return outim\n",
        "\n",
        "\n",
        "ffdnet=create_ffdnet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NwFwy5s5M12"
      },
      "outputs": [],
      "source": [
        "#Test FFDNET\n",
        "testimage=\"Donnees_TP1_Delires_2024/Data/FFDNET/zebres.png\"\n",
        "img=read_gray_image(testimage)\n",
        "viewimage(img)\n",
        "toto=Apply_FFDNET_model(ffdnet,img,noiselevel=2)\n",
        "viewimage(toto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5x8psxtu7uc"
      },
      "source": [
        "The Next sections are dedicated to the study of a super-resolution network.\n",
        "The corresponding paper is:\n",
        "Zhang, Yulun, et al. \"Residual dense network for image super-resolution.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n",
        "\n",
        "The implementation is taken from:\n",
        "https://github.com/yjn870/RDN-pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MgAsFV9_erPJ"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import PIL.Image as pil_image\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"Donnees_TP1_Delires_2024/Code\")\n",
        "from SR.models import RDN\n",
        "from SR.utils import convert_rgb_to_y, denormalize, calc_psnr\n",
        "\n",
        "import tempfile\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72y8UtdlixIA"
      },
      "outputs": [],
      "source": [
        "# tests the availability of a GPU and sets the global variable device\n",
        "cudnn.benchmark = True\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #POSSIBLE BUG choose other that cuda:0\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPnCfqz2fs1B"
      },
      "outputs": [],
      "source": [
        "def viewimage(im, normalize=True,titre='',displayfilename=False):\n",
        "   imin=im.copy().astype(np.float32)\n",
        "   if normalize:\n",
        "       imin-=imin.min()\n",
        "       if imin.max()>0:\n",
        "           imin/=imin.max()\n",
        "   else:\n",
        "       imin=imin.clip(0,255)/255\n",
        "\n",
        "\n",
        "   imin=(imin*255).astype(np.uint8)\n",
        "   filename=tempfile.mktemp(titre+'.png')\n",
        "   if displayfilename:\n",
        "       print (filename)\n",
        "   plt.imsave(filename, imin, cmap='gray')\n",
        "   IPython.display.display(IPython.display.Image(filename))\n",
        "\n",
        "#La fonction viewimage_color est la mÃªme que viewimage. Ca a l'air de marcher\n",
        "#USE ONLY viewimage\n",
        "def viewimage_color(im, normalize=True,titre='',displayfilename=False):\n",
        "   imin=im.copy().astype(np.float32)\n",
        "   if normalize:\n",
        "       imin-=imin.min()\n",
        "       if imin.max()>0:\n",
        "           imin/=imin.max()\n",
        "   else:\n",
        "       imin=imin.clip(0,255)/255\n",
        "\n",
        "\n",
        "   imin=(imin*255).astype(np.uint8)\n",
        "   filename=tempfile.mktemp(titre+'.png')\n",
        "   if displayfilename:\n",
        "       print (filename)\n",
        "   plt.imsave(filename, imin, cmap='gray')\n",
        "   IPython.display.display(IPython.display.Image(filename))\n",
        "\n",
        "\n",
        "def read_image_from_disk(filename):\n",
        "  \"\"\"reads an image from the disk. \"\"\"\n",
        "  image = pil_image.open(filename).convert('RGB')\n",
        "  imgnp=np.array(image).astype(np.float32)\n",
        "  #(h,w,c)=imgnp.shape\n",
        "  #imgnp=imgnp[:(h//scale)*scale,:(w//scale)*scale,:]\n",
        "  return imgnp/255.0\n",
        "\n",
        "def apply_model(model,input, scale=2,subsampling_type=\"bicubic\"):\n",
        "  \"\"\" prepares an image to be scaled up. Returns the  (O,S,ZB,ZM)\n",
        "  input is a [h,w,c] numpy array\n",
        "  O= original image with size multpile of scale so that zoomed images can be compared\n",
        "    to the original image\n",
        "  S= subsampled image subsample(O)\n",
        "  ZB= zoomed image with bicubic (zoom_bicubic(subsample(O)))\n",
        "  ZM= zoomed image with model (zoom_model(subsample(O)))\n",
        "  subsample_type: if \"bicubic\" then the subsampling of the original paper\n",
        "                 is kept. if \"decim\" one pixel each scal pixels is kept.\n",
        "            \"\"\"\n",
        "\n",
        "  (h,w,c)=input.shape\n",
        "  input=input[:(h//scale)*scale,:(w//scale)*scale,:]\n",
        "  if subsampling_type==\"decim\":\n",
        "    S=input[::scale,::scale,:]\n",
        "  if subsampling_type==\"locmean\":\n",
        "    S=input[::scale,::scale,:]*0\n",
        "    for k in range(scale):\n",
        "      for l in range(scale):\n",
        "        S+=input[k::scale,l::scale,:]\n",
        "    S/=scale*scale\n",
        "  if subsampling_type==\"bicubic\":\n",
        "    #convert to PILLOW image\n",
        "    pilim=pil_image.fromarray((input*255).astype(np.uint8))\n",
        "    #subsample using PIL BICUBIC\n",
        "    lr = pilim.resize((pilim.width // scale, pilim.height // scale),\n",
        "                      resample=pil_image.BICUBIC)\n",
        "\n",
        "\n",
        "    #back to numpy image\n",
        "    S=np.array(lr).astype(np.float32)/255\n",
        "  ZB=pil_image.fromarray((S*255).astype(np.uint8))\n",
        "  ZB=ZB.resize((ZB.width*scale,ZB.height*scale),resample=pil_image.BICUBIC)\n",
        "  ZB=np.array(ZB).astype(np.float32)/255\n",
        "  lrmodel=np.expand_dims(S.transpose([2, 0, 1]), 0)\n",
        "  lrmodel=torch.from_numpy(lrmodel).to(device)\n",
        "  with torch.no_grad():\n",
        "    preds=model(lrmodel ).squeeze(0)\n",
        "  ZM=preds.permute(1, 2, 0).cpu().numpy()\n",
        "  return (input,S,ZB,ZM)\n",
        "\n",
        "\n",
        "def RDN_model(scale=2):\n",
        "  \"\"\"This function retruns an RDN model that scales images by\n",
        "  a factor given as a parameter. \"\"\"\n",
        "\n",
        "  if scale==2:\n",
        "    weightsfile=\"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x2.pth\"\n",
        "  if scale==3:\n",
        "    weightsfile=\"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x3.pth\"\n",
        "  if scale==4:\n",
        "    weightsfile=\"/content/Donnees_TP1_Delires_2024/Data/SR/Pretrained/rdn_x4.pth\"\n",
        "\n",
        "  model = RDN(scale_factor=scale,\n",
        "                num_channels=3,\n",
        "                num_features=64,\n",
        "                growth_rate=64,\n",
        "                num_blocks=16,\n",
        "                num_layers=8).to(device)\n",
        "  state_dict = model.state_dict()\n",
        "  for n, p in torch.load(weightsfile, map_location=lambda storage, loc: storage).items():\n",
        "      if n in state_dict.keys():\n",
        "        state_dict[n].copy_(p)\n",
        "      else:\n",
        "        raise KeyError(n)\n",
        "\n",
        "  model.eval()\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibXVZ_e-j24u"
      },
      "outputs": [],
      "source": [
        "##BROUILLON\n",
        "sca=2\n",
        "model2=RDN_model(scale=sca)\n",
        "testimage1=\"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/img_043.png\"\n",
        "testimage2=\"/content/Donnees_TP1_Delires_2024/Data/SR/testimages/119082.png\"\n",
        "img=read_image_from_disk(testimage1)\n",
        "(h,w,c)=img.shape\n",
        "print (h,w,c)\n",
        "viewimage(img)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ddKCCrS5aSt"
      },
      "outputs": [],
      "source": [
        "(O,S,ZB,ZM)=apply_model(model2,img,scale=sca)\n",
        "viewimage(O)\n",
        "viewimage(S)\n",
        "viewimage(ZB)\n",
        "viewimage(ZM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_kYA2vs8xbz"
      },
      "outputs": [],
      "source": [
        "viewimage(S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DoFyyB_8RLD"
      },
      "outputs": [],
      "source": [
        "print(ZM.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0co6gGOn5z6k"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
