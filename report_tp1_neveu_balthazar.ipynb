{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1\n",
    "- Master MVA ENS-Paris Saclay\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCT denoiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. \n",
    "#### Maximizing the likelihood\n",
    "Given $Y = X+B$ where $B \\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\mathcal{p}(X) = \\frac{e^{-\\|X\\|}}{ \\int_{-\\infty}^{\\infty} e^{-\\|X\\|}dx}$\n",
    "\n",
    "We're looking for the most likely value $x$ of $X$  given the observation $y$ of the random variable $Y$.\n",
    "\n",
    "$x^{*} = \\text{argmax}_{x}P(X=x|Y=y)$\n",
    "\n",
    "Let's first apply Bayes rule $P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(Y=y)}{P(X=x)} = \\frac{P(B=y-x|X=x)P(Y=y)}{P(X=x)}  \\propto  \\frac{e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2}}}{e^{-\\|x\\|}}P(Y=y)$\n",
    "\n",
    "Since we're searching for the argmax of this expression regarding $x$ ($y$ is constant so $P(Y=y)$ is constant too).\n",
    "\n",
    "We have $x^{*} = \\text{argmax}_{x} e^{-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|}$\n",
    "\n",
    "*since $e^{-u}$ is a monotonic decreasing function.*\n",
    "\n",
    "$x^{*}= \\text{argmin}_{x} (-\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|) $\n",
    "\n",
    "-------\n",
    "#### Finding the solution of the cost function\n",
    "Let's minimize $C(x) = -\\frac{\\|y-x\\|^{2}}{2\\sigma^2} + \\|x\\|$\n",
    "\n",
    "- if $x>0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} + 1$. Critical point shall statisfy $x=y - \\sigma^{2}$ \n",
    "- if $x<0$ , $\\frac{dC(x)}{dx} = \\frac{x-y}{\\sigma^2} - 1$. Critical point shall statisfy $x=y + \\sigma^{2}$ \n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "- If $ y > \\sigma^2$, then $ y - \\sigma^2 > 0$, and the critical point $ x = y - \\sigma^2$ is valid as it falls in the $ x > 0$ case.\n",
    "- If $ y < -\\sigma^2$, then $ y + \\sigma^2 < 0$, and the critical point $ x = y + \\sigma^2$ is valid as it falls in the $ x < 0$ case.\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2$, the solution might be $ x = 0$ as neither of the critical points fall within their respective ranges.\n",
    "\n",
    "\n",
    "Let's assume $y>0$\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most probable value of $ X $ given an observation $ Y = y $, we need to minimize the expression you've provided:\n",
    "\n",
    "$\n",
    "\\text{argmin}_x \\left( |x| + \\frac{(x - y)^2}{2\\sigma^2} \\right)\n",
    "$\n",
    "\n",
    "This expression represents a cost function that combines the effects of the original distribution of $ X $ and the noise introduced by $ B $. The term $ |x| $ represents the exponential decay proportional to $ e^{-|X|} $ in the original distribution of $ X $, while the term $ \\frac{(x - y)^2}{2\\sigma^2} $ represents the Gaussian noise with variance $ \\sigma^2 $.\n",
    "\n",
    "To find the minimum of this function, we need to differentiate it with respect to $ x $ and set the derivative equal to zero. Since the absolute value function $ |x| $ is not differentiable at $ x = 0 $, we need to consider two cases: $ x > 0 $ and $ x < 0 $.\n",
    "\n",
    "1. **For $ x > 0 $**, the function becomes:\n",
    "   $\n",
    "   f(x) = x + \\frac{(x - y)^2}{2\\sigma^2}\n",
    "   $\n",
    "\n",
    "2. **For $ x < 0 $**, the function becomes:\n",
    "   $\n",
    "   f(x) = -x + \\frac{(x - y)^2}{2\\sigma^2}\n",
    "   $\n",
    "\n",
    "We will differentiate these two cases separately and find the points where the derivatives are zero. Then, we'll check which of these points actually minimizes the function.\n",
    "\n",
    "Let's start by differentiating these two cases.\n",
    "\n",
    "The derivatives of the two cases and their critical points are:\n",
    "\n",
    "1. For $ x > 0 $, the derivative is $ 1 + \\frac{2x - 2y}{2\\sigma^2} $ and the critical point is $ x = y - \\sigma^2 $.\n",
    "2. For $ x < 0 $, the derivative is $ -1 + \\frac{2x - 2y}{2\\sigma^2} $ and the critical point is $ x = y + \\sigma^2 $.\n",
    "\n",
    "Now, we need to determine which of these critical points actually minimizes the original function. This involves checking the second derivative at these points or evaluating the original function at these points and at the boundaries (in this case, at $ x = 0 $).\n",
    "\n",
    "However, since the critical points depend on the sign of $ x $, we should also consider the condition of the cases:\n",
    "- If $ y > \\sigma^2 $, then $ y - \\sigma^2 > 0 $, and the critical point $ x = y - \\sigma^2 $ is valid as it falls in the $ x > 0 $ case.\n",
    "- If $ y < -\\sigma^2 $, then $ y + \\sigma^2 < 0 $, and the critical point $ x = y + \\sigma^2 $ is valid as it falls in the $ x < 0 $ case.\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2 $, the solution might be $ x = 0 $ as neither of the critical points fall within their respective ranges.\n",
    "\n",
    "Let's calculate the value of the original function at these critical points and at $ x = 0 $ to determine the minimum.\n",
    "\n",
    "The values of the original function at the critical points and at $ x = 0 $ are given by:\n",
    "\n",
    "1. At $ x = y - \\sigma^2 $, the value is:\n",
    "   $\n",
    "   \\begin{cases} \n",
    "   -\\frac{\\sigma^2}{2} + y & \\text{if } \\sigma^2 - y < 0 \\\\\n",
    "   \\frac{3\\sigma^2}{2} - y & \\text{if } \\sigma^2 - y > 0 \n",
    "   \\end{cases}\n",
    "   $\n",
    "2. At $ x = y + \\sigma^2 $, the value is:\n",
    "   $\n",
    "   \\begin{cases} \n",
    "   \\frac{3\\sigma^2}{2} + y & \\text{if } \\sigma^2 + y > 0 \\\\\n",
    "   -\\frac{\\sigma^2}{2} - y & \\text{if } \\sigma^2 + y < 0 \n",
    "   \\end{cases}\n",
    "   $\n",
    "3. At $ x = 0 $, the value is indeterminate in this form. \n",
    "\n",
    "However, to determine the minimum, we should consider the following:\n",
    "\n",
    "- If $ y > \\sigma^2 $, then $ x = y - \\sigma^2 $ is the most probable value for $ X $.\n",
    "- If $ y < -\\sigma^2 $, then $ x = y + \\sigma^2 $ is the most probable value for $ X $.\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2 $, we need to evaluate the original function at $ x = 0 $ properly.\n",
    "\n",
    "Let's evaluate the original function at $ x = 0 $ properly for the case where $ -\\sigma^2 \\leq y \\leq \\sigma^2 $.\n",
    "\n",
    "It appears that evaluating the original function at $ x = 0 $ directly leads to an indeterminate result. This is likely due to the piecewise definition of the function, which does not explicitly handle the case when $ x = 0 $.\n",
    "\n",
    "To properly evaluate the function at $ x = 0 $, we should consider the limit of the function as $ x $ approaches 0 from both positive and negative sides. Since the function is continuous at $ x = 0 $, this limit should give us the correct value. Let's calculate this limit.\n",
    "\n",
    "The limit of the original function as $ x $ approaches 0 from both the positive and negative sides is $ \\frac{y^2}{2\\sigma^2} $. Since the limits from both sides are equal, this is the value of the function at $ x = 0 $.\n",
    "\n",
    "To summarize:\n",
    "- If $ y > \\sigma^2 $, the most probable value of $ X $ is $ x = y - \\sigma^2 $.\n",
    "- If $ y < -\\sigma^2 $, the most probable value of $ X $ is $ x = y + \\sigma^2 $.\n",
    "- If $ -\\sigma^2 \\leq y \\leq \\sigma^2 $, we need to compare the value of the function at $ x = 0 $ and the values at the critical points to determine the minimum. Since the function's value at $ x = 0 $ is $ \\frac{y^2}{2\\sigma^2} $, this is likely to be the minimum in this range, given the quadratic nature of the term $ \\frac{(x - y)^2}{2\\sigma^2} $ in the original function.\n",
    "\n",
    "Therefore, the most probable value of $ X $ given $ Y = y $ is:\n",
    "- $ x = y - \\sigma^2 $ if $ y > \\sigma^2 $\n",
    "- $ x = y + \\sigma^2 $ if $ y < -\\sigma^2 $\n",
    "- $ x = 0 $ if $ -\\sigma^2 \\leq y \\leq \\sigma^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.\n",
    "`DCT_denoise` performs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "- We could use bias and a Relu to perform something similar to the thresholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.\n",
    "The number of operation per pixels is in $2*N^{4} $\n",
    "- $N^2$ frequencies (number of channels) multiplied by \n",
    "  - convolution kernel of size $N^2$ multipliciations.\n",
    "  - 2 because of DCT and inverse DCT.\n",
    "  - No bias addition, $N^2$ thresholdings.\n",
    "- Final normalization is negligible.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
