{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdpPHz2Lp6VD"
   },
   "source": [
    "# TP Coding autoencoders and variational autoencoders in Pytorch\n",
    "\n",
    "\n",
    "Author : Alasdair Newson\n",
    "\n",
    "alasdair.newson@telecom-paris.fr\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n",
    "\n",
    "\n",
    "![AUTOENCODER](https://perso.telecom-paristech.fr/anewson/doc/images/autoencoder_illustration_2.png)\n",
    "\n",
    "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
    "\n",
    "### Your task:\n",
    "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)\n",
    "\n",
    "\n",
    "First of all, let's load some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JqNeIJ8Op8Ao"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pytorch_to_numpy(x):\n",
    "  return x.detach().numpy()\n",
    "\n",
    "ae_dim_1 = 512\n",
    "ae_dim_2 = 256\n",
    "z_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hyj5dj_eui9D"
   },
   "source": [
    "First, we load the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4YPLKlPrufSk"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "\n",
    "# MNIST Dataset\n",
    "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "#create data loader with smaller dataset size\n",
    "max_mnist_size = 1000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "# download test dataset\n",
    "max_mnist_size = 512\n",
    "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=batch_size, shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r7YhlBT2PN9I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bneveu/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainset_reduced.dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-bkK4ktwfvC"
   },
   "source": [
    "# 1 Vanilla Autoencoder\n",
    "\n",
    "Now, we define the general parameters of the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mD56EDzbvUxq"
   },
   "outputs": [],
   "source": [
    "# autoencoder parameters\n",
    "n_rows = mnist_trainset_reduced.dataset.train_data.shape[1]\n",
    "n_cols = mnist_trainset_reduced.dataset.train_data.shape[2]\n",
    "n_channels = 1\n",
    "n_pixels = n_rows*n_cols\n",
    "\n",
    "img_shape = (n_rows, n_cols, n_channels)\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jLa2-jQwxSI"
   },
   "source": [
    "Now, define the autoencoder architecture. In the first part, we will use the following MLP architecture :\n",
    "\n",
    "Encoder :\n",
    "- Flatten input\n",
    "- Dense layer, output size h_dim_1 + ReLU\n",
    "- Dense layer, output size h_dim_2 + ReLU\n",
    "- Dense layer, output size z_dim (no non-linearity)\n",
    "\n",
    "Decoder :\n",
    "- Dense layer, output size h_dim_2 + ReLU\n",
    "- Dense layer, output size h_dim_1 + ReLU\n",
    "- Dense layer, output size x_dim + Sigmoid Activation\n",
    "- Reshape, to size $28\\times 28\\times 1$\n",
    "\n",
    "For the Reshape operation, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tEuZfnUlXxMl"
   },
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module ):\n",
    "  def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int,n_rows: int,n_cols: int,n_channels: int):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        x_dim (int): 28²\n",
    "        h_dim1 (int): hidden dimension layer 1\n",
    "        h_dim2 (int): hidden dimension layer 2\n",
    "        z_dim (int): latent dimension\n",
    "        n_rows (int): height of the image\n",
    "        n_cols (int): width of the image\n",
    "        n_channels (int): number of channels\n",
    "    \"\"\"\n",
    "    super(AE, self).__init__()\n",
    "    assert n_cols*n_rows == x_dim, \"wrong dimension\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.n_channels = n_channels\n",
    "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
    "    self.z_dim = z_dim\n",
    "\n",
    "    # encoder part\n",
    "    self.fc1 =  torch.nn.Linear(x_dim,  h_dim1)\n",
    "    self.fc2 = torch.nn.Linear(h_dim1,  h_dim2)\n",
    "    self.fc3 = torch.nn.Linear(h_dim2,  z_dim)\n",
    "    self.non_lin = torch.nn.ReLU()\n",
    "    self.encoder = torch.nn.Sequential(\n",
    "      self.fc1, self.non_lin, self.fc2, self.non_lin, self.fc3\n",
    "    )\n",
    "    # decoder part\n",
    "    self.fc4 = torch.nn.Linear(z_dim,  h_dim2)\n",
    "    self.fc5 = torch.nn.Linear(h_dim2,  h_dim1)\n",
    "    self.fc6 = torch.nn.Linear(h_dim1,  x_dim)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "    self.decoder = torch.nn.Sequential(\n",
    "      self.fc4, self.non_lin, self.fc5, self.non_lin, self.fc6, self.sigmoid\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # N, (C=1), H, W -> N, C*H*W\n",
    "    z = self.encoder(x.view(-1, self.n_pixels))\n",
    "    y = self.decoder(z)\n",
    "    y = y.view(x.shape)\n",
    "    return y\n",
    "  def loss_function(self,x, y):\n",
    "    mse_loss = torch.nn.functional.mse_loss(x, y, reduction=\"mean\")\n",
    "    return torch.mean(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oV40vRMQRoG1"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "ae_model = AE(x_dim=n_pixels, h_dim1= ae_dim_1, h_dim2=ae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
    "ae_optimizer = optim.Adam(ae_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "x_in = torch.rand(10, n_rows, n_cols, n_channels)\n",
    "x_out = ae_model(x_in)\n",
    "assert x_out.shape == x_in.shape, \"wrong shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-659aM36xvXX"
   },
   "source": [
    "Now, define a generic function to train the model for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wfqX6Brlxjyi"
   },
   "outputs": [],
   "source": [
    "def train_ae(ae_model,data_train_loader,epoch):\n",
    "\ttrain_loss = 0\n",
    "\tfor batch_idx, (data, _) in enumerate(data_train_loader):\n",
    "\t\tae_optimizer.zero_grad()\n",
    "\t\t\n",
    "\t\ty = ae_model(data) # FILL IN CODE HERE\n",
    "\t\tloss_ae = ae_model.loss_function(y, data) # FILL IN CODE HERE\n",
    "  \n",
    "\t\tloss_ae.backward()\n",
    "\t\ttrain_loss += loss_ae.item()\n",
    "\t\tae_optimizer.step()\n",
    "\t\t\n",
    "\t\tif batch_idx % 100 == 0:\n",
    "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "\t\t\t\tepoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
    "\t\t\t\t100. * batch_idx / len(data_train_loader), loss_ae.item() / len(data)))\n",
    "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EbmswSzJdK"
   },
   "source": [
    "We define a function to carry out testing on the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "11q_0PSibZk-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1000 (0%)]\tLoss: 0.001802\n",
      "====> Epoch: 0 Average loss: 0.0013\n",
      "Train Epoch: 1 [0/1000 (0%)]\tLoss: 0.000676\n",
      "====> Epoch: 1 Average loss: 0.0006\n",
      "Train Epoch: 2 [0/1000 (0%)]\tLoss: 0.000675\n",
      "====> Epoch: 2 Average loss: 0.0006\n",
      "Train Epoch: 3 [0/1000 (0%)]\tLoss: 0.000627\n",
      "====> Epoch: 3 Average loss: 0.0005\n",
      "Train Epoch: 4 [0/1000 (0%)]\tLoss: 0.000590\n",
      "====> Epoch: 4 Average loss: 0.0005\n",
      "Train Epoch: 5 [0/1000 (0%)]\tLoss: 0.000601\n",
      "====> Epoch: 5 Average loss: 0.0005\n",
      "Train Epoch: 6 [0/1000 (0%)]\tLoss: 0.000556\n",
      "====> Epoch: 6 Average loss: 0.0005\n",
      "Train Epoch: 7 [0/1000 (0%)]\tLoss: 0.000532\n",
      "====> Epoch: 7 Average loss: 0.0005\n",
      "Train Epoch: 8 [0/1000 (0%)]\tLoss: 0.000540\n",
      "====> Epoch: 8 Average loss: 0.0005\n",
      "Train Epoch: 9 [0/1000 (0%)]\tLoss: 0.000490\n",
      "====> Epoch: 9 Average loss: 0.0004\n",
      "Train Epoch: 10 [0/1000 (0%)]\tLoss: 0.000498\n",
      "====> Epoch: 10 Average loss: 0.0004\n",
      "Train Epoch: 11 [0/1000 (0%)]\tLoss: 0.000468\n",
      "====> Epoch: 11 Average loss: 0.0004\n",
      "Train Epoch: 12 [0/1000 (0%)]\tLoss: 0.000460\n",
      "====> Epoch: 12 Average loss: 0.0004\n",
      "Train Epoch: 13 [0/1000 (0%)]\tLoss: 0.000470\n",
      "====> Epoch: 13 Average loss: 0.0004\n",
      "Train Epoch: 14 [0/1000 (0%)]\tLoss: 0.000422\n",
      "====> Epoch: 14 Average loss: 0.0004\n",
      "Train Epoch: 15 [0/1000 (0%)]\tLoss: 0.000413\n",
      "====> Epoch: 15 Average loss: 0.0003\n",
      "Train Epoch: 16 [0/1000 (0%)]\tLoss: 0.000368\n",
      "====> Epoch: 16 Average loss: 0.0003\n",
      "Train Epoch: 17 [0/1000 (0%)]\tLoss: 0.000376\n",
      "====> Epoch: 17 Average loss: 0.0003\n",
      "Train Epoch: 18 [0/1000 (0%)]\tLoss: 0.000344\n",
      "====> Epoch: 18 Average loss: 0.0003\n",
      "Train Epoch: 19 [0/1000 (0%)]\tLoss: 0.000351\n",
      "====> Epoch: 19 Average loss: 0.0003\n",
      "Train Epoch: 20 [0/1000 (0%)]\tLoss: 0.000304\n",
      "====> Epoch: 20 Average loss: 0.0003\n",
      "Train Epoch: 21 [0/1000 (0%)]\tLoss: 0.000327\n",
      "====> Epoch: 21 Average loss: 0.0003\n",
      "Train Epoch: 22 [0/1000 (0%)]\tLoss: 0.000300\n",
      "====> Epoch: 22 Average loss: 0.0003\n",
      "Train Epoch: 23 [0/1000 (0%)]\tLoss: 0.000301\n",
      "====> Epoch: 23 Average loss: 0.0003\n",
      "Train Epoch: 24 [0/1000 (0%)]\tLoss: 0.000266\n",
      "====> Epoch: 24 Average loss: 0.0002\n",
      "Train Epoch: 25 [0/1000 (0%)]\tLoss: 0.000282\n",
      "====> Epoch: 25 Average loss: 0.0002\n",
      "Train Epoch: 26 [0/1000 (0%)]\tLoss: 0.000256\n",
      "====> Epoch: 26 Average loss: 0.0002\n",
      "Train Epoch: 27 [0/1000 (0%)]\tLoss: 0.000248\n",
      "====> Epoch: 27 Average loss: 0.0002\n",
      "Train Epoch: 28 [0/1000 (0%)]\tLoss: 0.000234\n",
      "====> Epoch: 28 Average loss: 0.0002\n",
      "Train Epoch: 29 [0/1000 (0%)]\tLoss: 0.000240\n",
      "====> Epoch: 29 Average loss: 0.0002\n",
      "Train Epoch: 30 [0/1000 (0%)]\tLoss: 0.000225\n",
      "====> Epoch: 30 Average loss: 0.0002\n",
      "Train Epoch: 31 [0/1000 (0%)]\tLoss: 0.000214\n",
      "====> Epoch: 31 Average loss: 0.0002\n",
      "Train Epoch: 32 [0/1000 (0%)]\tLoss: 0.000222\n",
      "====> Epoch: 32 Average loss: 0.0002\n",
      "Train Epoch: 33 [0/1000 (0%)]\tLoss: 0.000226\n",
      "====> Epoch: 33 Average loss: 0.0002\n",
      "Train Epoch: 34 [0/1000 (0%)]\tLoss: 0.000218\n",
      "====> Epoch: 34 Average loss: 0.0002\n",
      "Train Epoch: 35 [0/1000 (0%)]\tLoss: 0.000229\n",
      "====> Epoch: 35 Average loss: 0.0002\n",
      "Train Epoch: 36 [0/1000 (0%)]\tLoss: 0.000204\n",
      "====> Epoch: 36 Average loss: 0.0002\n",
      "Train Epoch: 37 [0/1000 (0%)]\tLoss: 0.000195\n",
      "====> Epoch: 37 Average loss: 0.0002\n",
      "Train Epoch: 38 [0/1000 (0%)]\tLoss: 0.000198\n",
      "====> Epoch: 38 Average loss: 0.0002\n",
      "Train Epoch: 39 [0/1000 (0%)]\tLoss: 0.000192\n",
      "====> Epoch: 39 Average loss: 0.0002\n",
      "Train Epoch: 40 [0/1000 (0%)]\tLoss: 0.000197\n",
      "====> Epoch: 40 Average loss: 0.0002\n",
      "Train Epoch: 41 [0/1000 (0%)]\tLoss: 0.000177\n",
      "====> Epoch: 41 Average loss: 0.0002\n",
      "Train Epoch: 42 [0/1000 (0%)]\tLoss: 0.000192\n",
      "====> Epoch: 42 Average loss: 0.0002\n",
      "Train Epoch: 43 [0/1000 (0%)]\tLoss: 0.000189\n",
      "====> Epoch: 43 Average loss: 0.0002\n",
      "Train Epoch: 44 [0/1000 (0%)]\tLoss: 0.000181\n",
      "====> Epoch: 44 Average loss: 0.0002\n",
      "Train Epoch: 45 [0/1000 (0%)]\tLoss: 0.000181\n",
      "====> Epoch: 45 Average loss: 0.0002\n",
      "Train Epoch: 46 [0/1000 (0%)]\tLoss: 0.000171\n",
      "====> Epoch: 46 Average loss: 0.0002\n",
      "Train Epoch: 47 [0/1000 (0%)]\tLoss: 0.000162\n",
      "====> Epoch: 47 Average loss: 0.0002\n",
      "Train Epoch: 48 [0/1000 (0%)]\tLoss: 0.000169\n",
      "====> Epoch: 48 Average loss: 0.0002\n",
      "Train Epoch: 49 [0/1000 (0%)]\tLoss: 0.000165\n",
      "====> Epoch: 49 Average loss: 0.0001\n",
      "Train Epoch: 50 [0/1000 (0%)]\tLoss: 0.000173\n",
      "====> Epoch: 50 Average loss: 0.0001\n",
      "Train Epoch: 51 [0/1000 (0%)]\tLoss: 0.000156\n",
      "====> Epoch: 51 Average loss: 0.0001\n",
      "Train Epoch: 52 [0/1000 (0%)]\tLoss: 0.000149\n",
      "====> Epoch: 52 Average loss: 0.0001\n",
      "Train Epoch: 53 [0/1000 (0%)]\tLoss: 0.000150\n",
      "====> Epoch: 53 Average loss: 0.0001\n",
      "Train Epoch: 54 [0/1000 (0%)]\tLoss: 0.000159\n",
      "====> Epoch: 54 Average loss: 0.0001\n",
      "Train Epoch: 55 [0/1000 (0%)]\tLoss: 0.000152\n",
      "====> Epoch: 55 Average loss: 0.0001\n",
      "Train Epoch: 56 [0/1000 (0%)]\tLoss: 0.000151\n",
      "====> Epoch: 56 Average loss: 0.0001\n",
      "Train Epoch: 57 [0/1000 (0%)]\tLoss: 0.000156\n",
      "====> Epoch: 57 Average loss: 0.0001\n",
      "Train Epoch: 58 [0/1000 (0%)]\tLoss: 0.000144\n",
      "====> Epoch: 58 Average loss: 0.0001\n",
      "Train Epoch: 59 [0/1000 (0%)]\tLoss: 0.000151\n",
      "====> Epoch: 59 Average loss: 0.0001\n",
      "Train Epoch: 60 [0/1000 (0%)]\tLoss: 0.000155\n",
      "====> Epoch: 60 Average loss: 0.0001\n",
      "Train Epoch: 61 [0/1000 (0%)]\tLoss: 0.000146\n",
      "====> Epoch: 61 Average loss: 0.0001\n",
      "Train Epoch: 62 [0/1000 (0%)]\tLoss: 0.000143\n",
      "====> Epoch: 62 Average loss: 0.0001\n",
      "Train Epoch: 63 [0/1000 (0%)]\tLoss: 0.000153\n",
      "====> Epoch: 63 Average loss: 0.0001\n",
      "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.000149\n",
      "====> Epoch: 64 Average loss: 0.0001\n",
      "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.000133\n",
      "====> Epoch: 65 Average loss: 0.0001\n",
      "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.000138\n",
      "====> Epoch: 66 Average loss: 0.0001\n",
      "Train Epoch: 67 [0/1000 (0%)]\tLoss: 0.000137\n",
      "====> Epoch: 67 Average loss: 0.0001\n",
      "Train Epoch: 68 [0/1000 (0%)]\tLoss: 0.000123\n",
      "====> Epoch: 68 Average loss: 0.0001\n",
      "Train Epoch: 69 [0/1000 (0%)]\tLoss: 0.000131\n",
      "====> Epoch: 69 Average loss: 0.0001\n",
      "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.000130\n",
      "====> Epoch: 70 Average loss: 0.0001\n",
      "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.000146\n",
      "====> Epoch: 71 Average loss: 0.0001\n",
      "Train Epoch: 72 [0/1000 (0%)]\tLoss: 0.000125\n",
      "====> Epoch: 72 Average loss: 0.0001\n",
      "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.000128\n",
      "====> Epoch: 73 Average loss: 0.0001\n",
      "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.000133\n",
      "====> Epoch: 74 Average loss: 0.0001\n",
      "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.000127\n",
      "====> Epoch: 75 Average loss: 0.0001\n",
      "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.000128\n",
      "====> Epoch: 76 Average loss: 0.0001\n",
      "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.000126\n",
      "====> Epoch: 77 Average loss: 0.0001\n",
      "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.000124\n",
      "====> Epoch: 78 Average loss: 0.0001\n",
      "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.000118\n",
      "====> Epoch: 79 Average loss: 0.0001\n",
      "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.000117\n",
      "====> Epoch: 80 Average loss: 0.0001\n",
      "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.000116\n",
      "====> Epoch: 81 Average loss: 0.0001\n",
      "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.000112\n",
      "====> Epoch: 82 Average loss: 0.0001\n",
      "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.000117\n",
      "====> Epoch: 83 Average loss: 0.0001\n",
      "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.000113\n",
      "====> Epoch: 84 Average loss: 0.0001\n",
      "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.000122\n",
      "====> Epoch: 85 Average loss: 0.0001\n",
      "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.000110\n",
      "====> Epoch: 86 Average loss: 0.0001\n",
      "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.000110\n",
      "====> Epoch: 87 Average loss: 0.0001\n",
      "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.000112\n",
      "====> Epoch: 88 Average loss: 0.0001\n",
      "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.000111\n",
      "====> Epoch: 89 Average loss: 0.0001\n",
      "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.000109\n",
      "====> Epoch: 90 Average loss: 0.0001\n",
      "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.000106\n",
      "====> Epoch: 91 Average loss: 0.0001\n",
      "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.000106\n",
      "====> Epoch: 92 Average loss: 0.0001\n",
      "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.000109\n",
      "====> Epoch: 93 Average loss: 0.0001\n",
      "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.000108\n",
      "====> Epoch: 94 Average loss: 0.0001\n",
      "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.000104\n",
      "====> Epoch: 95 Average loss: 0.0001\n",
      "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.000101\n",
      "====> Epoch: 96 Average loss: 0.0001\n",
      "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.000091\n",
      "====> Epoch: 97 Average loss: 0.0001\n",
      "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.000102\n",
      "====> Epoch: 98 Average loss: 0.0001\n",
      "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.000103\n",
      "====> Epoch: 99 Average loss: 0.0001\n",
      "Train Epoch: 100 [0/1000 (0%)]\tLoss: 0.000096\n",
      "====> Epoch: 100 Average loss: 0.0001\n",
      "Train Epoch: 101 [0/1000 (0%)]\tLoss: 0.000098\n",
      "====> Epoch: 101 Average loss: 0.0001\n",
      "Train Epoch: 102 [0/1000 (0%)]\tLoss: 0.000099\n",
      "====> Epoch: 102 Average loss: 0.0001\n",
      "Train Epoch: 103 [0/1000 (0%)]\tLoss: 0.000093\n",
      "====> Epoch: 103 Average loss: 0.0001\n",
      "Train Epoch: 104 [0/1000 (0%)]\tLoss: 0.000093\n",
      "====> Epoch: 104 Average loss: 0.0001\n",
      "Train Epoch: 105 [0/1000 (0%)]\tLoss: 0.000092\n",
      "====> Epoch: 105 Average loss: 0.0001\n",
      "Train Epoch: 106 [0/1000 (0%)]\tLoss: 0.000096\n",
      "====> Epoch: 106 Average loss: 0.0001\n",
      "Train Epoch: 107 [0/1000 (0%)]\tLoss: 0.000098\n",
      "====> Epoch: 107 Average loss: 0.0001\n",
      "Train Epoch: 108 [0/1000 (0%)]\tLoss: 0.000098\n",
      "====> Epoch: 108 Average loss: 0.0001\n",
      "Train Epoch: 109 [0/1000 (0%)]\tLoss: 0.000091\n",
      "====> Epoch: 109 Average loss: 0.0001\n",
      "Train Epoch: 110 [0/1000 (0%)]\tLoss: 0.000095\n",
      "====> Epoch: 110 Average loss: 0.0001\n",
      "Train Epoch: 111 [0/1000 (0%)]\tLoss: 0.000092\n",
      "====> Epoch: 111 Average loss: 0.0001\n",
      "Train Epoch: 112 [0/1000 (0%)]\tLoss: 0.000092\n",
      "====> Epoch: 112 Average loss: 0.0001\n",
      "Train Epoch: 113 [0/1000 (0%)]\tLoss: 0.000092\n",
      "====> Epoch: 113 Average loss: 0.0001\n",
      "Train Epoch: 114 [0/1000 (0%)]\tLoss: 0.000088\n",
      "====> Epoch: 114 Average loss: 0.0001\n",
      "Train Epoch: 115 [0/1000 (0%)]\tLoss: 0.000087\n",
      "====> Epoch: 115 Average loss: 0.0001\n",
      "Train Epoch: 116 [0/1000 (0%)]\tLoss: 0.000094\n",
      "====> Epoch: 116 Average loss: 0.0001\n",
      "Train Epoch: 117 [0/1000 (0%)]\tLoss: 0.000088\n",
      "====> Epoch: 117 Average loss: 0.0001\n",
      "Train Epoch: 118 [0/1000 (0%)]\tLoss: 0.000087\n",
      "====> Epoch: 118 Average loss: 0.0001\n",
      "Train Epoch: 119 [0/1000 (0%)]\tLoss: 0.000086\n",
      "====> Epoch: 119 Average loss: 0.0001\n",
      "Train Epoch: 120 [0/1000 (0%)]\tLoss: 0.000090\n",
      "====> Epoch: 120 Average loss: 0.0001\n",
      "Train Epoch: 121 [0/1000 (0%)]\tLoss: 0.000086\n",
      "====> Epoch: 121 Average loss: 0.0001\n",
      "Train Epoch: 122 [0/1000 (0%)]\tLoss: 0.000090\n",
      "====> Epoch: 122 Average loss: 0.0001\n",
      "Train Epoch: 123 [0/1000 (0%)]\tLoss: 0.000094\n",
      "====> Epoch: 123 Average loss: 0.0001\n",
      "Train Epoch: 124 [0/1000 (0%)]\tLoss: 0.000088\n",
      "====> Epoch: 124 Average loss: 0.0001\n",
      "Train Epoch: 125 [0/1000 (0%)]\tLoss: 0.000081\n",
      "====> Epoch: 125 Average loss: 0.0001\n",
      "Train Epoch: 126 [0/1000 (0%)]\tLoss: 0.000085\n",
      "====> Epoch: 126 Average loss: 0.0001\n",
      "Train Epoch: 127 [0/1000 (0%)]\tLoss: 0.000076\n",
      "====> Epoch: 127 Average loss: 0.0001\n",
      "Train Epoch: 128 [0/1000 (0%)]\tLoss: 0.000075\n",
      "====> Epoch: 128 Average loss: 0.0001\n",
      "Train Epoch: 129 [0/1000 (0%)]\tLoss: 0.000082\n",
      "====> Epoch: 129 Average loss: 0.0001\n",
      "Train Epoch: 130 [0/1000 (0%)]\tLoss: 0.000082\n",
      "====> Epoch: 130 Average loss: 0.0001\n",
      "Train Epoch: 131 [0/1000 (0%)]\tLoss: 0.000082\n",
      "====> Epoch: 131 Average loss: 0.0001\n",
      "Train Epoch: 132 [0/1000 (0%)]\tLoss: 0.000079\n",
      "====> Epoch: 132 Average loss: 0.0001\n",
      "Train Epoch: 133 [0/1000 (0%)]\tLoss: 0.000074\n",
      "====> Epoch: 133 Average loss: 0.0001\n",
      "Train Epoch: 134 [0/1000 (0%)]\tLoss: 0.000073\n",
      "====> Epoch: 134 Average loss: 0.0001\n",
      "Train Epoch: 135 [0/1000 (0%)]\tLoss: 0.000081\n",
      "====> Epoch: 135 Average loss: 0.0001\n",
      "Train Epoch: 136 [0/1000 (0%)]\tLoss: 0.000074\n",
      "====> Epoch: 136 Average loss: 0.0001\n",
      "Train Epoch: 137 [0/1000 (0%)]\tLoss: 0.000079\n",
      "====> Epoch: 137 Average loss: 0.0001\n",
      "Train Epoch: 138 [0/1000 (0%)]\tLoss: 0.000071\n",
      "====> Epoch: 138 Average loss: 0.0001\n",
      "Train Epoch: 139 [0/1000 (0%)]\tLoss: 0.000076\n",
      "====> Epoch: 139 Average loss: 0.0001\n",
      "Train Epoch: 140 [0/1000 (0%)]\tLoss: 0.000077\n",
      "====> Epoch: 140 Average loss: 0.0001\n",
      "Train Epoch: 141 [0/1000 (0%)]\tLoss: 0.000069\n",
      "====> Epoch: 141 Average loss: 0.0001\n",
      "Train Epoch: 142 [0/1000 (0%)]\tLoss: 0.000069\n",
      "====> Epoch: 142 Average loss: 0.0001\n",
      "Train Epoch: 143 [0/1000 (0%)]\tLoss: 0.000070\n",
      "====> Epoch: 143 Average loss: 0.0001\n",
      "Train Epoch: 144 [0/1000 (0%)]\tLoss: 0.000074\n",
      "====> Epoch: 144 Average loss: 0.0001\n",
      "Train Epoch: 145 [0/1000 (0%)]\tLoss: 0.000072\n",
      "====> Epoch: 145 Average loss: 0.0001\n",
      "Train Epoch: 146 [0/1000 (0%)]\tLoss: 0.000070\n",
      "====> Epoch: 146 Average loss: 0.0001\n",
      "Train Epoch: 147 [0/1000 (0%)]\tLoss: 0.000070\n",
      "====> Epoch: 147 Average loss: 0.0001\n",
      "Train Epoch: 148 [0/1000 (0%)]\tLoss: 0.000069\n",
      "====> Epoch: 148 Average loss: 0.0001\n",
      "Train Epoch: 149 [0/1000 (0%)]\tLoss: 0.000074\n",
      "====> Epoch: 149 Average loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, n_epochs):\n",
    "  train_ae(ae_model,mnist_train_loader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T8jXjdRyzMy2"
   },
   "outputs": [],
   "source": [
    "def display_images(imgs):\n",
    "  \n",
    "  r = 1\n",
    "  c = imgs.shape[0]\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  for j in range(c):\n",
    "    #black and white images\n",
    "    axs[j].imshow(pytorch_to_numpy(imgs[j, 0,:,:]), cmap='gray')\n",
    "    axs[j].axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def display_ae_images(ae_model, test_imgs):\n",
    "  n_images = 5\n",
    "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
    "  test_imgs = test_imgs[idx,:,:,:]\n",
    "  print(test_imgs.shape)\n",
    "\n",
    "  #get output images\n",
    "  output_imgs = pytorch_to_numpy(ae_model.forward( test_imgs ))\n",
    "  print(output_imgs.shape)\n",
    "  \n",
    "  r = 2\n",
    "  c = n_images\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  for j in range(c):\n",
    "    #black and white images\n",
    "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
    "    axs[0,j].axis('off')\n",
    "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
    "    axs[1,j].axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9pbXch29d68D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 28, 28])\n",
      "(5, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_imgs = next(iter(mnist_train_loader))[0]\n",
    "display_ae_images(ae_model, test_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assPaJqB5sa-"
   },
   "source": [
    "__Question__ Are you satisfied with the results, do they look good ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7uoWa4O5x8R"
   },
   "source": [
    "__Your answer:__\n",
    "Overall, digits look the same despite the auto-encoder compression (factor 1.276%)\n",
    "The images look slightly blurier than the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression factor 1.276%\n"
     ]
    }
   ],
   "source": [
    "print(f\"compression factor {z_dim/n_pixels:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO9ATQEiyr3b"
   },
   "source": [
    "## 2/ Two simple generative models\n",
    "\n",
    "In this section, we consider two naïve approaches to creating generative models. The general idea is the following:\n",
    "\n",
    "- train an autoencoder\n",
    "- estimate different statistics (average, variance) of the data in the latent space\n",
    "- using these statistics, define a model based on a Gaussian distribution\n",
    "- generate data with this distribution\n",
    "\n",
    "We will consider these two situations :\n",
    "\n",
    "- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent component is an independent random variable). This requires the average and variance in each latent component\n",
    "- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the average and covariance matrix of the latent components\n",
    "\n",
    "Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2M1-BRmf56d"
   },
   "source": [
    "## 2.0 Defining and generating random Gaussian latent codes\n",
    "\n",
    "Let $z$ be a latent code and $d$ the dimension of the latent space (called ``z_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n",
    "\n",
    "\\begin{equation}\n",
    "z \\sim \\mathcal{N}\\left(\n",
    "\\mu,\n",
    "\\bf{C}\n",
    "\\right),\n",
    "\\end{equation}\n",
    "where $\\mu$ and $\\bf{C}$ are the average vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\mu + {\\bf{L}} \\varepsilon,\n",
    "\\end{equation}\n",
    "where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is the Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n",
    "\\end{equation}\n",
    "\n",
    "This gives a simple method of producing a multivariate Gaussian random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWpucm972i7j"
   },
   "source": [
    "## 2.1/ A Gaussian model with diagonal covariance\n",
    "\n",
    "The first naïve model is  defined in this first case as:\n",
    "\n",
    "- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n",
    "- $\n",
    "  \\bf{C} = \\begin{pmatrix}\n",
    "\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_1^2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma_{d-1}^2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n",
    "- $\n",
    "  \\bf{L} = \\begin{pmatrix}\n",
    "\\sigma_0 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma_{d-1}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "In the next cell, calculate the empirical average and variances over a certain number of batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUXHCtvW2iQ0"
   },
   "outputs": [],
   "source": [
    "n_batches = np.floor( len(mnist_train_loader.dataset.indices)/batch_size ).astype(int)\n",
    "\n",
    "z_average = torch.zeros(n_batches,ae_model.z_dim)\n",
    "z_sigma = torch.zeros(n_batches,ae_model.z_dim)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
    "  z = ae_model.encoder(data)\n",
    "  z_average[batch_idx,:] = ... # FILL IN CODE HERE\n",
    "  z_sigma[batch_idx,:] = ... # FILL IN CODE HERE\n",
    "\n",
    "z_average = ... # FILL IN CODE HERE\n",
    "z_sigma = ... # FILL IN CODE HERE\n",
    "\n",
    "print(\"Average of latent codes:\",z_average)\n",
    "print(\"Standard deviation of latent codes:\",z_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpc62ML9K4l"
   },
   "source": [
    "Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the ``display_images`` function. \n",
    "\n",
    "__NB__ You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n",
    "\n",
    "- ```torch.randn```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_Tekii-9QEo"
   },
   "outputs": [],
   "source": [
    "def generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 5):\n",
    "\n",
    "  epsilon = ... # FILL IN CODE HERE\n",
    "  z_generated = ... # FILL IN CODE HERE\n",
    "  imgs_generated = ... # FILL IN CODE HERE\n",
    "  return(imgs_generated)\n",
    "\n",
    "imgs_generated = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images=5)\n",
    "display_images(imgs_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiNaEgLIloeA"
   },
   "source": [
    "As you should be able to see, these results are not that good. Let's try a slightly more sophisticated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjVPfkRKYMSh"
   },
   "source": [
    "## 2.1 Non-diagonal Gaussian model\n",
    "\n",
    "The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the average and covariance matrix over several batches of latent codes.\n",
    "\n",
    "__NB__ You can use the ```torch.cov``` function. Make sure to put the data in the right format for this (see documentation : https://pytorch.org/docs/stable/generated/torch.cov.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArXgre39CD2H"
   },
   "outputs": [],
   "source": [
    "n_batches = np.floor( len(mnist_train_loader.dataset.indices)/batch_size ).astype(int)\n",
    "\n",
    "z_average = torch.zeros(n_batches,ae_model.z_dim)\n",
    "z_covariance = torch.zeros(n_batches,ae_model.z_dim,ae_model.z_dim)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
    "  z = ae_model.encoder(data)\n",
    "  z_average[batch_idx,:] = ... # FILL IN CODE HERE\n",
    "  z_covariance[batch_idx,:,:] = ... # FILL IN CODE HERE\n",
    "\n",
    "z_average = ... # FILL IN CODE HERE\n",
    "z_covariance = ... # FILL IN CODE HERE\n",
    "\n",
    "print(\"Average of latent codes:\",z_average)\n",
    "print(\"Covariance matrix of latent codes:\",z_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhXU8cnTZ0E8"
   },
   "source": [
    "Now, generate some samples with this distribution. In this case, you will actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use \n",
    "\n",
    "- ```torch.linalg.cholesky```\n",
    "\n",
    "In this model, you will need to carry out matrix multiplication over a batch of latent codes, which is a bit more complicated than the previous naïve model (which used element-wise vector multiplication). So you have two options:\n",
    "\n",
    "- copy the matrix $\\bf{L}$ several times and carry out batch matrix multiplication\n",
    "- simply loop and carry out normal matrix multiplication to produce each image (this has the disadvantage of not taking advantage of any parallelisation, but it should not matter too much).\n",
    "\n",
    "In the first case, you can use the following functions:\n",
    "\n",
    "- ```torch.bmm```\n",
    "- ```torch.tile```\n",
    "\n",
    "Fill in the function to generate images using this model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXGlJTZ7Z4ed"
   },
   "outputs": [],
   "source": [
    "def generate_images_non_diagonal_gaussian(ae_model,z_average,z_covariance,n_images = 5):\n",
    "\n",
    "  #calcualte Cholesky decomposition of covariance matrix : C = L L^T\n",
    "  # Make sure that the first dimension is the batch dimension (with batch size n_images)\n",
    "  # You can use the torch.unsqueeze function for this\n",
    "  L = ... # FILL IN CODE HERE\n",
    "\n",
    "  epsilon = ... # FILL IN CODE HERE\n",
    "  z_generated = ... # FILL IN CODE HERE\n",
    "  imgs_generated = ... # FILL IN CODE HERE\n",
    "  return(imgs_generated)\n",
    "\n",
    "\n",
    "imgs_generated = generate_images_non_diagonal_gaussian(ae_model,z_average,z_covariance,n_images = 5)\n",
    "display_images(imgs_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLtsdri6zKEm"
   },
   "source": [
    "You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UqeNhuSdnDt"
   },
   "source": [
    "# 3/ Variational autoencoder\n",
    "\n",
    "Now, we are going to create an variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder\n",
    "\n",
    "## Main idea\n",
    "\n",
    "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools : \n",
    "\n",
    "- A specific architecture, where the encoder produces the average and variance of the latent codes\n",
    "- A specially designed loss function\n",
    "\n",
    "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture of the VAE model is as follows:\n",
    "\n",
    "The encoder consists of:\n",
    "\n",
    "Encoder :\n",
    "- Flatten input\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer (no non-linarity) to produce the average, Dense layer (no non-linarity) to produce the variance (these last two layers are in parallel)\n",
    "\n",
    "Decoder :\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ Sigmoid Activation\n",
    "- Reshape, to size $28\\times 28\\times 1$\n",
    "\n",
    "\n",
    "## Variational Autoencoder loss\n",
    "\n",
    "Recall that for the VAE, the loss function is in fact a function to __maximise__. In fact, for implementation, you will see that it is easier to __minimise__ $-\\mathcal{L}$.\n",
    "\n",
    "In the case of an image which is represented by a set of __Bernoulli__ variables (which is relevant for mnist), the original loss function (to maximise) is written :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\log\\left(p_\\theta(x|z)\\right) - KL\\left( q_\\phi(z|x) \\; || \\; p_\\theta(z)\\right) \\\\\n",
    "    &= \\left(\\sum_{i} x_i \\log y_i + (1-x_i) \\log (1-y_i)\\right) - \\left(\\frac{1}{2} \\sum_j \\left( \\sigma_j^2 + \\mu_j^2 - \\log \\sigma_j^2 -1 \\right)\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $i$ is summed over the image pixels, and $j$ is summed over the elements of the latent space. $\\sigma_j^2$ is the $j$th element of the latent space variance, and $\\mu_j$ is the $j$th element of the latent space mean.\n",
    "\n",
    "The left part of the loss (reconstruction error) can be implemented simply as the binary cross-entropy between the input x and the output y. Since we are __maximising__ $-$[binary cross-entropy] (look at the formula), this is equivalent to minimising the binary cross-entropy.\n",
    "\n",
    "For the right part of the equation (KL divergence), you need to implement it manually. \n",
    "\n",
    "The final loss is the average, over the batch size, of the sum of the reconstruction error (left part) and the KL divergence (right part). Be careful, in the formula, the sums over $i$ and $j$ are over the number of pixels and the number of latent elements, respectively. To achieve a sum rather than an average, you can use ```torch.nn.BCELoss(reduction='sum')()```, and the ```torch.sum()``` functions.\n",
    "\n",
    "As in the case of the normal autoencoder, you will need to flatten and then reshape the tensors at the beginning/end of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "6siMHQLheM4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "class VAE(torch.nn.Module ):\n",
    "\n",
    "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
    "    super(VAE, self).__init__()\n",
    "\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.n_channels = n_channels\n",
    "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
    "    self.z_dim = z_dim\n",
    "\n",
    "    # encoder part\n",
    "    self.non_lin = torch.nn.ReLU()\n",
    "    self.fc1 =  torch.nn.Linear(x_dim,  h_dim1)\n",
    "    self.fc2 = torch.nn.Linear(h_dim1,  h_dim2)\n",
    "    self.fc31 = torch.nn.Linear(h_dim2,  z_dim)\n",
    "    self.fc32 = torch.nn.Linear(h_dim2,  z_dim)\n",
    "    \n",
    "    self.encoder_start = torch.nn.Sequential(\n",
    "      self.fc1, self.non_lin, self.fc2, self.non_lin\n",
    "    )\n",
    "    # decoder part\n",
    "    self.fc4 = torch.nn.Linear(z_dim,  h_dim2)\n",
    "    self.fc5 = torch.nn.Linear(h_dim2,  h_dim1)\n",
    "    self.fc6 = torch.nn.Linear(h_dim1,  x_dim)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "    self.decoder = torch.nn.Sequential(\n",
    "      self.fc4, self.non_lin, self.fc5, self.non_lin, self.fc6, self.sigmoid\n",
    "    )\n",
    "\n",
    "  def encoder(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    h = self.encoder_start(x)\n",
    "    mu = self.fc31(h)\n",
    "    log_var = self.fc32(h)\n",
    "    return mu, log_var # mu, log_var\n",
    "\n",
    "  def log_var_to_var(self, log_var: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(log_var)\n",
    "  def sampling(self, mu, log_var):\n",
    "    # this function samples a Gaussian distribution, with average (mu) and standard deviation specified (using log_var)\n",
    "    std = self.log_var_to_var(log_var)\n",
    "    eps = torch.randn(mu.shape, requires_grad=False)\n",
    "    return eps.mul(std).add_(mu) # return z sample\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    z_mu, z_log_var = self.encoder(x.view(-1, self.n_pixels))\n",
    "    z = self.sampling(z_mu, z_log_var)\n",
    "    y = self.decoder(z)\n",
    "    y = y.view(x.shape)\n",
    "    return y, z_mu, z_log_var\n",
    "\n",
    "  def loss_function(self,x: torch.Tensor, y: torch.Tensor, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Cross entropy loss function + KL divergence on the latent space µ and sigma\n",
    "    Warning: \n",
    "    - the reconstruction error should not be divided by the number of pixels in the image\n",
    "    - 1/2 is the KL weight\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): input N, C, H, W\n",
    "        y (torch.Tensor): output (to compute the reconstuction error)\n",
    "        mu (torch.Tensor): gaussian average µ\n",
    "        log_var (torch.Tensor): log of the variance (so we avoid negative variances)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: _description_\n",
    "    \"\"\"\n",
    "    variance = self.log_var_to_var(log_var) #exp basically\n",
    "    # loss should be around 105-115\n",
    "    # reconstruction_error =  (x-y).sum(axis=(-1, -2))\n",
    "    \n",
    "    \n",
    "    # reconstruction_error = x* torch.log(y) + (1.-x)*torch.log(1.-y) # y between 0 and 1 thanks to the sigmoid\n",
    "    # reconstruction_error = reconstruction_error.sum(dim=(-1, -2))\n",
    "    reconstruction_loss = torch.nn.BCELoss(reduction=\"sum\")\n",
    "    reconstruction_error = reconstruction_loss(y, x)\n",
    "    # print(reconstruction_error)\n",
    "    # reconstruction_error = reconstruction_error.squeeze()\n",
    "    kld = mu**2 + variance -1 - log_var\n",
    "    kld = kld.sum()\n",
    "    # kld = kld.sum(axis=-1)\n",
    "    # print(kld.shape)\n",
    "    loss = (reconstruction_error + 0.5*kld)/x.shape[0] #average over the batch dimension!\n",
    "\n",
    "    return loss # FILL IN CODE HERE\n",
    "\n",
    "\n",
    "vae_dim_1 = ae_dim_1\n",
    "vae_dim_2 = ae_dim_2\n",
    "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
    "for x_in in [torch.rand(8, 1, 28, 28), torch.rand(8, 28, 28)]:\n",
    "  y_out, mu_out, log_var_out  = vae_model(x_in)\n",
    "  assert y_out.shape == x_in.shape\n",
    "  # print(mu_out.shape, log_var_out.shape)\n",
    "  my_loss = vae_model.loss_function(x_in, y_out, mu_out, log_var_out)\n",
    "  print(my_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk_9fDIphlsi"
   },
   "source": [
    "Now, create the model (similarly as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "pVlpC2R3htyU"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "vae_dim_1 = ae_dim_1\n",
    "vae_dim_2 = ae_dim_2\n",
    "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
    "vae_optimizer = optim.Adam(vae_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYKLF_oMh5HO"
   },
   "source": [
    "Finally, train the model. First modify the training function to the case of the vae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "z6DjKTWmmssb"
   },
   "outputs": [],
   "source": [
    "def train_vae(vae_model,data_train_loader,epoch):\n",
    "  train_loss = 0\n",
    "  for batch_idx, (data, _) in enumerate(data_train_loader):\n",
    "    vae_optimizer.zero_grad()\n",
    "\n",
    "    y, z_mu, z_log_var = vae_model(data) # FILL IN CODE HERE\n",
    "    loss_vae =  vae_model.loss_function(data, y, z_mu, z_log_var) # FILL IN CODE HERE\n",
    "    loss_vae_neg  = - loss_vae\n",
    "    (loss_vae_neg).backward()\n",
    "    train_loss += loss_vae.item()\n",
    "    vae_optimizer.step() \n",
    "\t\t\n",
    "    if batch_idx % 100 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "      epoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
    "      100. * batch_idx / len(data_train_loader), loss_vae.item() / len(data)))\n",
    "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "L9JUUs6Kh8HB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1000 (0%)]\tLoss: 4.250200\n",
      "====> Epoch: 0 Average loss: 6.6959\n",
      "Train Epoch: 1 [0/1000 (0%)]\tLoss: 71.561852\n",
      "====> Epoch: 1 Average loss: 1983738.7816\n",
      "Train Epoch: 2 [0/1000 (0%)]\tLoss: 1487892.750000\n",
      "====> Epoch: 2 Average loss: 131902256417236896.0000\n",
      "Train Epoch: 3 [0/1000 (0%)]\tLoss: 14433050543792324608.000000\n",
      "====> Epoch: 3 Average loss: 27942987057243862107737292800.0000\n",
      "Train Epoch: 4 [0/1000 (0%)]\tLoss: 13531823558669498461454336.000000\n",
      "====> Epoch: 4 Average loss: 27956817415785003432477196288.0000\n",
      "Train Epoch: 5 [0/1000 (0%)]\tLoss: 61223530707219155410485248.000000\n",
      "====> Epoch: 5 Average loss: 28297873289403886985019392000.0000\n",
      "Train Epoch: 6 [0/1000 (0%)]\tLoss: 205450992861737812488345354240.000000\n",
      "====> Epoch: 6 Average loss: 28299999844012090607630548992.0000\n",
      "Train Epoch: 7 [0/1000 (0%)]\tLoss: 511610817625381142528.000000\n",
      "====> Epoch: 7 Average loss: 28494307927593496537498386432.0000\n",
      "Train Epoch: 8 [0/1000 (0%)]\tLoss: 23027042254533980848128.000000\n",
      "====> Epoch: 8 Average loss: 28497510172764951246482178048.0000\n",
      "Train Epoch: 9 [0/1000 (0%)]\tLoss: 15665645643434718721927544832.000000\n",
      "====> Epoch: 9 Average loss: 28498622307632868854675275776.0000\n",
      "Train Epoch: 10 [0/1000 (0%)]\tLoss: 206902742765901598819939778560.000000\n",
      "====> Epoch: 10 Average loss: 28498644922538723617099743232.0000\n",
      "Train Epoch: 11 [0/1000 (0%)]\tLoss: 1870178328089551299411968.000000\n",
      "====> Epoch: 11 Average loss: 28498462285756378213557207040.0000\n",
      "Train Epoch: 12 [0/1000 (0%)]\tLoss: 222566549047591239815056588800.000000\n",
      "====> Epoch: 12 Average loss: 28498637219716290079610634240.0000\n",
      "Train Epoch: 13 [0/1000 (0%)]\tLoss: 61820209091027364567056384.000000\n",
      "====> Epoch: 13 Average loss: 28498463261078194020537073664.0000\n",
      "Train Epoch: 14 [0/1000 (0%)]\tLoss: 13599427416935130147586048.000000\n",
      "====> Epoch: 14 Average loss: 28498644846310272956691906560.0000\n",
      "Train Epoch: 15 [0/1000 (0%)]\tLoss: 1480581075640172507299840.000000\n",
      "====> Epoch: 15 Average loss: 28498410701768218034843942912.0000\n",
      "Train Epoch: 16 [0/1000 (0%)]\tLoss: 53507940073905061888.000000\n",
      "====> Epoch: 16 Average loss: 26493493452785118304196886528.0000\n",
      "Train Epoch: 17 [0/1000 (0%)]\tLoss: 61819720252309411263938560.000000\n",
      "====> Epoch: 17 Average loss: 26493674238137027975077429248.0000\n",
      "Train Epoch: 18 [0/1000 (0%)]\tLoss: 16773762646732655558656.000000\n",
      "====> Epoch: 18 Average loss: 28498631557243145419348770816.0000\n",
      "Train Epoch: 19 [0/1000 (0%)]\tLoss: 415414191356475688878080.000000\n",
      "====> Epoch: 19 Average loss: 28498463965309691742786682880.0000\n",
      "Train Epoch: 20 [0/1000 (0%)]\tLoss: 15372044230268157373186048.000000\n",
      "====> Epoch: 20 Average loss: 28498645716651026406875070464.0000\n",
      "Train Epoch: 21 [0/1000 (0%)]\tLoss: 46201325195285212692480.000000\n",
      "====> Epoch: 21 Average loss: 28498645786090423774324195328.0000\n",
      "Train Epoch: 22 [0/1000 (0%)]\tLoss: 483994502078915336470528.000000\n",
      "====> Epoch: 22 Average loss: 26493676256412714927120187392.0000\n",
      "Train Epoch: 23 [0/1000 (0%)]\tLoss: 15666024613344969010955943936.000000\n",
      "====> Epoch: 23 Average loss: 2015092065526491110975733760.0000\n",
      "Train Epoch: 24 [0/1000 (0%)]\tLoss: 13602338543734262436200448.000000\n",
      "====> Epoch: 24 Average loss: 28498642885910973736638480384.0000\n",
      "Train Epoch: 25 [0/1000 (0%)]\tLoss: 660812399267886923776.000000\n",
      "====> Epoch: 25 Average loss: 28498645318873170972678029312.0000\n",
      "Train Epoch: 26 [0/1000 (0%)]\tLoss: 416041478045323755520.000000\n",
      "====> Epoch: 26 Average loss: 28498645681218204272584294400.0000\n",
      "Train Epoch: 27 [0/1000 (0%)]\tLoss: 61820121468993014446686208.000000\n",
      "====> Epoch: 27 Average loss: 28498640253856744796594372608.0000\n",
      "Train Epoch: 28 [0/1000 (0%)]\tLoss: 15663831074113676060754182144.000000\n",
      "====> Epoch: 28 Average loss: 28498645733313095982936752128.0000\n",
      "Train Epoch: 29 [0/1000 (0%)]\tLoss: 1814770073499650840592384.000000\n",
      "====> Epoch: 29 Average loss: 28498642654195257758101536768.0000\n",
      "Train Epoch: 30 [0/1000 (0%)]\tLoss: 45699317952021478244352.000000\n",
      "====> Epoch: 30 Average loss: 26493486604544529482117021696.0000\n",
      "Train Epoch: 31 [0/1000 (0%)]\tLoss: 46540910117987830202368.000000\n",
      "====> Epoch: 31 Average loss: 28498645917177775649592442880.0000\n",
      "Train Epoch: 32 [0/1000 (0%)]\tLoss: 206903082776288365434395164672.000000\n",
      "====> Epoch: 32 Average loss: 26493443704340541552317169664.0000\n",
      "Train Epoch: 33 [0/1000 (0%)]\tLoss: 521444919992953864192.000000\n",
      "====> Epoch: 33 Average loss: 28498639204677166136180080640.0000\n",
      "Train Epoch: 34 [0/1000 (0%)]\tLoss: 382265031968049434984448.000000\n",
      "====> Epoch: 34 Average loss: 28498645323019174234323681280.0000\n",
      "Train Epoch: 35 [0/1000 (0%)]\tLoss: 15663829893522055343342878720.000000\n",
      "====> Epoch: 35 Average loss: 28498646059588505198116470784.0000\n",
      "Train Epoch: 36 [0/1000 (0%)]\tLoss: 399050848499184747675648.000000\n",
      "====> Epoch: 36 Average loss: 28498642908434689333955395584.0000\n",
      "Train Epoch: 37 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 37 Average loss: 28498642756064763780943314944.0000\n",
      "Train Epoch: 38 [0/1000 (0%)]\tLoss: 206964567987895328215077486592.000000\n",
      "====> Epoch: 38 Average loss: 26493443452595479647421988864.0000\n",
      "Train Epoch: 39 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 39 Average loss: 26493438253337730501562073088.0000\n",
      "Train Epoch: 40 [0/1000 (0%)]\tLoss: 384596671595928707137536.000000\n",
      "====> Epoch: 40 Average loss: 28498645479263602841757941760.0000\n",
      "Train Epoch: 41 [0/1000 (0%)]\tLoss: 206903082776288365434395164672.000000\n",
      "====> Epoch: 41 Average loss: 28498645886787036763352203264.0000\n",
      "Train Epoch: 42 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 42 Average loss: 28498638456026599318477602816.0000\n",
      "Train Epoch: 43 [0/1000 (0%)]\tLoss: 384686563444491022237696.000000\n",
      "====> Epoch: 43 Average loss: 28498639464238942075958067200.0000\n",
      "Train Epoch: 44 [0/1000 (0%)]\tLoss: 1817322209365286172229632.000000\n",
      "====> Epoch: 44 Average loss: 28498639114102850706504417280.0000\n",
      "Train Epoch: 45 [0/1000 (0%)]\tLoss: 94984942359310629863424.000000\n",
      "====> Epoch: 45 Average loss: 26491940912473651445246197760.0000\n",
      "Train Epoch: 46 [0/1000 (0%)]\tLoss: 23803512369888555433984.000000\n",
      "====> Epoch: 46 Average loss: 28498633612182782073349603328.0000\n",
      "Train Epoch: 47 [0/1000 (0%)]\tLoss: 17192511843685191647232.000000\n",
      "====> Epoch: 47 Average loss: 28498465446664964482323709952.0000\n",
      "Train Epoch: 48 [0/1000 (0%)]\tLoss: 75153453990175334989824.000000\n",
      "====> Epoch: 48 Average loss: 28498414227158333831820345344.0000\n",
      "Train Epoch: 49 [0/1000 (0%)]\tLoss: 2258270009169101625229312.000000\n",
      "====> Epoch: 49 Average loss: 28498647002694172914422382592.0000\n",
      "Train Epoch: 50 [0/1000 (0%)]\tLoss: 159602644981917941760.000000\n",
      "====> Epoch: 50 Average loss: 28496729766772053875911294976.0000\n",
      "Train Epoch: 51 [0/1000 (0%)]\tLoss: 19807490938520851185664.000000\n",
      "====> Epoch: 51 Average loss: 28498633035977872931570057216.0000\n",
      "Train Epoch: 52 [0/1000 (0%)]\tLoss: 17716200541454994505728.000000\n",
      "====> Epoch: 52 Average loss: 28498646245269749597672046592.0000\n",
      "Train Epoch: 53 [0/1000 (0%)]\tLoss: 3231735246358626745450496.000000\n",
      "====> Epoch: 53 Average loss: 28498642365791097196550029312.0000\n",
      "Train Epoch: 54 [0/1000 (0%)]\tLoss: 15665698770057651005436198912.000000\n",
      "====> Epoch: 54 Average loss: 28498646026912664490008903680.0000\n",
      "Train Epoch: 55 [0/1000 (0%)]\tLoss: 1047638463580504850432.000000\n",
      "====> Epoch: 55 Average loss: 28498646205075879387390279680.0000\n",
      "Train Epoch: 56 [0/1000 (0%)]\tLoss: 329858238223031992320.000000\n",
      "====> Epoch: 56 Average loss: 28498465045031865039376613376.0000\n",
      "Train Epoch: 57 [0/1000 (0%)]\tLoss: 445093952950886203392.000000\n",
      "====> Epoch: 57 Average loss: 28498595103692646037141323776.0000\n",
      "Train Epoch: 58 [0/1000 (0%)]\tLoss: 124234146600020619558912.000000\n",
      "====> Epoch: 58 Average loss: 2007187198849042920622784512.0000\n",
      "Train Epoch: 59 [0/1000 (0%)]\tLoss: 803109047887959425024.000000\n",
      "====> Epoch: 59 Average loss: 28490494275578237219711746048.0000\n",
      "Train Epoch: 60 [0/1000 (0%)]\tLoss: 663499359395566845952.000000\n",
      "====> Epoch: 60 Average loss: 28498643499599091888371531776.0000\n",
      "Train Epoch: 61 [0/1000 (0%)]\tLoss: 330524102464813137920.000000\n",
      "====> Epoch: 61 Average loss: 28498405720728861599919505408.0000\n",
      "Train Epoch: 62 [0/1000 (0%)]\tLoss: 22545850648747952832512.000000\n",
      "====> Epoch: 62 Average loss: 28498646090269123727950086144.0000\n",
      "Train Epoch: 63 [0/1000 (0%)]\tLoss: 2739960739894631858176.000000\n",
      "====> Epoch: 63 Average loss: 28498639691359957291693506560.0000\n",
      "Train Epoch: 64 [0/1000 (0%)]\tLoss: 50626580203537971544064.000000\n",
      "====> Epoch: 64 Average loss: 28498645396028307625161523200.0000\n",
      "Train Epoch: 65 [0/1000 (0%)]\tLoss: 1798646754488100237344768.000000\n",
      "====> Epoch: 65 Average loss: 26491941413408039641108447232.0000\n",
      "Train Epoch: 66 [0/1000 (0%)]\tLoss: 63332689919432939695767552.000000\n",
      "====> Epoch: 66 Average loss: 2015100050180077155822600192.0000\n",
      "Train Epoch: 67 [0/1000 (0%)]\tLoss: 1465152968295899832778752.000000\n",
      "====> Epoch: 67 Average loss: 28498637503948010721938243584.0000\n",
      "Train Epoch: 68 [0/1000 (0%)]\tLoss: 854755624727202496512.000000\n",
      "====> Epoch: 68 Average loss: 28498411769993748217826639872.0000\n",
      "Train Epoch: 69 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 69 Average loss: 28498416176731538099582009344.0000\n",
      "Train Epoch: 70 [0/1000 (0%)]\tLoss: 864682402730857201664.000000\n",
      "====> Epoch: 70 Average loss: 28498639286826636445743054848.0000\n",
      "Train Epoch: 71 [0/1000 (0%)]\tLoss: 59190035283394521202688.000000\n",
      "====> Epoch: 71 Average loss: 28490724659611828119847043072.0000\n",
      "Train Epoch: 72 [0/1000 (0%)]\tLoss: 1420600334363001293373440.000000\n",
      "====> Epoch: 72 Average loss: 28496905272255145576581038080.0000\n",
      "Train Epoch: 73 [0/1000 (0%)]\tLoss: 5069814690521651609600.000000\n",
      "====> Epoch: 73 Average loss: 28496861385186440072990294016.0000\n",
      "Train Epoch: 74 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 74 Average loss: 28498413785792890781420224512.0000\n",
      "Train Epoch: 75 [0/1000 (0%)]\tLoss: 382657133366006819848192.000000\n",
      "====> Epoch: 75 Average loss: 2015094489052363997604478976.0000\n",
      "Train Epoch: 76 [0/1000 (0%)]\tLoss: 206966343597692887201677836288.000000\n",
      "====> Epoch: 76 Average loss: 28498646968606704411785297920.0000\n",
      "Train Epoch: 77 [0/1000 (0%)]\tLoss: 61825221993729395137708032.000000\n",
      "====> Epoch: 77 Average loss: 28498595154894857451041980416.0000\n",
      "Train Epoch: 78 [0/1000 (0%)]\tLoss: 46624519445069963460608.000000\n",
      "====> Epoch: 78 Average loss: 28490730731619373886486872064.0000\n",
      "Train Epoch: 79 [0/1000 (0%)]\tLoss: 425797438455761971707904.000000\n",
      "====> Epoch: 79 Average loss: 28498645773220288327485227008.0000\n",
      "Train Epoch: 80 [0/1000 (0%)]\tLoss: 206904575044096952242282692608.000000\n",
      "====> Epoch: 80 Average loss: 28498638160565034258045861888.0000\n",
      "Train Epoch: 81 [0/1000 (0%)]\tLoss: 15368712287119843585425408.000000\n",
      "====> Epoch: 81 Average loss: 28490730712434470482446647296.0000\n",
      "Train Epoch: 82 [0/1000 (0%)]\tLoss: 15727466143061965247420039168.000000\n",
      "====> Epoch: 82 Average loss: 28498646204016376788725792768.0000\n",
      "Train Epoch: 83 [0/1000 (0%)]\tLoss: 20348035483795994968064.000000\n",
      "====> Epoch: 83 Average loss: 28498645242579717184836075520.0000\n",
      "Train Epoch: 84 [0/1000 (0%)]\tLoss: 15370213390918841700188160.000000\n",
      "====> Epoch: 84 Average loss: 28498636220186227696723820544.0000\n",
      "Train Epoch: 85 [0/1000 (0%)]\tLoss: 1477021862840263094829056.000000\n",
      "====> Epoch: 85 Average loss: 28498645381694810162682920960.0000\n",
      "Train Epoch: 86 [0/1000 (0%)]\tLoss: 61882231656289194506977280.000000\n",
      "====> Epoch: 86 Average loss: 28498645061045487597623705600.0000\n",
      "Train Epoch: 87 [0/1000 (0%)]\tLoss: 3232320930482967023714304.000000\n",
      "====> Epoch: 87 Average loss: 28498645623478499030067052544.0000\n",
      "Train Epoch: 88 [0/1000 (0%)]\tLoss: 380112167230978262040576.000000\n",
      "====> Epoch: 88 Average loss: 28498463510526050679566368768.0000\n",
      "Train Epoch: 89 [0/1000 (0%)]\tLoss: 5411476898552393760768.000000\n",
      "====> Epoch: 89 Average loss: 28498413613167537721146605568.0000\n",
      "Train Epoch: 90 [0/1000 (0%)]\tLoss: 14977048711103833171820544.000000\n",
      "====> Epoch: 90 Average loss: 28498597605654396407100997632.0000\n",
      "Train Epoch: 91 [0/1000 (0%)]\tLoss: 206916267623508537483831803904.000000\n",
      "====> Epoch: 91 Average loss: 28498646793559311953474617344.0000\n",
      "Train Epoch: 92 [0/1000 (0%)]\tLoss: 381097014397491642105856.000000\n",
      "====> Epoch: 92 Average loss: 26493676116543919922572427264.0000\n",
      "Train Epoch: 93 [0/1000 (0%)]\tLoss: 381094024007339068096512.000000\n",
      "====> Epoch: 93 Average loss: 28498645563663870210401632256.0000\n",
      "Train Epoch: 94 [0/1000 (0%)]\tLoss: 18980758399525008179200.000000\n",
      "====> Epoch: 94 Average loss: 28496730624872722828296716288.0000\n",
      "Train Epoch: 95 [0/1000 (0%)]\tLoss: 383454738874412644171776.000000\n",
      "====> Epoch: 95 Average loss: 28496909028208047051394514944.0000\n",
      "Train Epoch: 96 [0/1000 (0%)]\tLoss: 1834035824187195481128960.000000\n",
      "====> Epoch: 96 Average loss: 28498597579979348543182733312.0000\n",
      "Train Epoch: 97 [0/1000 (0%)]\tLoss: 1814189145176517065572352.000000\n",
      "====> Epoch: 97 Average loss: 28498635767224846836635271168.0000\n",
      "Train Epoch: 98 [0/1000 (0%)]\tLoss: 15665247784058536954318290944.000000\n",
      "====> Epoch: 98 Average loss: 28498646451368463130144276480.0000\n",
      "Train Epoch: 99 [0/1000 (0%)]\tLoss: 384889621744489903161344.000000\n",
      "====> Epoch: 99 Average loss: 26485763547127927083638980608.0000\n",
      "Train Epoch: 100 [0/1000 (0%)]\tLoss: 61824585581058852158177280.000000\n",
      "====> Epoch: 100 Average loss: 28498638171223396512497139712.0000\n",
      "Train Epoch: 101 [0/1000 (0%)]\tLoss: 206902761655367530298520633344.000000\n",
      "====> Epoch: 101 Average loss: 28490684112365951796826341376.0000\n",
      "Train Epoch: 102 [0/1000 (0%)]\tLoss: 944502584045255786496.000000\n",
      "====> Epoch: 102 Average loss: 28498645656391697910332391424.0000\n",
      "Train Epoch: 103 [0/1000 (0%)]\tLoss: 58523389948553003532288.000000\n",
      "====> Epoch: 103 Average loss: 28496679051790367796842463232.0000\n",
      "Train Epoch: 104 [0/1000 (0%)]\tLoss: 228807050326232268800.000000\n",
      "====> Epoch: 104 Average loss: 28490733044049816746930995200.0000\n",
      "Train Epoch: 105 [0/1000 (0%)]\tLoss: 380922022530370534113280.000000\n",
      "====> Epoch: 105 Average loss: 28498645416507010759652278272.0000\n",
      "Train Epoch: 106 [0/1000 (0%)]\tLoss: 1418840976146971244888064.000000\n",
      "====> Epoch: 106 Average loss: 26491934507155343769753092096.0000\n",
      "Train Epoch: 107 [0/1000 (0%)]\tLoss: 59331205117314076770304.000000\n",
      "====> Epoch: 107 Average loss: 26493676343187170947760128000.0000\n",
      "Train Epoch: 108 [0/1000 (0%)]\tLoss: 15353899551628654815477760.000000\n",
      "====> Epoch: 108 Average loss: 2015100093641371842802352128.0000\n",
      "Train Epoch: 109 [0/1000 (0%)]\tLoss: 1424060107683138365292544.000000\n",
      "====> Epoch: 109 Average loss: 2015100094959071107750035456.0000\n",
      "Train Epoch: 110 [0/1000 (0%)]\tLoss: 61844102236288836863787008.000000\n",
      "====> Epoch: 110 Average loss: 28498646351357998173406625792.0000\n",
      "Train Epoch: 111 [0/1000 (0%)]\tLoss: 1286019199144294350848.000000\n",
      "====> Epoch: 111 Average loss: 28498465464465741076668350464.0000\n",
      "Train Epoch: 112 [0/1000 (0%)]\tLoss: 1419188870210986360963072.000000\n",
      "====> Epoch: 112 Average loss: 28498645811106367193948880896.0000\n",
      "Train Epoch: 113 [0/1000 (0%)]\tLoss: 75477983379323652931584.000000\n",
      "====> Epoch: 113 Average loss: 28498645861461501655671373824.0000\n",
      "Train Epoch: 114 [0/1000 (0%)]\tLoss: 2730264771371879890944.000000\n",
      "====> Epoch: 114 Average loss: 28498455119544235036889841664.0000\n",
      "Train Epoch: 115 [0/1000 (0%)]\tLoss: 46291707936206911176704.000000\n",
      "====> Epoch: 115 Average loss: 2015099516298148208390963200.0000\n",
      "Train Epoch: 116 [0/1000 (0%)]\tLoss: 46877243441759486214144.000000\n",
      "====> Epoch: 116 Average loss: 28498637825100104770007334912.0000\n",
      "Train Epoch: 117 [0/1000 (0%)]\tLoss: 1813148057057857082753024.000000\n",
      "====> Epoch: 117 Average loss: 28498646902707140749495894016.0000\n",
      "Train Epoch: 118 [0/1000 (0%)]\tLoss: 15663831074113676060754182144.000000\n",
      "====> Epoch: 118 Average loss: 28498646445182966954043899904.0000\n",
      "Train Epoch: 119 [0/1000 (0%)]\tLoss: 58750533499359061868544.000000\n",
      "====> Epoch: 119 Average loss: 28498647148778537937113448448.0000\n",
      "Train Epoch: 120 [0/1000 (0%)]\tLoss: 13560027477436695759028224.000000\n",
      "====> Epoch: 120 Average loss: 28498590761907889839741599744.0000\n",
      "Train Epoch: 121 [0/1000 (0%)]\tLoss: 206904518375699157806540128256.000000\n",
      "====> Epoch: 121 Average loss: 28498645698213337910931881984.0000\n",
      "Train Epoch: 122 [0/1000 (0%)]\tLoss: 62248611663709176766398464.000000\n",
      "====> Epoch: 122 Average loss: 2015100109021524935208599552.0000\n",
      "Train Epoch: 123 [0/1000 (0%)]\tLoss: 15664208863432305632371277824.000000\n",
      "====> Epoch: 123 Average loss: 28498646251648232054679666688.0000\n",
      "Train Epoch: 124 [0/1000 (0%)]\tLoss: 1832048763974005580365824.000000\n",
      "====> Epoch: 124 Average loss: 28498416695949054242847719424.0000\n",
      "Train Epoch: 125 [0/1000 (0%)]\tLoss: 13593553281869158262243328.000000\n",
      "====> Epoch: 125 Average loss: 2015100425273739147460214784.0000\n",
      "Train Epoch: 126 [0/1000 (0%)]\tLoss: 1437331243007479704977408.000000\n",
      "====> Epoch: 126 Average loss: 2015038129361435256279793664.0000\n",
      "Train Epoch: 127 [0/1000 (0%)]\tLoss: 15663829893522055343342878720.000000\n",
      "====> Epoch: 127 Average loss: 28498648456294706730474930176.0000\n",
      "Train Epoch: 128 [0/1000 (0%)]\tLoss: 206916305402440400440993513472.000000\n",
      "====> Epoch: 128 Average loss: 28498647480981537482935894016.0000\n",
      "Train Epoch: 129 [0/1000 (0%)]\tLoss: 46654639519377817337856.000000\n",
      "====> Epoch: 129 Average loss: 28498647130720831863636623360.0000\n",
      "Train Epoch: 130 [0/1000 (0%)]\tLoss: 206904121696914596756342177792.000000\n",
      "====> Epoch: 130 Average loss: 28498645988264826375635992576.0000\n",
      "Train Epoch: 131 [0/1000 (0%)]\tLoss: 1829978693412483986620416.000000\n",
      "====> Epoch: 131 Average loss: 28498647140992280374320037888.0000\n",
      "Train Epoch: 132 [0/1000 (0%)]\tLoss: 1832890540787556654514176.000000\n",
      "====> Epoch: 132 Average loss: 28490677576073638700749160448.0000\n",
      "Train Epoch: 133 [0/1000 (0%)]\tLoss: 61914264427373191143358464.000000\n",
      "====> Epoch: 133 Average loss: 28498597189979007355315027968.0000\n",
      "Train Epoch: 134 [0/1000 (0%)]\tLoss: 1324015506250465804288.000000\n",
      "====> Epoch: 134 Average loss: 28498638646602640786473353216.0000\n",
      "Train Epoch: 135 [0/1000 (0%)]\tLoss: 222641918016657839352667176960.000000\n",
      "====> Epoch: 135 Average loss: 28498592320859577228453740544.0000\n",
      "Train Epoch: 136 [0/1000 (0%)]\tLoss: 206902704986969735862778068992.000000\n",
      "====> Epoch: 136 Average loss: 28498643135167466993923129344.0000\n",
      "Train Epoch: 137 [0/1000 (0%)]\tLoss: 47376390899259840397312.000000\n",
      "====> Epoch: 137 Average loss: 28490731274856368051356434432.0000\n",
      "Train Epoch: 138 [0/1000 (0%)]\tLoss: 490800563356604301312.000000\n",
      "====> Epoch: 138 Average loss: 28498646490074698506091626496.0000\n",
      "Train Epoch: 139 [0/1000 (0%)]\tLoss: 759654800245879996416.000000\n",
      "====> Epoch: 139 Average loss: 28498637840457647780177903616.0000\n",
      "Train Epoch: 140 [0/1000 (0%)]\tLoss: 397068436000810293264384.000000\n",
      "====> Epoch: 140 Average loss: 28498637455689200731686961152.0000\n",
      "Train Epoch: 141 [0/1000 (0%)]\tLoss: 3233292266850598292291584.000000\n",
      "====> Epoch: 141 Average loss: 26493670267334043680555139072.0000\n",
      "Train Epoch: 142 [0/1000 (0%)]\tLoss: 15663829893522055343342878720.000000\n",
      "====> Epoch: 142 Average loss: 28498646079695969226604937216.0000\n",
      "Train Epoch: 143 [0/1000 (0%)]\tLoss: 1860769768036019048808448.000000\n",
      "====> Epoch: 143 Average loss: 28498414133513548618696491008.0000\n",
      "Train Epoch: 144 [0/1000 (0%)]\tLoss: 15665267854116089150310449152.000000\n",
      "====> Epoch: 144 Average loss: 28490730754976347773844062208.0000\n",
      "Train Epoch: 145 [0/1000 (0%)]\tLoss: 4090307385791449399296.000000\n",
      "====> Epoch: 145 Average loss: 28496911639041068256664223744.0000\n",
      "Train Epoch: 146 [0/1000 (0%)]\tLoss: 61821297448927713430601728.000000\n",
      "====> Epoch: 146 Average loss: 28498597968873479878438027264.0000\n",
      "Train Epoch: 147 [0/1000 (0%)]\tLoss: 15727461420695482377774825472.000000\n",
      "====> Epoch: 147 Average loss: 28498646254207644435491061760.0000\n",
      "Train Epoch: 148 [0/1000 (0%)]\tLoss: 63344403601919745261043712.000000\n",
      "====> Epoch: 148 Average loss: 28498643818102933016104927232.0000\n",
      "Train Epoch: 149 [0/1000 (0%)]\tLoss: 15663875936595263322383712256.000000\n",
      "====> Epoch: 149 Average loss: 28498646273430793251991846912.0000\n"
     ]
    }
   ],
   "source": [
    "# now train the model\n",
    "for epoch in range(0, n_epochs):\n",
    "  train_vae(vae_model,mnist_train_loader,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTfRje_AkKDr"
   },
   "source": [
    "Now, generate some images with the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41tXdNsFkKk5"
   },
   "outputs": [],
   "source": [
    "def generate_images_vae(vae_model,n_images = 5):\n",
    "\n",
    "  epsilon = ... # FILL IN CODE HERE\n",
    "  imgs_generated = ... # FILL IN CODE HERE\n",
    "  return(imgs_generated)\n",
    "\n",
    "imgs_generated = generate_images_vae(vae_model,n_images=5)\n",
    "display_images(imgs_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGnvKoynzaFN"
   },
   "source": [
    "Do you think the results are better ? What difference can you see ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second autoencoder approach has a more complex probabilistic latent model (a full covariance matrix) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of $q_\\phi$\n",
    "\n",
    "Remember, our goal was to create an autoencoder such that the distributions $q_{\\phi}(z|x)$ of each input data point in the latent space is a normal distribution, ie such that $\\mu=0$ and $\\sigma=1$ for all inputs. Let us see if this goal was attained.\n",
    "\n",
    "For this, take a batch of test images, and calculate the vectors $\\mu=0$ and $\\sigma=1$ of these images (recall that they should be 0 and 1). Now,use a 2D scatter plot on any two of the latent coordinates to visualise the values of $\\mu$ and $\\sigma$ (you can try several different combinations of the latent coordinates.\n",
    "\n",
    "You will need to use ```detach().numpy()``` to plot these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we indeed see most values around $\\mu=0$, $\\sigma=1$ ? Is this in fact possible ? Why ? What compromise are we trying to achieve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuYn3_PBzjkI"
   },
   "source": [
    "__Your answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXm-D9Ef9vYm"
   },
   "source": [
    "Let us now compare the three models (AE diagonal covariance, AE full covariance and VAE) quantitavely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04MddkzuE324"
   },
   "source": [
    "# 3 Evaluating and comparing the models\n",
    "\n",
    "We will evaluate the models, in the following manner:\n",
    "\n",
    "- we train a simple convolutional neural network classifier on mnist, to a good accuracy\n",
    "- we generate images with each model\n",
    "- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images look like a \n",
    "\n",
    "We will use the following convoluional architecture for the classifier:\n",
    "\n",
    "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
    "- ReLU\n",
    "- conv2d, filter size  3×3 , 32 filters, stride=(2,2), padding=\"SAME\"\n",
    "- ReLU\n",
    "- MaxPool2D, stride=(2,2)\n",
    "- Flatten\n",
    "- Dense layer\n",
    "\n",
    "Now, define the model. To make things easier, use the ```torch.nn.Sequential``` API (there is no need for a Class in this simple case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P87a-DkXFOCv"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "nb_classes = int(mnist_trainset.targets.max()+1)\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "# --- Size of the successive layers\n",
    "n_h_0 = 1 #greyscale input images\n",
    "n_h_1 = nb_filters\n",
    "n_h_2 = nb_filters\n",
    "\n",
    "mnist_classification_model = ... # FILL IN CODE HERE\n",
    "\n",
    "criterion = ... # FILL IN CODE HERE\n",
    "optimizer = torch.optim.Adam(mnist_classification_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sUZONxw2bnt"
   },
   "source": [
    "Create a function to calculate accuracy, instead of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOww0ydr2fT0"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(x_pred,x_label):\n",
    "  acc = torch.sum(x_pred == x_label)/(x_pred.shape[0])\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw0vkZIqFcse"
   },
   "source": [
    "Now, train the model. You should be able to achieve an accuracy close to 1.00 within 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FA8YoX2FcHP"
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "  train_loss=0.0\n",
    "\n",
    "  for batch_idx, (imgs, labels) in enumerate(mnist_train_loader):\n",
    "\n",
    "    # set the gradients back to 0\n",
    "    optimizer.zero_grad()\n",
    "    predict=mnist_classification_model(imgs)\n",
    "    # apply loss function\n",
    "    loss=criterion(predict,labels)\n",
    "    acc = get_accuracy(torch.argmax(predict,dim=1),labels)\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss=loss.item()\n",
    "  print('Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}'.format(epoch,train_loss/imgs.shape[0],acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFgN5LblFwTa"
   },
   "source": [
    "### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n",
    "\n",
    "Now, we will evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n",
    "\n",
    "__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n",
    "- ```torch.nn.Softmax()(...)```\n",
    "\n",
    "Define this metric now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCJ_0qqjOXHT"
   },
   "outputs": [],
   "source": [
    "def generative_model_score(imgs_in,classification_model):\n",
    "  gen_score = ... # FILL IN CODE HERE\n",
    "  return(gen_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGq7YFg51UoP"
   },
   "source": [
    "Now, generate some images with each of the three models, and evaluate these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-L4u2jhILFx"
   },
   "outputs": [],
   "source": [
    "imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 50)\n",
    "imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,z_covariance,n_images = 50)\n",
    "imgs_vae = generate_images_vae(vae_model,n_images=50)\n",
    "\n",
    "# average of maximum of first model \n",
    "diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n",
    "non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n",
    "vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n",
    "\n",
    "print(\"Diagonal gaussian generative model score : \",diagonal_gaussian_score)\n",
    "print(\"Non diagonal gaussian generative model score : \",non_diagonal_gaussian_score)\n",
    "print(\"Variational autoencoder model score: \",vae_gaussian_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxvsG8FC1gNS"
   },
   "source": [
    "Please answer the following questions:\n",
    "\n",
    "- Which model is better quantitatively ? \n",
    "- Do the quantitative result support the qualitative results ?\n",
    "- Can you see any drawbacks of this method of evaluation ?\n",
    "- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SYCyfKR3G7Y"
   },
   "source": [
    "__Your answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
